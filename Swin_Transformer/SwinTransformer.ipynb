{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0a3f2d",
   "metadata": {},
   "source": [
    "# Swin Transformer (Tiny) PyTorch Implementation\n",
    "\n",
    "An implementation of the Swin Transformer in PyTorch, trained on Tiny ImageNet.\n",
    "This project is based on the original paper \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\", with several modifications from the published design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28956c78",
   "metadata": {},
   "source": [
    "Fixing validation dataset. Tiny ImageNet's validation set is not formatted the same way as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c02de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:07:31.096044Z",
     "iopub.status.busy": "2025-08-18T01:07:31.095414Z",
     "iopub.status.idle": "2025-08-18T01:08:56.403682Z",
     "shell.execute_reply": "2025-08-18T01:08:56.403026Z"
    },
    "papermill": {
     "duration": 85.315041,
     "end_time": "2025-08-18T01:08:56.405312",
     "exception": false,
     "start_time": "2025-08-18T01:07:31.090271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, shutil, csv, pathlib\n",
    "\n",
    "ROOT = \"/kaggle/input/tiny-imagenet-200/tiny-imagenet-200\"\n",
    "VAL_DIR = os.path.join(ROOT, \"val\")\n",
    "VAL_ANN = os.path.join(VAL_DIR, \"val_annotations.txt\")\n",
    "VAL_IMAGES = os.path.join(VAL_DIR, \"images\")\n",
    "\n",
    "OUT_VAL = \"/kaggle/working/tiny-imagenet-200-val\"\n",
    "\n",
    "os.makedirs(OUT_VAL, exist_ok=True)\n",
    "\n",
    "fname_to_wnid = {}\n",
    "with open(VAL_ANN, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        fname, wnid = row[0], row[1]\n",
    "        fname_to_wnid[fname] = wnid\n",
    "\n",
    "for fname, wnid in fname_to_wnid.items():\n",
    "    src = os.path.join(VAL_IMAGES, fname)\n",
    "    dst_dir = os.path.join(OUT_VAL, wnid)\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    dst = os.path.join(dst_dir, fname)\n",
    "    if not os.path.exists(dst):\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4da722",
   "metadata": {},
   "source": [
    "# Data Preparation and Augmentation\n",
    "\n",
    "Data augmentation techniques include RandomResizedCrop, RandomHorizontalFlip, RandAugment (applies random augmentations), ColorJitter and RandomErasing (randomly erases a rectangular region in an image). Mixup and CutMix will be implemented later. The pixel values are also normalized using the mean and standard deviation of the dataset. I found this in my ResNet-34 from scratch project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50099c9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:08:56.415846Z",
     "iopub.status.busy": "2025-08-18T01:08:56.415238Z",
     "iopub.status.idle": "2025-08-18T01:13:56.570258Z",
     "shell.execute_reply": "2025-08-18T01:13:56.569658Z"
    },
    "papermill": {
     "duration": 300.161493,
     "end_time": "2025-08-18T01:13:56.571826",
     "exception": false,
     "start_time": "2025-08-18T01:08:56.410333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "image_size = 64\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandAugment(num_ops=2, magnitude=5),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.2), value='random'),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "])\n",
    "\n",
    "train_dir = os.path.join(ROOT, \"train\")\n",
    "val_dir   = OUT_VAL\n",
    "\n",
    "train_set = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_set   = datasets.ImageFolder(val_dir,   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=64, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6d6af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:56.591673Z",
     "iopub.status.busy": "2025-08-18T01:13:56.591325Z",
     "iopub.status.idle": "2025-08-18T01:13:56.977857Z",
     "shell.execute_reply": "2025-08-18T01:13:56.976431Z"
    },
    "papermill": {
     "duration": 0.393181,
     "end_time": "2025-08-18T01:13:56.979553",
     "exception": false,
     "start_time": "2025-08-18T01:13:56.586372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val batch images shape: torch.Size([64, 3, 64, 64])\n",
      "Val batch labels shape: torch.Size([64])\n",
      "Sample labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(val_loader))\n",
    "\n",
    "print(\"Val batch images shape:\", xb.shape)   # should be (BS, 3, H, W)\n",
    "print(\"Val batch labels shape:\", yb.shape)   # should be (BS,)\n",
    "print(\"Sample labels:\", yb[:10].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8dafa",
   "metadata": {
    "papermill": {
     "duration": 0.004396,
     "end_time": "2025-08-18T01:13:56.989597",
     "exception": false,
     "start_time": "2025-08-18T01:13:56.985201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Patching and Splitting Windows\n",
    "Unlike traditional CNNs that use sliding convolutional filters, Vision Transformers break the image down into a sequence of patches, treating them similarly to words in a sentence.\n",
    "\n",
    "Patchify Class: Takes an input image and converts it into patch embeddings. It uses a single convolutional layer where the kernel size and stride are equal to the patch_size. It divides the image into non overlapping patches and creating an initial vector embedding for each one.\n",
    "\n",
    "split_into_windows: Takes the patches and splits them into smaller windows. Self-attention is calculated within these windows, which is far more computationally efficient than the original ViT's approach of global attention across all patches.\n",
    "\n",
    "reverse_windows: Merges the split windows back into their original spatial layout.\n",
    "\n",
    "Class documentation was generated using chat gpt after I finished coding and commenting explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17e27d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.001631Z",
     "iopub.status.busy": "2025-08-18T01:13:57.001267Z",
     "iopub.status.idle": "2025-08-18T01:13:57.008768Z",
     "shell.execute_reply": "2025-08-18T01:13:57.007792Z"
    },
    "papermill": {
     "duration": 0.015463,
     "end_time": "2025-08-18T01:13:57.010270",
     "exception": false,
     "start_time": "2025-08-18T01:13:56.994807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Patchify(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert an image into patch embeddings using a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        in_channels: Number of input channels (e.g., 3 for RGB).\n",
    "        embed_dim: Output embedding dimension per patch.\n",
    "        patch_size: Size of each square patch\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, H//patch_size, W//patch_size, embed_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, embed_dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (B, 3, H, W)\n",
    "        x = self.conv(x)             # (BS, embed_dim, H//patch_size, W//patch_size)\n",
    "        return x.permute(0, 2, 3, 1) # (BS, H//patch_size, W//patch_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bae2ae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.021330Z",
     "iopub.status.busy": "2025-08-18T01:13:57.021094Z",
     "iopub.status.idle": "2025-08-18T01:13:57.025852Z",
     "shell.execute_reply": "2025-08-18T01:13:57.024960Z"
    },
    "papermill": {
     "duration": 0.012175,
     "end_time": "2025-08-18T01:13:57.027077",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.014902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_into_windows(x, M):\n",
    "    \"\"\"\n",
    "    Splits (BS, H, W, channels) into non overlapping MxM windows.\n",
    "    Args:\n",
    "        x: Tensor of shape (BS, H, W, channels)\n",
    "        M: Window size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS * num_windows, M*M, channels)\n",
    "    \"\"\"\n",
    "    BS, H, W, channels = x.shape\n",
    "    x = x.reshape(BS, H//M, M, W//M, M, channels)\n",
    "    # Permute fixes the order so the MxM pixels are together properly\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)     # (BS, H//M, W//M, M, M, channels)\n",
    "    return x.reshape(-1, M*M, channels) # (BS * num_windows, M*M, channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7b9f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.037816Z",
     "iopub.status.busy": "2025-08-18T01:13:57.037222Z",
     "iopub.status.idle": "2025-08-18T01:13:57.041889Z",
     "shell.execute_reply": "2025-08-18T01:13:57.041184Z"
    },
    "papermill": {
     "duration": 0.011412,
     "end_time": "2025-08-18T01:13:57.043018",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.031606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reverse_windows(x, M, H, W, channels):\n",
    "    \"\"\"\n",
    "    Reverses MxM window tokens back into the original layout.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape (BS * num_windows, M*M, channels)\n",
    "        M: Window size\n",
    "        H: Original image height\n",
    "        W: Original image width\n",
    "        channels: Number of channels\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, H, W, channels)\n",
    "    \"\"\"\n",
    "    BS = x.shape[0] // (H//M * W//M) # Original BS\n",
    "    x = x.reshape(BS, H//M, W//M, M, M, channels)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)      # (BS, H//M, M, W//M, M, channels)\n",
    "    return x.reshape(BS, H, W, channels) # (BS, H, W, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2dc1d6",
   "metadata": {
    "papermill": {
     "duration": 0.004119,
     "end_time": "2025-08-18T01:13:57.051591",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.047472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Relative Position Bias\n",
    "\n",
    "Relative Position Bias shows self-attention mechanism about the geometry of the image. It computes a learnable bias between every pair of tokens in a window, based on how far apart they are. For each pair of tokens in an M x M window, compute relative position, use that to index into a learnable bias table, add this bias to the attention logits.\n",
    "\n",
    "When reading through the original Swin Transformer paper, Relative Position Bias gave me a lot of trouble. It took a very long time to finish this module. Comments and code were written by me, class documentation was generated after completion like all the other modules I wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd583965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.061206Z",
     "iopub.status.busy": "2025-08-18T01:13:57.061007Z",
     "iopub.status.idle": "2025-08-18T01:13:57.068491Z",
     "shell.execute_reply": "2025-08-18T01:13:57.067950Z"
    },
    "papermill": {
     "duration": 0.013558,
     "end_time": "2025-08-18T01:13:57.069492",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.055934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes relative position bias for self-attention.\n",
    "\n",
    "    Generates a table of learnable relative position biases between token pairs\n",
    "    within an attention window of shape (M x M). The relative position between\n",
    "    any two tokens is encoded as a bias vector per attention head, and these biases are\n",
    "    added to the attention scores in self-attention.\n",
    "\n",
    "    Args:\n",
    "        M: The height/width of the attention window. The total number of tokens is M * M.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        relative_table: Learnable parameter of shape ((2M - 1)^2, nheads),\n",
    "            where each entry represents a bias value for a specific relative position and head.\n",
    "        relative_index: Lookup table of shape (M*M, M*M), where each entry is an index\n",
    "            into relative_table that maps the relative position between two tokens to a bias vector.\n",
    "\n",
    "    Forward Output:\n",
    "        Tensor of shape (nheads, M*M, M*M) containing the relative bias for each token pair\n",
    "        and each attention head. This can be directly added to attention logits.\n",
    "\n",
    "    Example:\n",
    "        relative = RelativePositionBias(M=3, nheads=4)\n",
    "        bias = relative()  # Output shape: (4, 9, 9), for 4 heads and 3x3 tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        # (2M-1)^2 because there are up to M-1 tokens above or below or left or right of each token.\n",
    "        # 2M - 1 possibilities for above and below, same for left and right. So (2M -1)^2 total.\n",
    "        # If M = 3 row and col would go between -2 and +2 when comparing two tokens.\n",
    "        # That gives 5^2 possible combinations. len(-2, -1, 0, 1, 2)^2\n",
    "        self.relative_table = nn.Parameter(torch.zeros(size=((2*M - 1) * (2*M -1), nheads)))\n",
    "\n",
    "        # Coordinate grid of token positions shows where each token is in the window.\n",
    "        # It gives every token a (row, col) coordinate.\n",
    "        # If M = 3: coords[0] (rows): [[0, 0, 0], [1, 1, 1], [2, 2, 2]], \n",
    "        #           coords[1] (cols): [[0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
    "        coords = torch.stack(torch.meshgrid( # Matrix style over Cartesian\n",
    "            torch.arange(M), torch.arange(M), indexing='ij'))  # (2, M, M)\n",
    "        \n",
    "        # Flatten the coordinates for token indices, so coords[:, i] is the (row, col) of token i\n",
    "        coords = coords.flatten(1) # (2, M*M)\n",
    "\n",
    "        # Compute relative positions so we have the position of a token relative to another token\n",
    "        # coords[:, :, None] shape: (2, M*M, 1), coords[:, None, :] shape: (2, 1, M*M)\n",
    "        relative = coords[:, :, None] - coords[:, None, :] # (2, M*M, M*M)\n",
    "\n",
    "        # Reformat so we can use each (row, col) as an index into a table but row/col values\n",
    "        # range from -(M-1) to (M-1) so we shift them up so they are positive: [0, 2M -2]\n",
    "        relative = relative.permute(1, 2, 0) # (M*M, M*M, 2)\n",
    "        relative[:, :, 0] += M - 1\n",
    "        relative[:, :, 1] += M - 1\n",
    "\n",
    "        # Flatten 2D positions into 1D. To convert: row * num_cols + col\n",
    "        self.register_buffer(   # Register buffer to move to GPU\n",
    "            \"relative_index\",\n",
    "            (relative[:, :, 0] * (2*M - 1) + relative[:, :, 1]).long() # (M*M, M*M)\n",
    "        )\n",
    "    def forward(self): \n",
    "        # Use index to get bias values, look up the bias vector for each token pair\n",
    "        bias = self.relative_table[self.relative_index.view(-1)] # (M*M * M*M, nheads)\n",
    "        bias = bias.reshape(self.relative_index.shape[0], self.relative_index.shape[1], self.nheads)\n",
    "        return bias.permute(2, 0, 1) # (nheads, M*M, M*M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300ba25",
   "metadata": {
    "papermill": {
     "duration": 0.004195,
     "end_time": "2025-08-18T01:13:57.078097",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.073902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Windowed Multi-Head Self-Attention (W-MSA)\n",
    "\n",
    "Implements Windowed Multi-head Self-Attention, which is a more efficient version of the standard attention used in ViT. Instead of calculating attention across all patches in the entire image, W-MSA computes attention within M x M windows. Significantly reduces the number of calculations needed.\n",
    "\n",
    "Attention(Q, K, V) = Softmax((QK.T / √d) + B + mask) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa44e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.087558Z",
     "iopub.status.busy": "2025-08-18T01:13:57.087346Z",
     "iopub.status.idle": "2025-08-18T01:13:57.095148Z",
     "shell.execute_reply": "2025-08-18T01:13:57.094533Z"
    },
    "papermill": {
     "duration": 0.013868,
     "end_time": "2025-08-18T01:13:57.096208",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.082340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window-based Multihead Self-Attention with relative position bias.\n",
    "\n",
    "    Performs self-attention within non overlapping MxM windows of the input feature map.\n",
    "    It incorporates relative positional encoding and attention masks for shifted windows\n",
    "\n",
    "    Attention(Q, K, V) = Softmax((QK.T / √d) + B + mask) @ V\n",
    "    B is relative position bias.\n",
    "    Args:\n",
    "        channels: Input channels\n",
    "        M: Height and width of the attention window.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        q, k, v: Linear layers for queries, keys, and values.\n",
    "        out: Output linear layer after attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        self.rootd = (channels // nheads) ** -0.5 # (1 / √d) == (1 / √dim_per_head)\n",
    "\n",
    "        self.q = nn.Linear(channels, channels)\n",
    "        self.k = nn.Linear(channels, channels)\n",
    "        self.v = nn.Linear(channels, channels)\n",
    "        self.out = nn.Linear(channels, channels)\n",
    "\n",
    "        self.relative = RelativePositionBias(M, nheads)\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for window based self-attention.\n",
    "\n",
    "        Args:\n",
    "            x: shape (B * nW, M*M, channels) where nW is number of windows\n",
    "            attn_mask: Attention mask used for shifted windows to prevent cross-window attention.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B*nW, M*M, channels)\n",
    "        \"\"\"\n",
    "        # x shape = (B * nW, M*M, channels) where nW is number of windows\n",
    "        BnW, M_sq, channels = x.shape # M_sq is M*M\n",
    "\n",
    "        # d stands for dim_per_head. Permute on k so no transpose when computing attn.\n",
    "        # q: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        # k: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> permute(0, 2, 3, 1) --> (BnW, heads, d, M*M)\n",
    "        # v: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        q = self.q(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "        k = self.k(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).permute(0, 2, 3, 1)\n",
    "        v = self.v(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "\n",
    "        # Attention: (Q @ K.T) / √d + relative bias + optional mask\n",
    "        # k is already transposed.\n",
    "        attn = (q @ k) * self.rootd # (BnW, nheads, M*M, M*M)\n",
    "        attn = attn + self.relative()\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (M*M, M*M) --> unsqueeze(0) --> (1, M*M, M*M)\n",
    "            # Broadcasted to (BnW, nheads, M*M, M*M)\n",
    "            nW = attn_mask.shape[0]\n",
    "            BS = BnW // nW\n",
    "            attn_mask = attn_mask.to(attn.device)\n",
    "            attn = attn.view(BS, nW, self.nheads, M_sq, M_sq) + attn_mask.unsqueeze(0).unsqueeze(2)\n",
    "            attn = attn.view(BnW, self.nheads, M_sq, M_sq)\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(BnW, M_sq, channels)\n",
    "        return self.out(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ab4b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.106335Z",
     "iopub.status.busy": "2025-08-18T01:13:57.106113Z",
     "iopub.status.idle": "2025-08-18T01:13:57.112384Z",
     "shell.execute_reply": "2025-08-18T01:13:57.111670Z"
    },
    "papermill": {
     "duration": 0.012547,
     "end_time": "2025-08-18T01:13:57.113495",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.100948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_attention_mask(H, W, M, shift):\n",
    "    \"\"\"\n",
    "    Creates an attention mask for shifted window self-attention (SW-MSA).\n",
    "\n",
    "    This function generates a mask to prevent tokens from attending across windows when \n",
    "    performing SW-MSA. It divides the feature map into distinct regions, assigns unique labels\n",
    "    to each, uses cyclic shifting, partitions it into non overlapping windows, and then\n",
    "    builds an attention mask that blocks attention between different labeled regions.\n",
    "\n",
    "    Args:\n",
    "        H: Height of the feature map.\n",
    "        W: Width of the feature map.\n",
    "        M: Window size.\n",
    "        shift: Number of pixels to cyclically shift the window. \n",
    "               If shift is 0, no mask is needed. In Swin it is M // 2\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (nW, M*M, M*M) where nW is the number of windows.\n",
    "        Or None if shift is 0.\n",
    "    \"\"\"\n",
    "    if shift == 0:\n",
    "        return None\n",
    "    \n",
    "    img_mask = torch.zeros((1, H, W, 1))  # Mask\n",
    "\n",
    "    count = 0\n",
    "    H, W = img_mask.shape[1:3]\n",
    "\n",
    "    # Split image into 9 regions\n",
    "    h_ranges = [(0, H - M), (H - M, H - shift), (H - shift, H)]\n",
    "    w_ranges = [(0, W - M), (W - M, W - shift), (W - shift, W)]\n",
    "\n",
    "    # so if H = W = 12, M = 6, shift = 3\n",
    "    # h_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "    # w_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "\n",
    "    # Fill each region with a unique integer\n",
    "    for h_start, h_end in h_ranges:\n",
    "        for w_start, w_end in w_ranges:\n",
    "            img_mask[:, h_start:h_end, w_start:w_end, :] = count\n",
    "            count += 1\n",
    "            \n",
    "    # Cyclic shift the mask\n",
    "    img_mask = torch.roll(img_mask, shifts=(-shift, -shift), dims=(1,2))\n",
    "\n",
    "    # Split into M*M windows\n",
    "    mask_windows = split_into_windows(img_mask, M)  # (nW, M*M, 1)\n",
    "    mask_windows = mask_windows.squeeze(-1)       # (nW, M*M)\n",
    "    # Create attention mask\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # (nW, M*M, M*M)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float('-inf')).masked_fill(attn_mask == 0, 0.0)\n",
    "    return attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd151030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.123194Z",
     "iopub.status.busy": "2025-08-18T01:13:57.123006Z",
     "iopub.status.idle": "2025-08-18T01:13:57.147328Z",
     "shell.execute_reply": "2025-08-18T01:13:57.146599Z"
    },
    "papermill": {
     "duration": 0.03034,
     "end_time": "2025-08-18T01:13:57.148503",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.118163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_attention_mask(12, 12, 6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbe74a",
   "metadata": {
    "papermill": {
     "duration": 0.004362,
     "end_time": "2025-08-18T01:13:57.157934",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.153572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stochastic Depth\n",
    "\n",
    "Stochastic depth is a regularization technique used to improve generalization and reduce overfitting. Instead of dropping individual neurons, entire residual branches are skipped during training with a given probability (drop_prob)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a7c5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.167739Z",
     "iopub.status.busy": "2025-08-18T01:13:57.167508Z",
     "iopub.status.idle": "2025-08-18T01:13:57.172397Z",
     "shell.execute_reply": "2025-08-18T01:13:57.171870Z"
    },
    "papermill": {
     "duration": 0.010985,
     "end_time": "2025-08-18T01:13:57.173351",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.162366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stochastic_depth(x, drop_prob, training):\n",
    "    \"\"\"\n",
    "    Applies stochastic depth to the input.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        drop_prob: Probability of dropping the path.\n",
    "        training: If True, stochastic depth is applied. If False, input is returned unchanged.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor with some residual paths zeroed out.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "\n",
    "    keep_prob = 1.0 - drop_prob\n",
    "    # Create mask with shape (BS, 1, 1, ..., 1) so it broadcasts over all non batch dims\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    # 1 with prob keep_prob, 0 with prob drop_prob\n",
    "    mask = torch.rand(shape, dtype=x.dtype, device=x.device) < keep_prob\n",
    "    # Scale\n",
    "    x = x / keep_prob\n",
    "    return x * mask\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"\n",
    "    Module for stochastic depth.\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return stochastic_depth(x, self.drop_prob, self.training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d64f0",
   "metadata": {
    "papermill": {
     "duration": 0.004263,
     "end_time": "2025-08-18T01:13:57.182246",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.177983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SwinBlock (Shifted Window Transformer Block)\n",
    "\n",
    "Applies windowed self-attention over non overlapping M×M windows and alternates between non-shifted and shifted windows across consecutive blocks to enable cross-window connections. There is also an MLP with GELU at the end. Residual connections are used.\n",
    "\n",
    "![Block Architecture](figures/Block_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed8877a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.192291Z",
     "iopub.status.busy": "2025-08-18T01:13:57.192093Z",
     "iopub.status.idle": "2025-08-18T01:13:57.199263Z",
     "shell.execute_reply": "2025-08-18T01:13:57.198766Z"
    },
    "papermill": {
     "duration": 0.013438,
     "end_time": "2025-08-18T01:13:57.200286",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.186848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer block with shifted/non-shifted window self-attention + MLP.\n",
    "\n",
    "    Args:\n",
    "        dim: Channel dimension of the input features.\n",
    "        H: Feature map height.\n",
    "        W: Feature map width.\n",
    "        nheads: Number of attention heads in WindowAttention.\n",
    "        M: Window size.\n",
    "        shift: Cyclic shift size (0 for non-shifted windows, M//2 for shifted).\n",
    "        ratio: Expansion ratio for the MLP hidden size (hidden dim = ratio * dim).\n",
    "        stoch_depth: stochastic depth probability for dropping residual branches.\n",
    "\n",
    "    Attributes:\n",
    "        norm1: Pre attention normalization.\n",
    "        attn (WindowAttention): Window-based multi-head self-attention.\n",
    "        drop_path: Stochastic depth module or identity if stoch_depth == 0.\n",
    "        norm2: Pre MLP normalization.\n",
    "        mlp: Two-layer feed forward network with GELU activation.\n",
    "        attn_mask: Mask for shifted attention, shape (nW, M*M, M*M) when shift > 0, else None.\n",
    "\n",
    "    Input:\n",
    "        x: Tensor of shape (BS, H, W, channels).\n",
    "\n",
    "    Output:\n",
    "        Tensor of shape (BS, H, W, Channels), same spatial shape and channels as input.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, H, W, nheads, M, shift, ratio, stoch_depth):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "        self.shift = shift\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(channels=dim, M=M, nheads=nheads)\n",
    "        if stoch_depth > 0:\n",
    "            self.stoch = StochasticDepth(stoch_depth)\n",
    "        else:\n",
    "            self.stoch = nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * ratio)), # Error I had before, forgot to wrap with int()\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * ratio), dim)\n",
    "        )\n",
    "        self.attn_mask = create_attention_mask(H, W, M, shift)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of SwinBlock.\n",
    "\n",
    "        LayerNorm --> cyclic shift --> split windows --> WindowAttention (masked if shifted)\n",
    "        --> reverse windows --> reverse shift --> residual + stochastic depth --> LayerNorm\n",
    "        --> MLP → residual +stochastic depth\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (BS, H, W, channels).\n",
    "        \"\"\"\n",
    "        BS, H, W, channels = x.shape\n",
    "\n",
    "        store = x # For Residual connection\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        if self.shift > 0:\n",
    "            x = torch.roll(x, shifts=(-self.shift, -self.shift), dims=(1, 2)) # Cyclic shift\n",
    "        x_windows = split_into_windows(x, self.M)                  # (BS*nW, M*M, channels)\n",
    "        x_windows = self.attn(x_windows, attn_mask=self.attn_mask) # (BS*nW, M*M, channels)\n",
    "        x = reverse_windows(x_windows, self.M, H, W, channels)     # (BS, H, W, channels)\n",
    "        if self.shift > 0:\n",
    "            x = torch.roll(x, shifts=(self.shift, self.shift), dims=(1, 2))\n",
    "\n",
    "        x = store + self.stoch(x)\n",
    "\n",
    "        return x + self.stoch(self.mlp(self.norm2(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce0b0c",
   "metadata": {
    "papermill": {
     "duration": 0.004506,
     "end_time": "2025-08-18T01:13:57.209563",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.205057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Patch Merging Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cedc0bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.219246Z",
     "iopub.status.busy": "2025-08-18T01:13:57.219050Z",
     "iopub.status.idle": "2025-08-18T01:13:57.223870Z",
     "shell.execute_reply": "2025-08-18T01:13:57.223375Z"
    },
    "papermill": {
     "duration": 0.010941,
     "end_time": "2025-08-18T01:13:57.224913",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.213972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduces H and W by 2x in each dimension. Doubles the channel dimension.\n",
    "    - Extract non overlapping 2x2 patches from the feature map.\n",
    "    - Concatenate features from each patch along the channel dimension.\n",
    "    - Apply LayerNorm for normalization across channels.\n",
    "    - Lower 4*channels down to 2*channels with a Linear layer.\n",
    "\n",
    "    Input:\n",
    "        x: shape (BS, H, W, channels)\n",
    "\n",
    "    Output:\n",
    "        shape (BS, H/2, W/2, 2*channels)\n",
    "\n",
    "    H and W should be even.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4*dim, 2*dim)\n",
    "        self.norm = nn.LayerNorm(4*dim) # 4 * channels --> 2 * channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (BS, H, W, channels)\n",
    "        x1 = x[:, 0::2, 0::2, :]\n",
    "        x2 = x[:, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 1::2, :]\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=3) # (BS, H/2, W/2, 4*channels)\n",
    "        x = self.norm(x)\n",
    "        return self.lin(x) # (BS, H/2, W/2, 2*channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51e185",
   "metadata": {
    "papermill": {
     "duration": 0.00444,
     "end_time": "2025-08-18T01:13:57.234030",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.229590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stage (Stack of Swin Blocks + Optional Patch Merging)\n",
    "\n",
    "A Stage stacks SwinBlocks, alternating between:\n",
    "- W-MSA (non-shifted windows, shift=0)\n",
    "- SW-MSA (shifted windows, shift=M//2)\n",
    "\n",
    "Then optionally applies PatchMerging to downsample and increase channels for the next stage.\n",
    "After PatchMerging, the next stage should be constructed with dim = 2 * previous_dim and H and W halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e7175b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.243921Z",
     "iopub.status.busy": "2025-08-18T01:13:57.243715Z",
     "iopub.status.idle": "2025-08-18T01:13:57.249309Z",
     "shell.execute_reply": "2025-08-18T01:13:57.248661Z"
    },
    "papermill": {
     "duration": 0.011857,
     "end_time": "2025-08-18T01:13:57.250372",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.238515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stage(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dim: Channel dimension.\n",
    "        H: Feature map height.\n",
    "        W: Feature map width.\n",
    "        M: Window size.\n",
    "        blocks: Number of SwinBlocks in this stage.\n",
    "        nheads: Number of attention heads per block.\n",
    "        stoch_depth_list: List of stochastic depth probabilities (len == blocks).\n",
    "        patch_merging: If True, apply PatchMerging at the end of the stage.\n",
    "        ratio: MLP expansion ratio (hidden dim = ratio * dim).\n",
    "\n",
    "    Input:\n",
    "        x: Tensor of shape (BS, H, W, channels or dim)\n",
    "\n",
    "    Output:\n",
    "        - If patch_merging is False: (BS, H, W, dim)\n",
    "        - If patch_merging is True:  (BS, H/2, W/2, 2*dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, H, W, M, blocks, nheads, stoch_depth_list, patch_merging, ratio):\n",
    "        # stoch_depth_list is a list of the stochastic depth rates for each SwinBlock.\n",
    "\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(blocks):\n",
    "            if i % 2 == 0:\n",
    "                shift = 0  # Alternate between W-MSA and SW-MSA, W-MSA has no shift.\n",
    "            else:\n",
    "                shift = M // 2\n",
    "            self.blocks.append(\n",
    "                SwinBlock(dim, H, W, nheads, M, shift, ratio, stoch_depth_list[i])\n",
    "            )\n",
    "        if patch_merging:\n",
    "            self.patch = PatchMerging(dim)\n",
    "        else:\n",
    "            self.patch = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (BS, H, W, channels)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.patch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b1851",
   "metadata": {
    "papermill": {
     "duration": 0.004387,
     "end_time": "2025-08-18T01:13:57.259354",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.254967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters\n",
    "\n",
    "Since 224 x 224 images was too large for my GPU. I went down to 64 x 64 and adjusted. I used stage specific window sizes (M).\n",
    "\n",
    "- Patch size: 4 × 4\n",
    "- Base embed dim C: 96\n",
    "- Depths: [2, 2, 6, 2]\n",
    "- Num heads: [2, 4, 8, 16]\n",
    "- M: [8, 8, 4, 2]\n",
    "- Window size: 7 for all blocks\n",
    "- Shift: 3 (7 // 2) for all shift blocks\n",
    "- MLP expantion ratio: 4.0 for all blocks\n",
    "- Drop path rate: 0.2 (linearly increased across all blocks)\n",
    "- Patch Merging / Downsample at the end of Stage 1, 2 and 3.\n",
    "\n",
    "\n",
    "| Stage         | Blocks | Heads | M | Stoch_dep | In Channels | Out Channels | Output Shape             |\n",
    "|---------------|--------|-------|---|-----------|-------------|--------------|--------------------------|\n",
    "| PatchEmbed    | None   | None  | None | None           | 3           | 96           | (BS, H/4, W/4, 96)       |\n",
    "| Stage 1       | 2      | 2     | 8 |[0.0000, 0.0182]          | 96          | 192          | (BS, H/8, W/8, 192)      |\n",
    "| Stage 2       | 2      | 4     | 8 |[0.0364, 0.0545]          | 192         | 384          | (BS, H/16, W/16, 384)    |\n",
    "| Stage 3       | 6      | 8     | 4 |[0.0727, 0.0909, 0.1091, 0.1273, 0.1455, 0.1636]          | 384         | 768          | (BS, H/32, W/32, 768)    |\n",
    "| Stage 4       | 2      | 16    | 2 |[0.1818, 0.2000]          | 768         | 768          | (BS, H/32, W/32, 768)    |\n",
    "| Head          | None   | None  | None | None          | 768         | num_classes  | (BS, num_classes)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f333f2b",
   "metadata": {
    "papermill": {
     "duration": 0.004517,
     "end_time": "2025-08-18T01:13:57.268516",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.263999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Swin Transformer Architecture\n",
    "\n",
    "![Architecture](figures/Architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d4747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.279038Z",
     "iopub.status.busy": "2025-08-18T01:13:57.278536Z",
     "iopub.status.idle": "2025-08-18T01:13:57.287183Z",
     "shell.execute_reply": "2025-08-18T01:13:57.286592Z"
    },
    "papermill": {
     "duration": 0.015176,
     "end_time": "2025-08-18T01:13:57.288231",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.273055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer\n",
    "\n",
    "    Steps:\n",
    "        - Patchify to get (BS, H/patch, W/patch, emb_dim)\n",
    "        - 4 stages of Swin blocks with alternating W-MSA (shift=0) and SW-MSA (shift=M//2)\n",
    "        - Patch Merging at the end of stages 1-3\n",
    "        - Global average pooling and linear classifier head\n",
    "\n",
    "    Args:\n",
    "        img_size: Input image size\n",
    "        patch_size: Patch size for Patchify.\n",
    "        emb_dim: Base embedding dimension channel for stage 1.\n",
    "        blocks (List[int]): Number of blocks per stage.\n",
    "        nheads (List[int]): Number of attention heads per stage.\n",
    "        M: Window size for all blocks.\n",
    "        n_classes: Number of output classes for the classifier head.\n",
    "        stochastic_endpoint: Stochastic depth ratio endpoint for linspace.\n",
    "\n",
    "    Shapes:\n",
    "        Input:  (BS, 3, 64, 64)\n",
    "        After patchify: (BS, 16, 16, 96)\n",
    "        After stage1:   (BS, 8, 8, 192)\n",
    "        After stage2:   (BS, 4, 4, 384)\n",
    "        After stage3:   (BS, 2, 2, 768)\n",
    "        After stage4:   (BS, 2, 2, 768)\n",
    "        Output logits:  (BS, n_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, emb_dim, blocks,\n",
    "                 nheads, M, n_classes, stochastic_endpoint):\n",
    "        super().__init__()\n",
    "\n",
    "        H = img_size // patch_size\n",
    "        W = img_size // patch_size\n",
    "        dims = [emb_dim, 2*emb_dim, 4*emb_dim, 8*emb_dim]\n",
    "\n",
    "        self.patchify = Patchify(3, emb_dim, patch_size)\n",
    "\n",
    "        # Linearly increase across all blocks (inclusive endpoints)\n",
    "        stoch_depth = list(np.linspace(0, stochastic_endpoint, sum(blocks)))\n",
    "        \n",
    "        # 4 Stages with stochastic depth aligned with the number of blocks in each stage.\n",
    "        ind = 0\n",
    "        self.stage1 = Stage(\n",
    "            dims[0], H, W, M[0], \n",
    "            blocks[0], nheads[0], \n",
    "            stoch_depth[ind:ind+blocks[0]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[0]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage2 = Stage(\n",
    "            dims[1], H, W, M[1], \n",
    "            blocks[1], nheads[1], \n",
    "            stoch_depth[ind:ind+blocks[1]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[1]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage3 = Stage(\n",
    "            dims[2], H, W, M[2], \n",
    "            blocks[2], nheads[2], \n",
    "            stoch_depth[ind:ind+blocks[2]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[2]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage4 = Stage(\n",
    "            dims[3], H, W, M[3], \n",
    "            blocks[3], nheads[3], \n",
    "            stoch_depth[ind:ind+blocks[3]], \n",
    "            patch_merging=False, ratio=4.0)\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[3])\n",
    "        self.head = nn.Linear(dims[3], n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (BS, 3, img_size, img_size).\n",
    "\n",
    "        Returns:\n",
    "            Class logits of shape (BS, n_classes).\n",
    "        \"\"\"\n",
    "        # x shape = (BS, 3, 64, 64)\n",
    "        x = self.patchify(x) # (BS, H//patch_size, W//patch_size, embed_dim)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.norm(x).mean(dim=(1, 2)) # Global average pooling over H, W.\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e78ca",
   "metadata": {
    "papermill": {
     "duration": 0.004394,
     "end_time": "2025-08-18T01:13:57.297179",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.292785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Soft Cross-Entropy\n",
    "\n",
    "Loss that works with both hard labels and soft labels produced by Mixup/CutMix. When the targets are hard we use nn.CrossEntropyLoss, when they are soft, we compute: -∑ p · log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d757e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.307048Z",
     "iopub.status.busy": "2025-08-18T01:13:57.306829Z",
     "iopub.status.idle": "2025-08-18T01:13:57.312167Z",
     "shell.execute_reply": "2025-08-18T01:13:57.311660Z"
    },
    "papermill": {
     "duration": 0.011433,
     "end_time": "2025-08-18T01:13:57.313168",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.301735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def one_hot(labels, num_classes):\n",
    "    return F.one_hot(labels, num_classes=num_classes).float()\n",
    "\n",
    "class SoftCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    If target is LongTensor -> falls back to nn.CrossEntropyLoss (hard labels).\n",
    "    If target is FloatTensor (N,C) -> computes soft CE: -sum(p * log_softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        if target.dtype == torch.long:\n",
    "            return self.ce(logits, target)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(target * log_probs).sum(dim=1).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c3659",
   "metadata": {
    "papermill": {
     "duration": 0.004393,
     "end_time": "2025-08-18T01:13:57.322209",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.317816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mixup + CutMix module\n",
    "\n",
    "This utility randomly applies Mixup or CutMix per batch using Beta distributed mixing coefficients. It returns mixed images and soft targets. We keep probabilities modest at 64×64 to avoid over-regularization. I got chat gpt to generate a draft for this module and I adjusted everything from there. This was the only part of this project where I generated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e74583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.331959Z",
     "iopub.status.busy": "2025-08-18T01:13:57.331770Z",
     "iopub.status.idle": "2025-08-18T01:13:57.342109Z",
     "shell.execute_reply": "2025-08-18T01:13:57.341570Z"
    },
    "papermill": {
     "duration": 0.01649,
     "end_time": "2025-08-18T01:13:57.343154",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.326664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randbox(W, H, lam):\n",
    "    # CutMix box size from area ratio lam\n",
    "    cut_rat = (1.0 - lam) ** 0.5\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    cx = random.randint(0, W - 1)\n",
    "    cy = random.randint(0, H - 1)\n",
    "    x1 = max(cx - cut_w // 2, 0)\n",
    "    y1 = max(cy - cut_h // 2, 0)\n",
    "    x2 = min(cx + cut_w // 2, W)\n",
    "    y2 = min(cy + cut_h // 2, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "class MixupCutmix:\n",
    "    \"\"\"\n",
    "    On each batch, applies Mixup or CutMix with given probabilities.\n",
    "    Returns:\n",
    "      images: possibly mixed tensor\n",
    "      targets: either Long (no mix) or Float one-hot (mixed)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes,\n",
    "                 mixup_alpha = 0.8,\n",
    "                 cutmix_alpha = 1.0,\n",
    "                 p_mixup = 0.5,\n",
    "                 p_cutmix = 0.5):\n",
    "        self.num_classes = num_classes\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_alpha = cutmix_alpha\n",
    "        self.p_mixup = p_mixup\n",
    "        self.p_cutmix = p_cutmix\n",
    "        self.enabled = True\n",
    "\n",
    "    def off(self):\n",
    "        self.enabled = False\n",
    "    def on(self):\n",
    "        self.enabled = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, images, targets):\n",
    "        if (not self.enabled) or (self.p_mixup <= 0 and self.p_cutmix <= 0):\n",
    "            return images, targets  # no change\n",
    "\n",
    "        BS, channels, H, W = images.shape\n",
    "        # decide op\n",
    "        op = None\n",
    "        r = random.random()\n",
    "        if r < self.p_mixup:\n",
    "            op = 'mixup'\n",
    "        elif r < self.p_mixup + self.p_cutmix:\n",
    "            op = 'cutmix'\n",
    "        else:\n",
    "            return images, targets  # no change\n",
    "\n",
    "        # sample lambda from Beta\n",
    "        from torch.distributions import Beta\n",
    "\n",
    "        if op == 'mixup' and self.mixup_alpha > 0:\n",
    "            lam = Beta(self.mixup_alpha, self.mixup_alpha).sample().item()\n",
    "        elif op == 'cutmix' and self.cutmix_alpha > 0:\n",
    "            lam = Beta(self.cutmix_alpha, self.cutmix_alpha).sample().item()\n",
    "        else:\n",
    "            return images, targets\n",
    "\n",
    "        lam = max(min(lam, 0.999), 0.001)\n",
    "\n",
    "        # shuffle\n",
    "        index = torch.randperm(BS, device=images.device)\n",
    "        y1 = one_hot(targets, self.num_classes).to(images.dtype)\n",
    "        y2 = one_hot(targets[index], self.num_classes).to(images.dtype)\n",
    "\n",
    "        if op == 'mixup':\n",
    "            mixed = lam * images + (1.0 - lam) * images[index]\n",
    "            y = lam * y1 + (1.0 - lam) * y2\n",
    "            return mixed, y\n",
    "\n",
    "        # CutMix\n",
    "        x1, y1b, x2, y2b = randbox(W, H, lam)\n",
    "        mixed = images.clone()\n",
    "        mixed[:, :, y1b:y2b, x1:x2] = images[index, :, y1b:y2b, x1:x2]\n",
    "\n",
    "        # adjust lam to actual area\n",
    "        box_area = (x2 - x1) * (y2b - y1b)\n",
    "        lam_adj = 1.0 - float(box_area) / float(W * H)\n",
    "        y = lam_adj * one_hot(targets, self.num_classes).to(images.dtype) + \\\n",
    "            (1.0 - lam_adj) * one_hot(targets[index], self.num_classes).to(images.dtype)\n",
    "        return mixed, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94383e8",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "**Loss**\n",
    "- SoftCrossEntropy(label_smoothing=0.05)\n",
    "- Works with hard labels and soft labels (from Mixup/CutMix). Small smoothing for stability.\n",
    "\n",
    "**Regularization**\n",
    "- Mixup: p_mixup=0.3, mixup_alpha=0.4\n",
    "- CutMix: p_cutmix=0.3, cutmix_alpha=0.8\n",
    "- Policy: on for most of training; off for epoch 0 and the last 10 epochs to sharpen.\n",
    "\n",
    "**Optimizer**\n",
    "- AdamW(lr=start_lr=2.5e-4, betas=(0.9, 0.999))\n",
    "- Weight decay 0.05 on most params, excluded for bias/LayerNorm/relative-pos/pos_embed.\n",
    "- Gradient clipping: max_norm=3.0\n",
    "\n",
    "**Learning Rate Schedule**\n",
    "- Warmup: LinearLR for warmup_epochs=10 from 1% --> 100% of start_lr\n",
    "- Cosine: CosineAnnealingLR for cosine_epochs=190 with floor eta_min=5e-6\n",
    "- Total epochs: 200\n",
    "\n",
    "**Precision / Performance**\n",
    "- AMP enabled by torch.cuda.amp.GradScaler\n",
    "- non_blocking=True on device transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3138053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T01:13:57.353470Z",
     "iopub.status.busy": "2025-08-18T01:13:57.353285Z",
     "iopub.status.idle": "2025-08-18T08:47:37.505625Z",
     "shell.execute_reply": "2025-08-18T08:47:37.504675Z"
    },
    "papermill": {
     "duration": 27220.170968,
     "end_time": "2025-08-18T08:47:37.518943",
     "exception": false,
     "start_time": "2025-08-18T01:13:57.347975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/258014944.py:51: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
      "/tmp/ipykernel_19/258014944.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
      "/tmp/ipykernel_19/258014944.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n",
      "Epoch 1, Train Loss: 5.1915, Val Loss: 4.9821, Top1 Acc: 0.0336, Top5 Acc: 0.1240, LR 0.000027\n",
      "Saved new best model.\n",
      "Epoch 2, Train Loss: 4.9946, Val Loss: 4.5802, Top1 Acc: 0.0739, Top5 Acc: 0.2341, LR 0.000052\n",
      "Saved new best model.\n",
      "Epoch 3, Train Loss: 4.7811, Val Loss: 4.3221, Top1 Acc: 0.1142, Top5 Acc: 0.3072, LR 0.000077\n",
      "Saved new best model.\n",
      "Epoch 4, Train Loss: 4.6086, Val Loss: 4.1162, Top1 Acc: 0.1410, Top5 Acc: 0.3514, LR 0.000101\n",
      "Saved new best model.\n",
      "Epoch 5, Train Loss: 4.4472, Val Loss: 3.9678, Top1 Acc: 0.1593, Top5 Acc: 0.3931, LR 0.000126\n",
      "Saved new best model.\n",
      "Epoch 6, Train Loss: 4.3176, Val Loss: 3.7881, Top1 Acc: 0.1904, Top5 Acc: 0.4447, LR 0.000151\n",
      "Saved new best model.\n",
      "Epoch 7, Train Loss: 4.2444, Val Loss: 3.7120, Top1 Acc: 0.2052, Top5 Acc: 0.4634, LR 0.000176\n",
      "Saved new best model.\n",
      "Epoch 8, Train Loss: 4.1417, Val Loss: 3.6967, Top1 Acc: 0.2110, Top5 Acc: 0.4625, LR 0.000201\n",
      "Saved new best model.\n",
      "Epoch 9, Train Loss: 4.0691, Val Loss: 3.4861, Top1 Acc: 0.2505, Top5 Acc: 0.5178, LR 0.000225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n",
      "Epoch 10, Train Loss: 3.9755, Val Loss: 3.4789, Top1 Acc: 0.2510, Top5 Acc: 0.5200, LR 0.000250\n",
      "Saved new best model.\n",
      "Epoch 11, Train Loss: 3.9105, Val Loss: 3.4478, Top1 Acc: 0.2537, Top5 Acc: 0.5273, LR 0.000250\n",
      "Saved new best model.\n",
      "Epoch 12, Train Loss: 3.8741, Val Loss: 3.3205, Top1 Acc: 0.2829, Top5 Acc: 0.5594, LR 0.000250\n",
      "Saved new best model.\n",
      "Epoch 13, Train Loss: 3.7887, Val Loss: 3.1831, Top1 Acc: 0.3134, Top5 Acc: 0.5907, LR 0.000250\n",
      "Epoch 14, Train Loss: 3.6939, Val Loss: 3.2082, Top1 Acc: 0.3073, Top5 Acc: 0.5875, LR 0.000250\n",
      "Saved new best model.\n",
      "Epoch 15, Train Loss: 3.6365, Val Loss: 3.1031, Top1 Acc: 0.3382, Top5 Acc: 0.6039, LR 0.000250\n",
      "Epoch 16, Train Loss: 3.5900, Val Loss: 3.0840, Top1 Acc: 0.3380, Top5 Acc: 0.6162, LR 0.000249\n",
      "Saved new best model.\n",
      "Epoch 17, Train Loss: 3.5168, Val Loss: 2.9807, Top1 Acc: 0.3560, Top5 Acc: 0.6367, LR 0.000249\n",
      "Epoch 18, Train Loss: 3.4848, Val Loss: 2.9828, Top1 Acc: 0.3541, Top5 Acc: 0.6366, LR 0.000249\n",
      "Saved new best model.\n",
      "Epoch 19, Train Loss: 3.4456, Val Loss: 2.9431, Top1 Acc: 0.3679, Top5 Acc: 0.6455, LR 0.000249\n",
      "Saved new best model.\n",
      "Epoch 20, Train Loss: 3.3900, Val Loss: 2.8738, Top1 Acc: 0.3784, Top5 Acc: 0.6616, LR 0.000248\n",
      "Saved new best model.\n",
      "Epoch 21, Train Loss: 3.3221, Val Loss: 2.8554, Top1 Acc: 0.3861, Top5 Acc: 0.6647, LR 0.000248\n",
      "Saved new best model.\n",
      "Epoch 22, Train Loss: 3.2782, Val Loss: 2.8663, Top1 Acc: 0.3902, Top5 Acc: 0.6669, LR 0.000248\n",
      "Saved new best model.\n",
      "Epoch 23, Train Loss: 3.2578, Val Loss: 2.7539, Top1 Acc: 0.4151, Top5 Acc: 0.6878, LR 0.000247\n",
      "Epoch 24, Train Loss: 3.2138, Val Loss: 2.7658, Top1 Acc: 0.4101, Top5 Acc: 0.6805, LR 0.000247\n",
      "Saved new best model.\n",
      "Epoch 25, Train Loss: 3.1504, Val Loss: 2.7170, Top1 Acc: 0.4182, Top5 Acc: 0.6921, LR 0.000246\n",
      "Saved new best model.\n",
      "Epoch 26, Train Loss: 3.1427, Val Loss: 2.7212, Top1 Acc: 0.4201, Top5 Acc: 0.6937, LR 0.000246\n",
      "Saved new best model.\n",
      "Epoch 27, Train Loss: 3.0939, Val Loss: 2.6729, Top1 Acc: 0.4344, Top5 Acc: 0.6996, LR 0.000245\n",
      "Saved new best model.\n",
      "Epoch 28, Train Loss: 3.0619, Val Loss: 2.6516, Top1 Acc: 0.4348, Top5 Acc: 0.7094, LR 0.000245\n",
      "Saved new best model.\n",
      "Epoch 29, Train Loss: 3.0375, Val Loss: 2.6571, Top1 Acc: 0.4407, Top5 Acc: 0.7048, LR 0.000244\n",
      "Epoch 30, Train Loss: 2.9983, Val Loss: 2.6478, Top1 Acc: 0.4379, Top5 Acc: 0.7089, LR 0.000243\n",
      "Saved new best model.\n",
      "Epoch 31, Train Loss: 2.9296, Val Loss: 2.6164, Top1 Acc: 0.4428, Top5 Acc: 0.7167, LR 0.000243\n",
      "Saved new best model.\n",
      "Epoch 32, Train Loss: 2.9302, Val Loss: 2.6215, Top1 Acc: 0.4446, Top5 Acc: 0.7136, LR 0.000242\n",
      "Saved new best model.\n",
      "Epoch 33, Train Loss: 2.8941, Val Loss: 2.5706, Top1 Acc: 0.4599, Top5 Acc: 0.7244, LR 0.000241\n",
      "Epoch 34, Train Loss: 2.8162, Val Loss: 2.5966, Top1 Acc: 0.4520, Top5 Acc: 0.7198, LR 0.000240\n",
      "Epoch 35, Train Loss: 2.8040, Val Loss: 2.5984, Top1 Acc: 0.4520, Top5 Acc: 0.7191, LR 0.000240\n",
      "Epoch 36, Train Loss: 2.7333, Val Loss: 2.5922, Top1 Acc: 0.4535, Top5 Acc: 0.7241, LR 0.000239\n",
      "Saved new best model.\n",
      "Epoch 37, Train Loss: 2.7436, Val Loss: 2.5633, Top1 Acc: 0.4625, Top5 Acc: 0.7300, LR 0.000238\n",
      "Epoch 38, Train Loss: 2.7339, Val Loss: 2.5645, Top1 Acc: 0.4604, Top5 Acc: 0.7284, LR 0.000237\n",
      "Saved new best model.\n",
      "Epoch 39, Train Loss: 2.7012, Val Loss: 2.5628, Top1 Acc: 0.4662, Top5 Acc: 0.7320, LR 0.000236\n",
      "Saved new best model.\n",
      "Epoch 40, Train Loss: 2.6581, Val Loss: 2.5626, Top1 Acc: 0.4686, Top5 Acc: 0.7277, LR 0.000235\n",
      "Saved new best model.\n",
      "Epoch 41, Train Loss: 2.6540, Val Loss: 2.5310, Top1 Acc: 0.4761, Top5 Acc: 0.7322, LR 0.000234\n",
      "Epoch 42, Train Loss: 2.5861, Val Loss: 2.6163, Top1 Acc: 0.4560, Top5 Acc: 0.7225, LR 0.000233\n",
      "Epoch 43, Train Loss: 2.5477, Val Loss: 2.5837, Top1 Acc: 0.4644, Top5 Acc: 0.7244, LR 0.000232\n",
      "Epoch 44, Train Loss: 2.5014, Val Loss: 2.5592, Top1 Acc: 0.4723, Top5 Acc: 0.7297, LR 0.000231\n",
      "Epoch 45, Train Loss: 2.4879, Val Loss: 2.5844, Top1 Acc: 0.4747, Top5 Acc: 0.7307, LR 0.000230\n",
      "Saved new best model.\n",
      "Epoch 46, Train Loss: 2.4859, Val Loss: 2.5301, Top1 Acc: 0.4804, Top5 Acc: 0.7330, LR 0.000229\n",
      "Epoch 47, Train Loss: 2.4709, Val Loss: 2.5388, Top1 Acc: 0.4779, Top5 Acc: 0.7386, LR 0.000228\n",
      "Saved new best model.\n",
      "Epoch 48, Train Loss: 2.4684, Val Loss: 2.5434, Top1 Acc: 0.4811, Top5 Acc: 0.7377, LR 0.000227\n",
      "Saved new best model.\n",
      "Epoch 49, Train Loss: 2.3910, Val Loss: 2.5345, Top1 Acc: 0.4831, Top5 Acc: 0.7348, LR 0.000225\n",
      "Epoch 50, Train Loss: 2.4021, Val Loss: 2.5941, Top1 Acc: 0.4751, Top5 Acc: 0.7375, LR 0.000224\n",
      "Epoch 51, Train Loss: 2.3609, Val Loss: 2.5891, Top1 Acc: 0.4734, Top5 Acc: 0.7304, LR 0.000223\n",
      "Saved new best model.\n",
      "Epoch 52, Train Loss: 2.3529, Val Loss: 2.5766, Top1 Acc: 0.4853, Top5 Acc: 0.7390, LR 0.000222\n",
      "Saved new best model.\n",
      "Epoch 53, Train Loss: 2.3377, Val Loss: 2.5414, Top1 Acc: 0.4893, Top5 Acc: 0.7355, LR 0.000220\n",
      "Epoch 54, Train Loss: 2.2619, Val Loss: 2.5794, Top1 Acc: 0.4882, Top5 Acc: 0.7370, LR 0.000219\n",
      "Epoch 55, Train Loss: 2.2588, Val Loss: 2.5700, Top1 Acc: 0.4848, Top5 Acc: 0.7383, LR 0.000218\n",
      "Epoch 56, Train Loss: 2.2436, Val Loss: 2.5546, Top1 Acc: 0.4860, Top5 Acc: 0.7349, LR 0.000216\n",
      "Saved new best model.\n",
      "Epoch 57, Train Loss: 2.2669, Val Loss: 2.5513, Top1 Acc: 0.4919, Top5 Acc: 0.7357, LR 0.000215\n",
      "Epoch 58, Train Loss: 2.2520, Val Loss: 2.6283, Top1 Acc: 0.4822, Top5 Acc: 0.7279, LR 0.000213\n",
      "Epoch 59, Train Loss: 2.2307, Val Loss: 2.6137, Top1 Acc: 0.4887, Top5 Acc: 0.7353, LR 0.000212\n",
      "Epoch 60, Train Loss: 2.2178, Val Loss: 2.5925, Top1 Acc: 0.4890, Top5 Acc: 0.7347, LR 0.000210\n",
      "Epoch 61, Train Loss: 2.1466, Val Loss: 2.5792, Top1 Acc: 0.4905, Top5 Acc: 0.7362, LR 0.000209\n",
      "Epoch 62, Train Loss: 2.1452, Val Loss: 2.6151, Top1 Acc: 0.4880, Top5 Acc: 0.7326, LR 0.000207\n",
      "Epoch 63, Train Loss: 2.1489, Val Loss: 2.6198, Top1 Acc: 0.4918, Top5 Acc: 0.7303, LR 0.000206\n",
      "Epoch 64, Train Loss: 2.1258, Val Loss: 2.6544, Top1 Acc: 0.4822, Top5 Acc: 0.7327, LR 0.000204\n",
      "Epoch 65, Train Loss: 2.0975, Val Loss: 2.6889, Top1 Acc: 0.4801, Top5 Acc: 0.7263, LR 0.000203\n",
      "Epoch 66, Train Loss: 2.0968, Val Loss: 2.6230, Top1 Acc: 0.4858, Top5 Acc: 0.7358, LR 0.000201\n",
      "Saved new best model.\n",
      "Epoch 67, Train Loss: 2.0925, Val Loss: 2.5769, Top1 Acc: 0.4942, Top5 Acc: 0.7337, LR 0.000200\n",
      "Epoch 68, Train Loss: 2.0929, Val Loss: 2.6114, Top1 Acc: 0.4941, Top5 Acc: 0.7370, LR 0.000198\n",
      "Epoch 69, Train Loss: 2.0724, Val Loss: 2.6372, Top1 Acc: 0.4871, Top5 Acc: 0.7316, LR 0.000196\n",
      "Epoch 70, Train Loss: 2.0436, Val Loss: 2.5777, Top1 Acc: 0.4937, Top5 Acc: 0.7373, LR 0.000195\n",
      "Epoch 71, Train Loss: 1.9895, Val Loss: 2.6603, Top1 Acc: 0.4931, Top5 Acc: 0.7330, LR 0.000193\n",
      "Epoch 72, Train Loss: 2.0069, Val Loss: 2.6009, Top1 Acc: 0.4876, Top5 Acc: 0.7342, LR 0.000191\n",
      "Epoch 73, Train Loss: 2.0176, Val Loss: 2.6827, Top1 Acc: 0.4883, Top5 Acc: 0.7332, LR 0.000189\n",
      "Epoch 74, Train Loss: 1.9684, Val Loss: 2.6422, Top1 Acc: 0.4855, Top5 Acc: 0.7317, LR 0.000188\n",
      "Epoch 75, Train Loss: 1.9594, Val Loss: 2.6039, Top1 Acc: 0.4917, Top5 Acc: 0.7327, LR 0.000186\n",
      "Saved new best model.\n",
      "Epoch 76, Train Loss: 1.9667, Val Loss: 2.6236, Top1 Acc: 0.4983, Top5 Acc: 0.7386, LR 0.000184\n",
      "Epoch 77, Train Loss: 1.9260, Val Loss: 2.6149, Top1 Acc: 0.4961, Top5 Acc: 0.7314, LR 0.000182\n",
      "Epoch 78, Train Loss: 1.9103, Val Loss: 2.6278, Top1 Acc: 0.4926, Top5 Acc: 0.7359, LR 0.000180\n",
      "Saved new best model.\n",
      "Epoch 79, Train Loss: 1.9009, Val Loss: 2.6476, Top1 Acc: 0.4987, Top5 Acc: 0.7373, LR 0.000179\n",
      "Epoch 80, Train Loss: 1.8977, Val Loss: 2.6546, Top1 Acc: 0.4965, Top5 Acc: 0.7343, LR 0.000177\n",
      "Epoch 81, Train Loss: 1.9182, Val Loss: 2.6554, Top1 Acc: 0.4916, Top5 Acc: 0.7342, LR 0.000175\n",
      "Epoch 82, Train Loss: 1.8674, Val Loss: 2.6313, Top1 Acc: 0.4964, Top5 Acc: 0.7303, LR 0.000173\n",
      "Epoch 83, Train Loss: 1.8710, Val Loss: 2.6466, Top1 Acc: 0.4946, Top5 Acc: 0.7329, LR 0.000171\n",
      "Epoch 84, Train Loss: 1.8491, Val Loss: 2.5959, Top1 Acc: 0.4981, Top5 Acc: 0.7386, LR 0.000169\n",
      "Epoch 85, Train Loss: 1.7701, Val Loss: 2.6447, Top1 Acc: 0.4952, Top5 Acc: 0.7302, LR 0.000167\n",
      "Epoch 86, Train Loss: 1.8374, Val Loss: 2.6509, Top1 Acc: 0.4924, Top5 Acc: 0.7318, LR 0.000165\n",
      "Saved new best model.\n",
      "Epoch 87, Train Loss: 1.7964, Val Loss: 2.6067, Top1 Acc: 0.5021, Top5 Acc: 0.7387, LR 0.000163\n",
      "Epoch 88, Train Loss: 1.7892, Val Loss: 2.6231, Top1 Acc: 0.4985, Top5 Acc: 0.7363, LR 0.000161\n",
      "Saved new best model.\n",
      "Epoch 89, Train Loss: 1.8083, Val Loss: 2.5857, Top1 Acc: 0.5071, Top5 Acc: 0.7352, LR 0.000160\n",
      "Epoch 90, Train Loss: 1.7837, Val Loss: 2.6506, Top1 Acc: 0.4993, Top5 Acc: 0.7338, LR 0.000158\n",
      "Epoch 91, Train Loss: 1.7520, Val Loss: 2.6176, Top1 Acc: 0.5016, Top5 Acc: 0.7350, LR 0.000156\n",
      "Epoch 92, Train Loss: 1.7603, Val Loss: 2.6179, Top1 Acc: 0.5061, Top5 Acc: 0.7343, LR 0.000154\n",
      "Saved new best model.\n",
      "Epoch 93, Train Loss: 1.7541, Val Loss: 2.5982, Top1 Acc: 0.5082, Top5 Acc: 0.7369, LR 0.000152\n",
      "Epoch 94, Train Loss: 1.7276, Val Loss: 2.6539, Top1 Acc: 0.5007, Top5 Acc: 0.7306, LR 0.000150\n",
      "Epoch 95, Train Loss: 1.7933, Val Loss: 2.5965, Top1 Acc: 0.5061, Top5 Acc: 0.7360, LR 0.000148\n",
      "Saved new best model.\n",
      "Epoch 96, Train Loss: 1.7430, Val Loss: 2.6110, Top1 Acc: 0.5086, Top5 Acc: 0.7366, LR 0.000146\n",
      "Epoch 97, Train Loss: 1.7363, Val Loss: 2.5758, Top1 Acc: 0.5068, Top5 Acc: 0.7394, LR 0.000144\n",
      "Epoch 98, Train Loss: 1.7327, Val Loss: 2.6168, Top1 Acc: 0.5018, Top5 Acc: 0.7287, LR 0.000142\n",
      "Epoch 99, Train Loss: 1.6507, Val Loss: 2.6095, Top1 Acc: 0.5051, Top5 Acc: 0.7319, LR 0.000140\n",
      "Epoch 100, Train Loss: 1.6888, Val Loss: 2.6370, Top1 Acc: 0.5067, Top5 Acc: 0.7354, LR 0.000138\n",
      "Saved new best model.\n",
      "Epoch 101, Train Loss: 1.6850, Val Loss: 2.6215, Top1 Acc: 0.5117, Top5 Acc: 0.7420, LR 0.000136\n",
      "Epoch 102, Train Loss: 1.6801, Val Loss: 2.5947, Top1 Acc: 0.5074, Top5 Acc: 0.7363, LR 0.000134\n",
      "Epoch 103, Train Loss: 1.6816, Val Loss: 2.5989, Top1 Acc: 0.5104, Top5 Acc: 0.7376, LR 0.000132\n",
      "Saved new best model.\n",
      "Epoch 104, Train Loss: 1.6315, Val Loss: 2.5725, Top1 Acc: 0.5137, Top5 Acc: 0.7445, LR 0.000130\n",
      "Epoch 105, Train Loss: 1.6443, Val Loss: 2.5923, Top1 Acc: 0.5101, Top5 Acc: 0.7401, LR 0.000127\n",
      "Saved new best model.\n",
      "Epoch 106, Train Loss: 1.6300, Val Loss: 2.5548, Top1 Acc: 0.5151, Top5 Acc: 0.7373, LR 0.000125\n",
      "Epoch 107, Train Loss: 1.6593, Val Loss: 2.5589, Top1 Acc: 0.5146, Top5 Acc: 0.7455, LR 0.000123\n",
      "Saved new best model.\n",
      "Epoch 108, Train Loss: 1.6305, Val Loss: 2.5425, Top1 Acc: 0.5152, Top5 Acc: 0.7445, LR 0.000121\n",
      "Saved new best model.\n",
      "Epoch 109, Train Loss: 1.6014, Val Loss: 2.5582, Top1 Acc: 0.5153, Top5 Acc: 0.7419, LR 0.000119\n",
      "Epoch 110, Train Loss: 1.6147, Val Loss: 2.5929, Top1 Acc: 0.5082, Top5 Acc: 0.7342, LR 0.000117\n",
      "Saved new best model.\n",
      "Epoch 111, Train Loss: 1.5978, Val Loss: 2.5264, Top1 Acc: 0.5168, Top5 Acc: 0.7456, LR 0.000115\n",
      "Epoch 112, Train Loss: 1.6259, Val Loss: 2.5773, Top1 Acc: 0.5131, Top5 Acc: 0.7397, LR 0.000113\n",
      "Epoch 113, Train Loss: 1.5827, Val Loss: 2.5426, Top1 Acc: 0.5135, Top5 Acc: 0.7404, LR 0.000111\n",
      "Epoch 114, Train Loss: 1.5785, Val Loss: 2.5494, Top1 Acc: 0.5135, Top5 Acc: 0.7356, LR 0.000109\n",
      "Saved new best model.\n",
      "Epoch 115, Train Loss: 1.5789, Val Loss: 2.5530, Top1 Acc: 0.5178, Top5 Acc: 0.7398, LR 0.000107\n",
      "Epoch 116, Train Loss: 1.6054, Val Loss: 2.5349, Top1 Acc: 0.5173, Top5 Acc: 0.7428, LR 0.000105\n",
      "Saved new best model.\n",
      "Epoch 117, Train Loss: 1.5773, Val Loss: 2.5347, Top1 Acc: 0.5204, Top5 Acc: 0.7449, LR 0.000103\n",
      "Saved new best model.\n",
      "Epoch 118, Train Loss: 1.5453, Val Loss: 2.5189, Top1 Acc: 0.5221, Top5 Acc: 0.7468, LR 0.000101\n",
      "Epoch 119, Train Loss: 1.5329, Val Loss: 2.5380, Top1 Acc: 0.5184, Top5 Acc: 0.7451, LR 0.000099\n",
      "Epoch 120, Train Loss: 1.5524, Val Loss: 2.5277, Top1 Acc: 0.5204, Top5 Acc: 0.7402, LR 0.000097\n",
      "Epoch 121, Train Loss: 1.5525, Val Loss: 2.5064, Top1 Acc: 0.5202, Top5 Acc: 0.7416, LR 0.000095\n",
      "Epoch 122, Train Loss: 1.5146, Val Loss: 2.5026, Top1 Acc: 0.5200, Top5 Acc: 0.7402, LR 0.000094\n",
      "Saved new best model.\n",
      "Epoch 123, Train Loss: 1.5656, Val Loss: 2.4946, Top1 Acc: 0.5245, Top5 Acc: 0.7461, LR 0.000092\n",
      "Saved new best model.\n",
      "Epoch 124, Train Loss: 1.5418, Val Loss: 2.4930, Top1 Acc: 0.5262, Top5 Acc: 0.7461, LR 0.000090\n",
      "Epoch 125, Train Loss: 1.4890, Val Loss: 2.5074, Top1 Acc: 0.5246, Top5 Acc: 0.7404, LR 0.000088\n",
      "Saved new best model.\n",
      "Epoch 126, Train Loss: 1.5100, Val Loss: 2.4983, Top1 Acc: 0.5268, Top5 Acc: 0.7428, LR 0.000086\n",
      "Saved new best model.\n",
      "Epoch 127, Train Loss: 1.4774, Val Loss: 2.4858, Top1 Acc: 0.5271, Top5 Acc: 0.7494, LR 0.000084\n",
      "Saved new best model.\n",
      "Epoch 128, Train Loss: 1.5544, Val Loss: 2.4760, Top1 Acc: 0.5273, Top5 Acc: 0.7484, LR 0.000082\n",
      "Epoch 129, Train Loss: 1.4636, Val Loss: 2.5114, Top1 Acc: 0.5230, Top5 Acc: 0.7402, LR 0.000080\n",
      "Epoch 130, Train Loss: 1.5405, Val Loss: 2.4670, Top1 Acc: 0.5268, Top5 Acc: 0.7466, LR 0.000078\n",
      "Epoch 131, Train Loss: 1.4480, Val Loss: 2.4920, Top1 Acc: 0.5235, Top5 Acc: 0.7425, LR 0.000076\n",
      "Epoch 132, Train Loss: 1.4872, Val Loss: 2.4905, Top1 Acc: 0.5234, Top5 Acc: 0.7440, LR 0.000075\n",
      "Saved new best model.\n",
      "Epoch 133, Train Loss: 1.4684, Val Loss: 2.4739, Top1 Acc: 0.5307, Top5 Acc: 0.7490, LR 0.000073\n",
      "Epoch 134, Train Loss: 1.4505, Val Loss: 2.4593, Top1 Acc: 0.5302, Top5 Acc: 0.7483, LR 0.000071\n",
      "Epoch 135, Train Loss: 1.4688, Val Loss: 2.4619, Top1 Acc: 0.5302, Top5 Acc: 0.7450, LR 0.000069\n",
      "Epoch 136, Train Loss: 1.4195, Val Loss: 2.4785, Top1 Acc: 0.5280, Top5 Acc: 0.7425, LR 0.000067\n",
      "Epoch 137, Train Loss: 1.4532, Val Loss: 2.4706, Top1 Acc: 0.5302, Top5 Acc: 0.7453, LR 0.000066\n",
      "Epoch 138, Train Loss: 1.4321, Val Loss: 2.4583, Top1 Acc: 0.5298, Top5 Acc: 0.7481, LR 0.000064\n",
      "Saved new best model.\n",
      "Epoch 139, Train Loss: 1.4216, Val Loss: 2.4484, Top1 Acc: 0.5318, Top5 Acc: 0.7476, LR 0.000062\n",
      "Epoch 140, Train Loss: 1.4442, Val Loss: 2.4449, Top1 Acc: 0.5304, Top5 Acc: 0.7491, LR 0.000060\n",
      "Saved new best model.\n",
      "Epoch 141, Train Loss: 1.4553, Val Loss: 2.4538, Top1 Acc: 0.5379, Top5 Acc: 0.7474, LR 0.000059\n",
      "Epoch 142, Train Loss: 1.4402, Val Loss: 2.4485, Top1 Acc: 0.5352, Top5 Acc: 0.7477, LR 0.000057\n",
      "Epoch 143, Train Loss: 1.3845, Val Loss: 2.4562, Top1 Acc: 0.5369, Top5 Acc: 0.7502, LR 0.000055\n",
      "Saved new best model.\n",
      "Epoch 144, Train Loss: 1.3603, Val Loss: 2.4409, Top1 Acc: 0.5392, Top5 Acc: 0.7472, LR 0.000054\n",
      "Epoch 145, Train Loss: 1.4193, Val Loss: 2.4300, Top1 Acc: 0.5372, Top5 Acc: 0.7506, LR 0.000052\n",
      "Epoch 146, Train Loss: 1.3744, Val Loss: 2.4201, Top1 Acc: 0.5380, Top5 Acc: 0.7530, LR 0.000051\n",
      "Saved new best model.\n",
      "Epoch 147, Train Loss: 1.3648, Val Loss: 2.4247, Top1 Acc: 0.5415, Top5 Acc: 0.7522, LR 0.000049\n",
      "Epoch 148, Train Loss: 1.3908, Val Loss: 2.4071, Top1 Acc: 0.5407, Top5 Acc: 0.7548, LR 0.000048\n",
      "Epoch 149, Train Loss: 1.4203, Val Loss: 2.4165, Top1 Acc: 0.5406, Top5 Acc: 0.7545, LR 0.000046\n",
      "Epoch 150, Train Loss: 1.3789, Val Loss: 2.4225, Top1 Acc: 0.5383, Top5 Acc: 0.7535, LR 0.000045\n",
      "Saved new best model.\n",
      "Epoch 151, Train Loss: 1.4217, Val Loss: 2.4097, Top1 Acc: 0.5451, Top5 Acc: 0.7558, LR 0.000043\n",
      "Epoch 152, Train Loss: 1.3263, Val Loss: 2.4186, Top1 Acc: 0.5392, Top5 Acc: 0.7528, LR 0.000042\n",
      "Epoch 153, Train Loss: 1.3541, Val Loss: 2.4142, Top1 Acc: 0.5410, Top5 Acc: 0.7497, LR 0.000040\n",
      "Epoch 154, Train Loss: 1.3661, Val Loss: 2.4080, Top1 Acc: 0.5401, Top5 Acc: 0.7521, LR 0.000039\n",
      "Epoch 155, Train Loss: 1.3813, Val Loss: 2.4014, Top1 Acc: 0.5417, Top5 Acc: 0.7542, LR 0.000037\n",
      "Epoch 156, Train Loss: 1.3619, Val Loss: 2.4063, Top1 Acc: 0.5398, Top5 Acc: 0.7519, LR 0.000036\n",
      "Saved new best model.\n",
      "Epoch 157, Train Loss: 1.3973, Val Loss: 2.4006, Top1 Acc: 0.5462, Top5 Acc: 0.7543, LR 0.000035\n",
      "Epoch 158, Train Loss: 1.3559, Val Loss: 2.3934, Top1 Acc: 0.5445, Top5 Acc: 0.7569, LR 0.000033\n",
      "Epoch 159, Train Loss: 1.3473, Val Loss: 2.3943, Top1 Acc: 0.5449, Top5 Acc: 0.7562, LR 0.000032\n",
      "Epoch 160, Train Loss: 1.3393, Val Loss: 2.3945, Top1 Acc: 0.5452, Top5 Acc: 0.7551, LR 0.000031\n",
      "Saved new best model.\n",
      "Epoch 161, Train Loss: 1.3349, Val Loss: 2.3891, Top1 Acc: 0.5484, Top5 Acc: 0.7570, LR 0.000030\n",
      "Saved new best model.\n",
      "Epoch 162, Train Loss: 1.3341, Val Loss: 2.3835, Top1 Acc: 0.5520, Top5 Acc: 0.7593, LR 0.000028\n",
      "Epoch 163, Train Loss: 1.3765, Val Loss: 2.3865, Top1 Acc: 0.5491, Top5 Acc: 0.7596, LR 0.000027\n",
      "Epoch 164, Train Loss: 1.3163, Val Loss: 2.3905, Top1 Acc: 0.5485, Top5 Acc: 0.7603, LR 0.000026\n",
      "Epoch 165, Train Loss: 1.3619, Val Loss: 2.3875, Top1 Acc: 0.5474, Top5 Acc: 0.7574, LR 0.000025\n",
      "Epoch 166, Train Loss: 1.2963, Val Loss: 2.3770, Top1 Acc: 0.5512, Top5 Acc: 0.7599, LR 0.000024\n",
      "Epoch 167, Train Loss: 1.3388, Val Loss: 2.3853, Top1 Acc: 0.5519, Top5 Acc: 0.7610, LR 0.000023\n",
      "Epoch 168, Train Loss: 1.3262, Val Loss: 2.3759, Top1 Acc: 0.5519, Top5 Acc: 0.7593, LR 0.000022\n",
      "Saved new best model.\n",
      "Epoch 169, Train Loss: 1.3189, Val Loss: 2.3770, Top1 Acc: 0.5523, Top5 Acc: 0.7622, LR 0.000021\n",
      "Saved new best model.\n",
      "Epoch 170, Train Loss: 1.3156, Val Loss: 2.3749, Top1 Acc: 0.5539, Top5 Acc: 0.7605, LR 0.000020\n",
      "Epoch 171, Train Loss: 1.3014, Val Loss: 2.3787, Top1 Acc: 0.5531, Top5 Acc: 0.7614, LR 0.000019\n",
      "Epoch 172, Train Loss: 1.3099, Val Loss: 2.3739, Top1 Acc: 0.5529, Top5 Acc: 0.7618, LR 0.000018\n",
      "Epoch 173, Train Loss: 1.3211, Val Loss: 2.3750, Top1 Acc: 0.5511, Top5 Acc: 0.7615, LR 0.000017\n",
      "Epoch 174, Train Loss: 1.3137, Val Loss: 2.3709, Top1 Acc: 0.5537, Top5 Acc: 0.7613, LR 0.000016\n",
      "Epoch 175, Train Loss: 1.2725, Val Loss: 2.3637, Top1 Acc: 0.5523, Top5 Acc: 0.7633, LR 0.000015\n",
      "Epoch 176, Train Loss: 1.2913, Val Loss: 2.3745, Top1 Acc: 0.5508, Top5 Acc: 0.7623, LR 0.000015\n",
      "Epoch 177, Train Loss: 1.3196, Val Loss: 2.3703, Top1 Acc: 0.5521, Top5 Acc: 0.7630, LR 0.000014\n",
      "Saved new best model.\n",
      "Epoch 178, Train Loss: 1.2998, Val Loss: 2.3630, Top1 Acc: 0.5559, Top5 Acc: 0.7631, LR 0.000013\n",
      "Epoch 179, Train Loss: 1.3068, Val Loss: 2.3812, Top1 Acc: 0.5514, Top5 Acc: 0.7608, LR 0.000012\n",
      "Epoch 180, Train Loss: 1.2638, Val Loss: 2.3696, Top1 Acc: 0.5519, Top5 Acc: 0.7627, LR 0.000012\n",
      "Epoch 181, Train Loss: 1.2963, Val Loss: 2.3732, Top1 Acc: 0.5535, Top5 Acc: 0.7626, LR 0.000011\n",
      "Epoch 182, Train Loss: 1.2822, Val Loss: 2.3646, Top1 Acc: 0.5542, Top5 Acc: 0.7605, LR 0.000010\n",
      "Epoch 183, Train Loss: 1.2317, Val Loss: 2.3659, Top1 Acc: 0.5530, Top5 Acc: 0.7621, LR 0.000010\n",
      "Epoch 184, Train Loss: 1.2970, Val Loss: 2.3631, Top1 Acc: 0.5558, Top5 Acc: 0.7611, LR 0.000009\n",
      "Saved new best model.\n",
      "Epoch 185, Train Loss: 1.2307, Val Loss: 2.3653, Top1 Acc: 0.5563, Top5 Acc: 0.7634, LR 0.000009\n",
      "Epoch 186, Train Loss: 1.2503, Val Loss: 2.3661, Top1 Acc: 0.5546, Top5 Acc: 0.7640, LR 0.000008\n",
      "Epoch 187, Train Loss: 1.3155, Val Loss: 2.3650, Top1 Acc: 0.5536, Top5 Acc: 0.7646, LR 0.000008\n",
      "Epoch 188, Train Loss: 1.2425, Val Loss: 2.3588, Top1 Acc: 0.5550, Top5 Acc: 0.7670, LR 0.000007\n",
      "Epoch 189, Train Loss: 1.3001, Val Loss: 2.3650, Top1 Acc: 0.5528, Top5 Acc: 0.7653, LR 0.000007\n",
      "Saved new best model.\n",
      "Epoch 190, Train Loss: 1.2935, Val Loss: 2.3578, Top1 Acc: 0.5575, Top5 Acc: 0.7664, LR 0.000007\n",
      "Epoch 191, Train Loss: 0.5824, Val Loss: 2.3821, Top1 Acc: 0.5571, Top5 Acc: 0.7639, LR 0.000006\n",
      "Saved new best model.\n",
      "Epoch 192, Train Loss: 0.5822, Val Loss: 2.3798, Top1 Acc: 0.5592, Top5 Acc: 0.7653, LR 0.000006\n",
      "Epoch 193, Train Loss: 0.5810, Val Loss: 2.3820, Top1 Acc: 0.5569, Top5 Acc: 0.7639, LR 0.000006\n",
      "Epoch 194, Train Loss: 0.5796, Val Loss: 2.3785, Top1 Acc: 0.5576, Top5 Acc: 0.7640, LR 0.000006\n",
      "Epoch 195, Train Loss: 0.5804, Val Loss: 2.3816, Top1 Acc: 0.5559, Top5 Acc: 0.7647, LR 0.000005\n",
      "Epoch 196, Train Loss: 0.5794, Val Loss: 2.3784, Top1 Acc: 0.5562, Top5 Acc: 0.7634, LR 0.000005\n",
      "Epoch 197, Train Loss: 0.5775, Val Loss: 2.3811, Top1 Acc: 0.5584, Top5 Acc: 0.7660, LR 0.000005\n",
      "Saved new best model.\n",
      "Epoch 198, Train Loss: 0.5780, Val Loss: 2.3812, Top1 Acc: 0.5617, Top5 Acc: 0.7647, LR 0.000005\n",
      "Epoch 199, Train Loss: 0.5771, Val Loss: 2.3850, Top1 Acc: 0.5574, Top5 Acc: 0.7650, LR 0.000005\n",
      "Epoch 200, Train Loss: 0.5778, Val Loss: 2.3853, Top1 Acc: 0.5587, Top5 Acc: 0.7676, LR 0.000005\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "import json\n",
    "\n",
    "\n",
    "model = SwinTransformer(\n",
    "    img_size=64, patch_size=4, emb_dim=96,\n",
    "    blocks=[2, 2, 6, 2], nheads=[2, 4, 8, 16],\n",
    "    M=[8, 8, 4, 2], n_classes=200, stochastic_endpoint=0.2).to(device)\n",
    "\n",
    "loss_fn = SoftCrossEntropy(label_smoothing=0.05)\n",
    "mixer = MixupCutmix(num_classes=200, mixup_alpha=0.4, cutmix_alpha=0.8,\n",
    "                    p_mixup=0.3, p_cutmix=0.3)\n",
    "# Scheduler Epochs\n",
    "warmup_epochs = 10  # Gradually increase lr for the first 10 epochs\n",
    "cosine_epochs = 190 # Cosine-anneal lr for the remaining 190 epochs\n",
    "\n",
    "def param_groups_weight_decay(model, weight_decay=0.05):\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        name = n.lower()\n",
    "        if (\n",
    "            p.ndim == 1\n",
    "            or n.endswith(\".bias\")\n",
    "            or \"relative\" in name\n",
    "            or \"pos_embed\" in name\n",
    "        ):\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "    return [\n",
    "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "start_lr = 2.5e-4          # 1e-3 * (64/256)\n",
    "eta_min  = 5e-6            # ~2% floor\n",
    "\n",
    "# AdamW optimizer, weight decay taken directly from the paper.\n",
    "# fused=True uses a fused CUDA kernel\n",
    "optimizer = torch.optim.AdamW(param_groups_weight_decay(model, 0.05), lr=start_lr, betas=(0.9, 0.999), fused=True)\n",
    "\n",
    "# Linear warmup scheduler. Start at 1% of lr and gradually fo up to 100%\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs)\n",
    "# Cosine annealing scheduler. Decay lr following a cosine curve\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=cosine_epochs, eta_min=eta_min)\n",
    "# Run warmup_scheduler first, then cosine_scheduler\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "# GradScaler for Automatic Mixed Precision (AMP). Saves VRAM.\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
    "\n",
    "best_top1 = 0.0\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_top1\": [],\n",
    "    \"val_top5\": [],\n",
    "    \"lr\": []\n",
    "}\n",
    "\n",
    "for epoch in range(warmup_epochs + cosine_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    if epoch < 1 or epoch >= warmup_epochs + cosine_epochs - 10:\n",
    "        mixer.off()\n",
    "    else:\n",
    "        mixer.on()\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        xb, yb = mixer(xb, yb)  # labels now (N) or (N,C)\n",
    "        optimizer.zero_grad(set_to_none=True) # set_to_none saves a bit of memory.\n",
    "        # Forward and loss in mixed precision\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            yhat = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "        # Backprop with scaled loss\n",
    "        scaler.scale(loss).backward()\n",
    "        # unscale before clipping, then clip\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
    "        # Step optimizer only goes if gradients are finite\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n_samples += bs\n",
    "\n",
    "    # Safe averages\n",
    "    avg_loss = total_loss / n_samples\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_samples = 0\n",
    "    val_correct_top1 = 0\n",
    "    val_correct_top5 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "                yhat = model(xb)\n",
    "                loss = loss_fn(yhat, yb)\n",
    "            bs = xb.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            val_samples += bs\n",
    "            \n",
    "            topk = torch.topk(yhat, k=5, dim=1).indices  # (B,5)\n",
    "            val_correct_top1 += (topk[:, 0] == yb).sum().item()\n",
    "            val_correct_top5 += topk.eq(yb.view(-1, 1)).any(dim=1).sum().item()\n",
    "            \n",
    "    avg_val_loss = val_loss / val_samples\n",
    "    val_top1 = val_correct_top1 / val_samples\n",
    "    val_top5 = val_correct_top5 / val_samples\n",
    "    \n",
    "    scheduler.step()\n",
    "    # Get current learning rate. I had this wrong before, I was grabbing the past lr instead of current\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_top1\"].append(val_top1)\n",
    "    history[\"val_top5\"].append(val_top5)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "\n",
    "    with open(\"training_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "    if val_top1 > best_top1:\n",
    "        best_top1 = val_top1\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val_top1\": best_top1,\n",
    "            \"best_val_top5\": val_top5\n",
    "        }, \"swin_t_best.pt\")\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Top1 Acc: {val_top1:.4f}, Top5 Acc: {val_top5:.4f}, LR {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7e0dc",
   "metadata": {
    "papermill": {
     "duration": 0.012014,
     "end_time": "2025-08-18T08:47:37.569371",
     "exception": false,
     "start_time": "2025-08-18T08:47:37.557357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tiny-ImageNet @ 64×64 Notes, Configs & Results\n",
    "\n",
    "## Initial problems that were fixed later on\n",
    "- ### Augmentations too strong at 64×64\n",
    "    - RandAug(mag=9) + RandomResizedCrop(0.8,1.0) + RandomErasing(p=0.25) strip too much \n",
    "signal on tiny images. Train loss goes down but validation loss will not go higher.\n",
    "\n",
    "- ### Relative Position Bias (RPB) was decayed\n",
    "    - relative_table sat in the weight-decay group before I fixed it.  \n",
    "    - Slowly eroded locality info.\n",
    "\n",
    "- ### Validation split\n",
    "    - Earlier runs split the train set for validation instead of using the official val set.\n",
    "    - Not stratified.\n",
    "\n",
    "Swin's are very good with large amounts of data, but when you only have 100k images with 64x64 resolution, it does not scale nearly as well as it should scale. I could train on ImageNets 1000k images, but I do not have the computational resources to train a model that large. I think Cosine Annealing caused the learning rate to drop a bit too fast. Adding AMP, pinning memory, using fused AdamW, and a few other tweaks caused training speed to increase by around 5x.\n",
    "\n",
    "## Models\n",
    "\n",
    "Two of the models I noted down with different block hyperparameters. Other models were not included due to them not showing much information other then minor hyperparameter tweaks, or I interrupted training partway through due to unfavorable results.\n",
    "\n",
    "### [2, 2, 4, 2]\n",
    "```python\n",
    "model = SwinTransformer(\n",
    "    img_size=64, patch_size=4, emb_dim=64,\n",
    "    blocks=[2, 2, 4, 2], nheads=[2, 4, 8, 16],\n",
    "    M=[8, 8, 4, 2], n_classes=200, stochastic_endpoint=0.1).to(device)\n",
    "```\n",
    "- Results: Best Epoch: Epoch 92, Train Loss: 0.5086, Val Loss: 2.0378, Top1 Acc: 0.5295, Top5 Acc: 0.7681, LR 0.000005\n",
    "- History: [2, 2, 4, 2].txt\n",
    "- Used 5 warmup epochs with Linear scaling.\n",
    "- Used 95 Cosine Annealing epochs.\n",
    "- torch.optim.AdamW(param_groups_weight_decay(model, 0.05), lr=3e-4, betas=(0.9, 0.999), fused=True)\n",
    "- Used GradScaler for AMP.\n",
    "Augmentations:\n",
    "```python\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandAugment(num_ops=2, magnitude=5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random'),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "])\n",
    "\n",
    "mixer = MixupCutmix(num_classes=200, mixup_alpha=0.8, cutmix_alpha=1.0,\n",
    "                    p_mixup=0.5, p_cutmix=0.5)\n",
    "```\n",
    "\n",
    "![4BlockLoss](figures/4blockLoss.png)\n",
    "![4blockAcc](figures/4blockAcc.png)\n",
    "\n",
    "\n",
    "### [2, 2, 6, 2]\n",
    "```python\n",
    "model = SwinTransformer(\n",
    "    img_size=64, patch_size=4, emb_dim=96,\n",
    "    blocks=[2, 2, 6, 2], nheads=[2, 4, 8, 16],\n",
    "    M=[8, 8, 4, 2], n_classes=200, stochastic_endpoint=0.2).to(device)\n",
    "```\n",
    "- Results: Best Epoch: Epoch 198, Train Loss: 0.5780, Val Loss: 2.3812, Top1 Acc: 0.5617, Top5 Acc: 0.7647, LR 0.000005\n",
    "- History: [2, 2, 6, 2].txt\n",
    "- Used 10 warmup epochs with Linear scaling\n",
    "- Used 190 Cosine Annealing epochs.\n",
    "- torch.optim.AdamW(param_groups_weight_decay(model, 0.05), lr=2.5e-4, betas=(0.9, 0.999), fused=True)\n",
    "\n",
    "Augmentations:\n",
    "```python\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandAugment(num_ops=2, magnitude=5),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.2), value='random'),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "])\n",
    "\n",
    "mixer = MixupCutmix(num_classes=200, mixup_alpha=0.4, cutmix_alpha=0.8,\n",
    "                    p_mixup=0.3, p_cutmix=0.3)\n",
    "```\n",
    "\n",
    "![6BlockLoss](figures/6blockLoss.png)\n",
    "![6blockAcc](figures/6blockAcc.png)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7815483,
     "sourceId": 12394036,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27613.298727,
   "end_time": "2025-08-18T08:47:40.322761",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-18T01:07:27.024034",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
