{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329affdb",
   "metadata": {
    "papermill": {
     "duration": 0.004843,
     "end_time": "2025-09-17T03:19:25.043087",
     "exception": false,
     "start_time": "2025-09-17T03:19:25.038244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Single Shot Multibox Detection\n",
    "\n",
    "I was having difficulties with RandomIoUCrop, It wasn't working great for various reasons. The parameters I had initially: (min_scale=0.6, sampler_options=(0.5, 0.7, 0.9), aspect ratio between 0.5–2.0) didn't work because the pedestrians were too small and skinny to satisfy the IoU constraints. Requiring an IoU >= 0.9 with a crop that covers at least 60% of the image is pretty much impossible when each person covers only 20% of the frame. Because RandomIoUCrop kept trying to find a valid crop, it would get stuck in an infinite loop for some images. I tried multiple things, like giving sampler_option the option to pick 0.0, reducing min_scale, and widening the aspect ratio bounds. That fixed the infinite search, but the resulting crops were garbage, there were images that had people centered with no GT boxes, or there were crops of just background. I don't think RandomIoUCrop is ideal for Penn-Fudan. I still wanted to include cropping, so I asked Chat GPT to create a custom class that centers crops around a GT box.\n",
    "\n",
    "There were many errors once again, when multiple people were present, non-target individuals were often removed if they weren’t sufficiently visible. Sometimes all boxes were dropped, leaving an image of a person but no associated GT box. The fix was to add a safety net in \\__getitem__ which ensured that if all boxes are dropped after augmentation, the image falls back to no cropping, or at least preserves the largest original box. With this, the pipeline avoids infinite loops, keeps at least one valid GT, and has high quality crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f1f14a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:25.052251Z",
     "iopub.status.busy": "2025-09-17T03:19:25.051966Z",
     "iopub.status.idle": "2025-09-17T03:19:35.778569Z",
     "shell.execute_reply": "2025-09-17T03:19:35.777750Z"
    },
    "papermill": {
     "duration": 10.733143,
     "end_time": "2025-09-17T03:19:35.780140",
     "exception": false,
     "start_time": "2025-09-17T03:19:25.046997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2 as T\n",
    "from torch import nn, optim\n",
    "\n",
    "def _area_xyxy(b):\n",
    "    return (b[:,2]-b[:,0]).clamp(0) * (b[:,3]-b[:,1]).clamp(0)\n",
    "\n",
    "def _clip_to_canvas(boxes, H, W):\n",
    "    x1 = boxes[:,0].clamp(0, W-1)\n",
    "    y1 = boxes[:,1].clamp(0, H-1)\n",
    "    x2 = boxes[:,2].clamp(0, W-1)\n",
    "    y2 = boxes[:,3].clamp(0, H-1)\n",
    "    # ensure x2>=x1, y2>=y1 after clamp\n",
    "    x2 = torch.maximum(x2, x1)\n",
    "    y2 = torch.maximum(y2, y1)\n",
    "    return torch.stack([x1,y1,x2,y2], dim=1)\n",
    "\n",
    "class SafeBoxCenteredCrop(nn.Module):\n",
    "    \"\"\"\n",
    "    Object-centric crop around a randomly chosen GT box,\n",
    "    with visibility/AR/scale checks and bounded retries.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_size=(300,300),\n",
    "        context=(1.2, 2.0),           # crop size = context * target box size\n",
    "        scale_img=(0.3, 1.0),         # crop area as fraction of image area (feasibility guard)\n",
    "        ratio=(0.6, 1.8),             # crop aspect ratio bounds (w/h)\n",
    "        max_trials=30,\n",
    "        target_vis_thresh=0.7,        # target box must be at least 70% visible\n",
    "        keep_vis_thresh=0.5,          # other boxes kept if ≥50% visible\n",
    "        antialias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_size = out_size\n",
    "        self.context = context\n",
    "        self.scale_img = scale_img\n",
    "        self.ratio = ratio\n",
    "        self.max_trials = max_trials\n",
    "        self.target_vis_thresh = target_vis_thresh\n",
    "        self.keep_vis_thresh = keep_vis_thresh\n",
    "        self.antialias = antialias\n",
    "\n",
    "    def forward(self, sample):\n",
    "        img: tv_tensors.Image = sample[\"image\"]\n",
    "        boxes: tv_tensors.BoundingBoxes = sample[\"boxes\"]\n",
    "        labels: torch.Tensor = sample[\"labels\"]\n",
    "\n",
    "        C, H, W = img.shape\n",
    "        if boxes.numel() == 0:\n",
    "            # no boxes --> fallback to simple resize\n",
    "            return T.Resize(self.out_size, antialias=self.antialias)(sample)\n",
    "\n",
    "        img_area = float(H * W)\n",
    "        b = boxes.as_subclass(torch.Tensor)  # (N,4) float xyxy\n",
    "        areas = _area_xyxy(b)\n",
    "        # pick a target box proportional to sqrt(area) to bias toward larger persons\n",
    "        weights = (areas.clamp(min=1)).sqrt()\n",
    "        tidx = torch.multinomial(weights, 1).item()\n",
    "\n",
    "        for _ in range(self.max_trials):\n",
    "            tb = b[tidx]  # target box\n",
    "            bw = (tb[2]-tb[0]).item()\n",
    "            bh = (tb[3]-tb[1]).item()\n",
    "            if bw < 1 or bh < 1:\n",
    "                continue\n",
    "\n",
    "            # sample context scale\n",
    "            s = torch.empty(1).uniform_(*self.context).item()\n",
    "            crop_w = max(1.0, s * bw)\n",
    "            crop_h = max(1.0, s * bh)\n",
    "\n",
    "            ar_min, ar_max = self.ratio\n",
    "            # adjust width/height to satisfy aspect ratio softly\n",
    "            ar = crop_w / crop_h\n",
    "            if ar < ar_min:\n",
    "                crop_w = ar_min * crop_h\n",
    "            elif ar > ar_max:\n",
    "                crop_h = crop_w / ar_max\n",
    "\n",
    "            # make sure crop area is not absurd vs image\n",
    "            crop_area = crop_w * crop_h\n",
    "            if not (self.scale_img[0]*img_area <= crop_area <= self.scale_img[1]*img_area):\n",
    "                # rescale to nearest bound\n",
    "                target_area = min(max(crop_area, self.scale_img[0]*img_area), self.scale_img[1]*img_area)\n",
    "                scale = (target_area / (crop_w*crop_h))**0.5\n",
    "                crop_w *= scale\n",
    "                crop_h *= scale\n",
    "\n",
    "            # choose crop center near target box center with some jitter\n",
    "            cx = (tb[0] + tb[2]) / 2\n",
    "            cy = (tb[1] + tb[3]) / 2\n",
    "            jx = (torch.randn(1).item() * 0.15) * bw  # 15% bw jitter\n",
    "            jy = (torch.randn(1).item() * 0.15) * bh  # 15% bh jitter\n",
    "            cx = (cx + jx).clamp(0, W-1)\n",
    "            cy = (cy + jy).clamp(0, H-1)\n",
    "\n",
    "            x1 = (cx - crop_w/2); y1 = (cy - crop_h/2)\n",
    "            x2 = (cx + crop_w/2); y2 = (cy + crop_h/2)\n",
    "\n",
    "            crop = torch.tensor([[x1,y1,x2,y2]], dtype=b.dtype, device=b.device)\n",
    "            crop = _clip_to_canvas(crop, H, W)[0]\n",
    "\n",
    "            # compute visibility of each box inside crop\n",
    "            inter = torch.stack([\n",
    "                torch.maximum(b[:,0], crop[0]),\n",
    "                torch.maximum(b[:,1], crop[1]),\n",
    "                torch.minimum(b[:,2], crop[2]),\n",
    "                torch.minimum(b[:,3], crop[3]),\n",
    "            ], dim=1)\n",
    "            inter = _clip_to_canvas(inter, H, W)\n",
    "            inter_area = _area_xyxy(inter)\n",
    "            box_area = _area_xyxy(b).clamp(min=1)\n",
    "            vis_frac = (inter_area / box_area)  # per-box visible fraction\n",
    "\n",
    "            # require target box visibility\n",
    "            if vis_frac[tidx].item() < self.target_vis_thresh:\n",
    "                continue\n",
    "\n",
    "            # keep boxes with enough visibility\n",
    "            keep = vis_frac >= self.keep_vis_thresh\n",
    "            if keep.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # translate+clip kept boxes to crop frame\n",
    "            kb = b[keep].clone()\n",
    "            kb[:,0::2] = kb[:,0::2].clamp(crop[0], crop[2]) - crop[0]\n",
    "            kb[:,1::2] = kb[:,1::2].clamp(crop[1], crop[3]) - crop[1]\n",
    "\n",
    "            # apply crop to image\n",
    "            x1i, y1i, x2i, y2i = crop.tolist()\n",
    "            x1i, y1i, x2i, y2i = int(x1i), int(y1i), int(x2i), int(y2i)\n",
    "            img_c = img[:, y1i:y2i, x1i:x2i]\n",
    "\n",
    "            # resize to output\n",
    "            resize = T.Resize(self.out_size, antialias=self.antialias)\n",
    "            img_c = resize(tv_tensors.Image(img_c))\n",
    "            # resize boxes accordingly\n",
    "            inH, inW = y2i - y1i, x2i - x1i\n",
    "            scale_x = self.out_size[1] / max(1, inW)\n",
    "            scale_y = self.out_size[0] / max(1, inH)\n",
    "            kb[:, [0,2]] *= scale_x\n",
    "            kb[:, [1,3]] *= scale_y\n",
    "\n",
    "            out = {\n",
    "                \"image\": img_c,\n",
    "                \"boxes\": tv_tensors.BoundingBoxes(\n",
    "                    kb, format=tv_tensors.BoundingBoxFormat.XYXY, canvas_size=self.out_size\n",
    "                ),\n",
    "                \"labels\": labels[keep],\n",
    "            }\n",
    "            return out\n",
    "\n",
    "        # Fallback if no candidate passed quality in max_trials\n",
    "        fallback = T.Compose([\n",
    "            T.RandomResizedCrop(size=self.out_size, scale=(0.8,1.0), ratio=(0.75,1.5), antialias=True)\n",
    "        ])\n",
    "        out = fallback({\"image\": img, \"boxes\": boxes, \"labels\": labels})\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea73ed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:35.789141Z",
     "iopub.status.busy": "2025-09-17T03:19:35.788739Z",
     "iopub.status.idle": "2025-09-17T03:19:35.852750Z",
     "shell.execute_reply": "2025-09-17T03:19:35.851967Z"
    },
    "papermill": {
     "duration": 0.070157,
     "end_time": "2025-09-17T03:19:35.854275",
     "exception": false,
     "start_time": "2025-09-17T03:19:35.784118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PennFudan(Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        self.root = root\n",
    "        self.imgs  = sorted(os.listdir(os.path.join(root, \"PNGImages\")))\n",
    "        self.masks = sorted(os.listdir(os.path.join(root, \"PedMasks\")))\n",
    "        # SSD pipeline: photometric --> geometric --> bbox cleanup --> float\n",
    "        if train:\n",
    "            self.tf = T.Compose([\n",
    "                T.ToImage(),\n",
    "                T.RandomPhotometricDistort(),\n",
    "                T.RandomChoice([\n",
    "                    SafeBoxCenteredCrop(out_size=(300,300),\n",
    "                                        context=(1.3, 2.2),\n",
    "                                        scale_img=(0.25, 0.95),\n",
    "                                        ratio=(0.6, 1.8),\n",
    "                                        max_trials=30,\n",
    "                                        target_vis_thresh=0.7,\n",
    "                                        keep_vis_thresh=0.5),\n",
    "                    T.RandomResizedCrop(size=(300,300), scale=(0.85,1.0), ratio=(0.75,1.5), antialias=True),\n",
    "                    T.Identity(),\n",
    "                ], p=[0.5, 0.3, 0.2]),\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Resize((300, 300), antialias=True),\n",
    "                T.SanitizeBoundingBoxes(min_size=1),\n",
    "                T.ToDtype(torch.float32, scale=True),\n",
    "            ])\n",
    "\n",
    "            # Fallback\n",
    "            self.tf_without_crop = T.Compose([\n",
    "                T.ToImage(),\n",
    "                T.RandomPhotometricDistort(),\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Resize((300, 300), antialias=True),\n",
    "                T.SanitizeBoundingBoxes(min_size=1),\n",
    "                T.ToDtype(torch.float32, scale=True),\n",
    "            ])\n",
    "        else:\n",
    "            self.tf = T.Compose([\n",
    "                T.ToImage(),\n",
    "                T.Resize((300, 300), antialias=True),\n",
    "                T.SanitizeBoundingBoxes(min_size=1),\n",
    "                T.ToDtype(torch.float32, scale=True),\n",
    "            ])\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ip = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mp = os.path.join(self.root, \"PedMasks\",  self.masks[idx])\n",
    "\n",
    "        img  = read_image(ip)                  # (3,H,W), uint8\n",
    "        mask = read_image(mp)[0]               # (H,W), instance ids\n",
    "        ids  = torch.unique(mask)[1:]          # drop background=0\n",
    "        masks = (mask[None] == ids[:, None, None]).to(torch.uint8)  # (N,H,W)\n",
    "        boxes = masks_to_boxes(masks)          # (N,4) xyxy on original size\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        img = tv_tensors.Image(img)\n",
    "        boxes = tv_tensors.BoundingBoxes(\n",
    "            boxes, format=tv_tensors.BoundingBoxFormat.XYXY,\n",
    "            canvas_size=img.shape[-2:]\n",
    "        )\n",
    "\n",
    "        sample = {\"image\": img, \"boxes\": boxes, \"labels\": labels}\n",
    "        sample = self.tf(sample)\n",
    "\n",
    "        def _clamp_and_filter(s):\n",
    "            img = s[\"image\"]\n",
    "            H, W = img.shape[-2:]\n",
    "            b = s[\"boxes\"].to(torch.float32)\n",
    "            b[:, [0,2]] = b[:, [0,2]].clamp(0, W-1)\n",
    "            b[:, [1,3]] = b[:, [1,3]].clamp(0, H-1)\n",
    "            w = (b[:,2]-b[:,0]); h = (b[:,3]-b[:,1])\n",
    "            keep = (w >= 1) & (h >= 1)\n",
    "            s[\"boxes\"]  = b[keep]\n",
    "            s[\"labels\"] = s[\"labels\"][keep]\n",
    "            return s\n",
    "\n",
    "        sample = _clamp_and_filter(sample)\n",
    "\n",
    "        # If no GT boxes survive\n",
    "        if sample[\"boxes\"].numel() == 0:\n",
    "            if hasattr(self, \"tf_without_crop\"):\n",
    "                sample = self.tf_without_crop({\"image\": img, \"boxes\": boxes, \"labels\": labels})\n",
    "                sample = _clamp_and_filter(sample)\n",
    "\n",
    "            if sample[\"boxes\"].numel() == 0:\n",
    "                areas = (boxes[:,2]-boxes[:,0]) * (boxes[:,3]-boxes[:,1])\n",
    "                k = torch.argmax(areas)\n",
    "                sample[\"boxes\"]  = boxes[k:k+1].to(torch.float32)\n",
    "                sample[\"labels\"] = labels[k:k+1]\n",
    "\n",
    "        out = {\n",
    "            \"boxes\":  torch.as_tensor(sample[\"boxes\"], dtype=torch.float32),\n",
    "            \"labels\": sample[\"labels\"]\n",
    "        }\n",
    "        return sample[\"image\"], out\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    imgs, targets = list(zip(*batch))\n",
    "    return torch.stack(imgs, 0), list(targets)\n",
    "\n",
    "root = \"PennFudanPed\"\n",
    "train_ds = PennFudan(root, train=True)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,\n",
    "                          num_workers=0, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6fe6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:35.863027Z",
     "iopub.status.busy": "2025-09-17T03:19:35.862731Z",
     "iopub.status.idle": "2025-09-17T03:19:39.630405Z",
     "shell.execute_reply": "2025-09-17T03:19:39.629678Z"
    },
    "papermill": {
     "duration": 3.777739,
     "end_time": "2025-09-17T03:19:39.635912",
     "exception": false,
     "start_time": "2025-09-17T03:19:35.858173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes, make_grid\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for i in range(60):\n",
    "    img, label = train_ds[i]\n",
    "\n",
    "    img_uint8 = (img * 255).to(torch.uint8)  \n",
    "    out = draw_bounding_boxes(img_uint8, label[\"boxes\"], colors=\"red\", width=2)\n",
    "\n",
    "    imgs.append(out)\n",
    "\n",
    "grid = make_grid(imgs, nrow=5)\n",
    "pil_grid = TF.to_pil_image(grid)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pil_grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1cb94c",
   "metadata": {
    "papermill": {
     "duration": 0.007343,
     "end_time": "2025-09-17T03:19:39.651659",
     "exception": false,
     "start_time": "2025-09-17T03:19:39.644316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![Images](figures/images.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc86694a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:39.667919Z",
     "iopub.status.busy": "2025-09-17T03:19:39.667328Z",
     "iopub.status.idle": "2025-09-17T03:19:41.035761Z",
     "shell.execute_reply": "2025-09-17T03:19:41.035022Z"
    },
    "papermill": {
     "duration": 1.378083,
     "end_time": "2025-09-17T03:19:41.037194",
     "exception": false,
     "start_time": "2025-09-17T03:19:39.659111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = resnet34(weights=\"IMAGENET1K_V1\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17238a06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:41.056094Z",
     "iopub.status.busy": "2025-09-17T03:19:41.055345Z",
     "iopub.status.idle": "2025-09-17T03:19:41.062056Z",
     "shell.execute_reply": "2025-09-17T03:19:41.061308Z"
    },
    "papermill": {
     "duration": 0.017148,
     "end_time": "2025-09-17T03:19:41.063321",
     "exception": false,
     "start_time": "2025-09-17T03:19:41.046173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b9117",
   "metadata": {
    "papermill": {
     "duration": 0.008986,
     "end_time": "2025-09-17T03:19:41.081081",
     "exception": false,
     "start_time": "2025-09-17T03:19:41.072095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Original Architecture\n",
    "\n",
    "| Stage    | Operation                        | Output Shape (C × H × W) |\n",
    "| -------- | -------------------------------- | ------------------------ |\n",
    "| Input    | —                                | 3 × 300 × 300            |\n",
    "| Conv1    | 7×7, stride 2, pad 3             | 64 × 150 × 150           |\n",
    "| MaxPool  | 3×3, stride 2, pad 1             | 64 × 75 × 75             |\n",
    "| Layer 1  | 3 blocks, stride 1               | 64 × 75 × 75             |\n",
    "| Layer 2  | 4 blocks, stride 2 (first block) | 128 × 38 × 38            |\n",
    "| Layer 3  | 6 blocks, stride 2 (first block) | 256 × 19 × 19            |\n",
    "| Layer 4  | 3 blocks, stride 2 (first block) | 512 × 10 × 10            |\n",
    "| AvgPool  | global (kernel = 10×10)          | 512 × 1 × 1              |\n",
    "| FC       | 1000 (ImageNet)                  | —                        |\n",
    "\n",
    "### Adjusted Architecture\n",
    "\n",
    "| Stage    | Operation                        | Output Shape (C × H × W) | Anchors |\n",
    "| -------- | -------------------------------- | ------------------------ |---------|\n",
    "| Input    | —                                | 3 × 300 × 300            | No      |\n",
    "| Conv1    | 7×7, stride 2, pad 3             | 64 × 150 × 150           | No      |\n",
    "| MaxPool  | 3×3, stride 2, pad 1             | 64 × 75 × 75             | No      |\n",
    "| Layer 1  | 3 blocks, stride 1               | 64 × 75 × 75             | No      |\n",
    "| Layer 2  | 4 blocks, stride 2 (first block) | 128 × 38 × 38            | Yes     |\n",
    "| Layer 3  | 6 blocks, stride 2 (first block) | 256 × 19 × 19            | Yes     |\n",
    "| Layer 4  | 3 blocks, stride 2 (first block) | 512 × 10 × 10            | Yes     |\n",
    "| C5       | Conv2d(512, 256, ks=3, s=2, p=1) | 256 × 5 × 5              | Yes     |\n",
    "| C6       | Conv2d(256, 256, ks=3, s=2, p=1) | 256 × 3 × 3              | Yes     |\n",
    "| C7       | Conv2d(256, 256, ks=3, s=2, p=1) | 256 × 1 × 1              | Yes     |\n",
    "\n",
    "\n",
    "\n",
    "| Map         | in\\_ch | mid\\_ch  | out\\_ch | Why       |\n",
    "| ------------| -----| ----| ----| -------------------- |\n",
    "| C5 (10-->5) | 512  | 256 | 256 |-                     |\n",
    "| C6 (5-->3)  | 256  | 128 | 256 |-                     |\n",
    "| C7 (3-->1)  | 256  | -   | 256 | Single 3×3, s=1, p=0 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e528b797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:41.099188Z",
     "iopub.status.busy": "2025-09-17T03:19:41.098586Z",
     "iopub.status.idle": "2025-09-17T03:19:41.103482Z",
     "shell.execute_reply": "2025-09-17T03:19:41.102717Z"
    },
    "papermill": {
     "duration": 0.015551,
     "end_time": "2025-09-17T03:19:41.104896",
     "exception": false,
     "start_time": "2025-09-17T03:19:41.089345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "def ds_block(in_ch, mid_ch, out_ch):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, mid_ch, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(mid_ch),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(mid_ch, out_ch, 3, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_ch),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d7cdb85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:41.124647Z",
     "iopub.status.busy": "2025-09-17T03:19:41.124316Z",
     "iopub.status.idle": "2025-09-17T03:19:41.132528Z",
     "shell.execute_reply": "2025-09-17T03:19:41.131974Z"
    },
    "papermill": {
     "duration": 0.019419,
     "end_time": "2025-09-17T03:19:41.133790",
     "exception": false,
     "start_time": "2025-09-17T03:19:41.114371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet34Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        bone = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        self.stem = nn.Sequential(bone.conv1, \n",
    "                                  bone.bn1,\n",
    "                                  bone.relu, \n",
    "                                  bone.maxpool\n",
    "                                  )\n",
    "        \n",
    "        self.layer1 = bone.layer1\n",
    "        self.layer2 = bone.layer2\n",
    "        self.layer3 = bone.layer3\n",
    "        self.layer4 = bone.layer4\n",
    "\n",
    "        self.proj_f3 = nn.LazyConv2d(256, 1, bias=False)\n",
    "        self.proj_f4 = nn.LazyConv2d(256, 1, bias=False)\n",
    "        self.proj_f5 = nn.LazyConv2d(256, 1, bias=False)\n",
    "\n",
    "        self.c5 = ds_block(512, 256, 256)\n",
    "        self.c6 = ds_block(256, 128, 256)\n",
    "        self.c7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)          # 64 x 75 x 75\n",
    "        o2 = self.layer1(x)       # 64 x 75 x 75\n",
    "        o3 = self.layer2(o2)      # 128 x 38 x 38\n",
    "        o4 = self.layer3(o3)      # 256 x 19 x 19\n",
    "        o5 = self.layer4(o4)      # 512 x 10 x 10\n",
    "\n",
    "        f3 = self.proj_f3(o3)     # 256 x 38 x 38\n",
    "        f4 = self.proj_f4(o4)     # 256 x 19 x 19\n",
    "        f5 = self.proj_f5(o5)     # 256 x 10 x 10\n",
    "\n",
    "        f6 = self.c5(o5)          # 256 x 5 x 5\n",
    "        f7 = self.c6(f6)          # 256 x 3 x 3\n",
    "        f8 = self.c7(f7)          # 256 x 1 x 1\n",
    "\n",
    "        return [f3, f4, f5, f6, f7, f8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c97e7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:41.152400Z",
     "iopub.status.busy": "2025-09-17T03:19:41.152099Z",
     "iopub.status.idle": "2025-09-17T03:19:42.352645Z",
     "shell.execute_reply": "2025-09-17T03:19:42.351845Z"
    },
    "papermill": {
     "duration": 1.21178,
     "end_time": "2025-09-17T03:19:42.354221",
     "exception": false,
     "start_time": "2025-09-17T03:19:41.142441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rowek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f3: (1, 256, 38, 38)\n",
      "f4: (1, 256, 19, 19)\n",
      "f5: (1, 256, 10, 10)\n",
      "f6: (1, 256, 5, 5)\n",
      "f7: (1, 256, 3, 3)\n",
      "f8: (1, 256, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img, label = train_ds[0]\n",
    "model = ResNet34Backbone().eval()\n",
    "with torch.no_grad():\n",
    "    maps = model(img.unsqueeze(0))\n",
    "\n",
    "for i, f in enumerate(maps, 3):\n",
    "    print(f\"f{i}: {tuple(f.shape)}\")\n",
    "\n",
    "fig, axs = plt.subplots(1, len(maps), figsize=(20, 4))\n",
    "for i, f in enumerate(maps):\n",
    "    fmap = f[0, 0].detach().cpu()\n",
    "    axs[i].imshow(fmap, cmap=\"viridis\")\n",
    "    axs[i].set_title(f\"f{i+3} {f.shape[2]}x{f.shape[3]}\")\n",
    "    axs[i].axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5681c53",
   "metadata": {
    "papermill": {
     "duration": 0.009861,
     "end_time": "2025-09-17T03:19:42.374644",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.364783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![Feature Maps](figures/fmaps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e5667",
   "metadata": {
    "papermill": {
     "duration": 0.008543,
     "end_time": "2025-09-17T03:19:42.393271",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.384728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Testing math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e387336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:42.411626Z",
     "iopub.status.busy": "2025-09-17T03:19:42.411316Z",
     "iopub.status.idle": "2025-09-17T03:19:42.431513Z",
     "shell.execute_reply": "2025-09-17T03:19:42.430916Z"
    },
    "papermill": {
     "duration": 0.030808,
     "end_time": "2025-09-17T03:19:42.432719",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.401911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([180.,  90., 250., 240.]),\n",
       " tensor([177.9081,  89.6566, 248.8611, 239.5518]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def corner_to_center_(xyxy):\n",
    "    x1, y1, x2, y2 = xyxy.squeeze()\n",
    "    x = (x1 + x2) / 2\n",
    "    y = (y1 + y2) / 2\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    return torch.tensor([x, y, w, h])\n",
    "\n",
    "def center_to_corner_(xywh):\n",
    "    x, y, w, h = xywh.squeeze()\n",
    "    x1 = x - w / 2\n",
    "    y1 = y - h / 2\n",
    "    x2 = x + w / 2\n",
    "    y2 = y + h / 2\n",
    "    return torch.tensor([x1, y1, x2, y2])\n",
    "\n",
    "def encoding_(A, GT, sigma_xy, sigma_wh):\n",
    "    A_x, A_y, A_w, A_h = A.squeeze()\n",
    "    GT_x, GT_y, GT_w, GT_h = GT.squeeze()\n",
    "\n",
    "    t_x = (GT_x - A_x) / A_w / sigma_xy\n",
    "    t_y = (GT_y - A_y) / A_h / sigma_xy\n",
    "    t_w = torch.log(GT_w / A_w) / sigma_wh\n",
    "    t_h = torch.log(GT_h / A_h) / sigma_wh\n",
    "    return torch.tensor([t_x, t_y, t_w, t_h])\n",
    "\n",
    "def decoding_(A, t_hat, sigma_xy, sigma_wh):\n",
    "    A_x, A_y, A_w, A_h = A.squeeze()\n",
    "    T_x, T_y, T_w, T_h = t_hat.squeeze()\n",
    "\n",
    "    x = T_x * sigma_xy * A_w + A_x\n",
    "    y = T_y * sigma_xy * A_h + A_y\n",
    "    w = torch.exp(T_w * sigma_wh) * A_w\n",
    "    h = torch.exp(T_h * sigma_wh) * A_h\n",
    "    return torch.tensor([x, y, w, h])\n",
    "\n",
    "A = torch.tensor([165., 70., 260., 270.])   # Toy anchor in corner form\n",
    "GT = torch.tensor([180., 90., 250., 240.])   # Toy GT in corner form\n",
    "A_cen = corner_to_center_(A)\n",
    "GT_cen = corner_to_center_(GT)\n",
    "\n",
    "sigma_xy = 0.1\n",
    "sigma_wh = 0.2\n",
    "t = encoding_(A_cen, GT_cen, sigma_xy, sigma_wh)\n",
    "t_hat = t + torch.randn_like(t) * 0.1  # noise\n",
    "\n",
    "P_cen = decoding_(A_cen, t_hat, sigma_xy, sigma_wh)\n",
    "P = center_to_corner_(P_cen)\n",
    "\n",
    "GT, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f780a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:42.451264Z",
     "iopub.status.busy": "2025-09-17T03:19:42.451018Z",
     "iopub.status.idle": "2025-09-17T03:19:42.456385Z",
     "shell.execute_reply": "2025-09-17T03:19:42.455812Z"
    },
    "papermill": {
     "duration": 0.015962,
     "end_time": "2025-09-17T03:19:42.457630",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.441668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def center_to_corner(boxes):\n",
    "    # boxes: [..., 4] in (cx, cy, w, h)\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    return torch.stack((x1, y1, x2, y2), dim=-1)  # [..., 4]\n",
    "\n",
    "def corner_to_center(boxes):  # [...,4] (x1,y1,x2,y2) -> (cx,cy,w,h)\n",
    "    x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "    cx = (x1 + x2) * 0.5\n",
    "    cy = (y1 + y2) * 0.5\n",
    "    w  = (x2 - x1).clamp(min=1e-8)\n",
    "    h  = (y2 - y1).clamp(min=1e-8)\n",
    "    return torch.stack((cx, cy, w, h), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11f5cb2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:42.476323Z",
     "iopub.status.busy": "2025-09-17T03:19:42.476077Z",
     "iopub.status.idle": "2025-09-17T03:19:42.486325Z",
     "shell.execute_reply": "2025-09-17T03:19:42.485674Z"
    },
    "papermill": {
     "duration": 0.020635,
     "end_time": "2025-09-17T03:19:42.487381",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.466746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateAnchors(fmaps):\n",
    "    (O3, O4, O5, O6, O7, O8) = fmaps\n",
    "\n",
    "    # Scale and ratio, making sure most Anchor boxes are tall and skinny.\n",
    "    C3_anchor_scale = torch.tensor([0.20, 0.20, 0.20, 0.272])\n",
    "    C3_anchor_ratio = torch.tensor([0.25, 0.33, 0.50, 1.00])\n",
    "    C4_anchor_scale = torch.tensor([0.37, 0.37, 0.37, 0.447])\n",
    "    C4_anchor_ratio = torch.tensor([0.25, 0.33, 0.50, 1.00])\n",
    "    C5_anchor_scale = torch.tensor([0.54, 0.54, 0.54, 0.619])\n",
    "    C5_anchor_ratio = torch.tensor([0.25, 0.33, 0.50, 1.00])\n",
    "    C6_anchor_scale = torch.tensor([0.71, 0.71, 0.71, 0.790])\n",
    "    C6_anchor_ratio = torch.tensor([0.25, 0.33, 0.50, 1.00])\n",
    "    C7_anchor_scale = torch.tensor([0.88, 0.88, 0.88, 0.961])\n",
    "    C7_anchor_ratio = torch.tensor([0.25, 0.33, 0.50, 1.00])\n",
    "    C8_anchor_scale = torch.tensor([0.88, 0.88, 0.88, 0.961])\n",
    "    C8_anchor_ratio = torch.tensor([0.25, 0.33, 0.50, 1.00])\n",
    "\n",
    "    def place_anchors(fmap, scale_list, ratio_list): # fmap can be either the box or the class, because they both contain the feature map dims.\n",
    "        device, dtype = fmap.device, fmap.dtype\n",
    "        H, W = fmap.shape[-2], fmap.shape[-1]\n",
    "        X = (torch.arange(W, device=device, dtype=dtype) + 0.5) / W\n",
    "        Y = (torch.arange(H, device=device, dtype=dtype) + 0.5) / H\n",
    "        cy, cx = torch.meshgrid(Y, X, indexing='ij')  # cy, cx are [H,W]\n",
    "        centers = torch.stack([cx, cy], dim=-1)        # [H,W,2] as (cx, cy)\n",
    "\n",
    "        s = torch.tensor(scale_list, device=device, dtype=dtype)\n",
    "        r = torch.tensor(ratio_list, device=device, dtype=dtype)\n",
    "        w = s * torch.sqrt(r)\n",
    "        h = s / torch.sqrt(r)\n",
    "        wh = torch.stack([w, h], dim=-1)\n",
    "\n",
    "        wh = wh.unsqueeze(0).unsqueeze(0).expand(H, W, -1, 2)\n",
    "        centers = centers.unsqueeze(2).expand(H, W, wh.shape[2], 2)\n",
    "        anchors = torch.cat([centers, wh], dim=-1)\n",
    "        return center_to_corner(anchors).clamp(0, 1)\n",
    "    \n",
    "    \n",
    "    A3 = place_anchors(O3, C3_anchor_scale, C3_anchor_ratio).flatten(0, 2)\n",
    "    A4 = place_anchors(O4, C4_anchor_scale, C4_anchor_ratio).flatten(0, 2)\n",
    "    A5 = place_anchors(O5, C5_anchor_scale, C5_anchor_ratio).flatten(0, 2)\n",
    "    A6 = place_anchors(O6, C6_anchor_scale, C6_anchor_ratio).flatten(0, 2)\n",
    "    A7 = place_anchors(O7, C7_anchor_scale, C7_anchor_ratio).flatten(0, 2)\n",
    "    A8 = place_anchors(O8, C8_anchor_scale, C8_anchor_ratio).flatten(0, 2)\n",
    "    anchors_corner = torch.concatenate([A3,A4,A5,A6,A7,A8], dim=0)\n",
    "    anchors_center = corner_to_center(anchors_corner)\n",
    "    return anchors_corner, anchors_center\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d8f1d9",
   "metadata": {
    "papermill": {
     "duration": 0.008301,
     "end_time": "2025-09-17T03:19:42.504400",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.496099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4\\*38\\*38 + 4\\*19\\*19 + 4\\*10\\*10 + 4\\*5\\*5 + 4\\*3\\*3 + 4\\*1\\*1 = 7760 total anchors\n",
    "96 training examples\n",
    "96 \\* 7760 = 589456 examples per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a461f84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:42.522808Z",
     "iopub.status.busy": "2025-09-17T03:19:42.522535Z",
     "iopub.status.idle": "2025-09-17T03:19:42.526313Z",
     "shell.execute_reply": "2025-09-17T03:19:42.525530Z"
    },
    "papermill": {
     "duration": 0.014483,
     "end_time": "2025-09-17T03:19:42.527516",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.513033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_ANCHORS = 4\n",
    "N_CLASSES = 2 #bg included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb91757f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:42.547854Z",
     "iopub.status.busy": "2025-09-17T03:19:42.546873Z",
     "iopub.status.idle": "2025-09-17T03:19:42.560341Z",
     "shell.execute_reply": "2025-09-17T03:19:42.559491Z"
    },
    "papermill": {
     "duration": 0.025251,
     "end_time": "2025-09-17T03:19:42.561750",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.536499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNet34Backbone()\n",
    "\n",
    "        self.O3_box = nn.Conv2d(256, 4*N_ANCHORS, 1, bias=False)\n",
    "        self.O3_cls = nn.Conv2d(256, N_ANCHORS*N_CLASSES, 1, bias=False)\n",
    "        self.O4_box = nn.Conv2d(256, 4*N_ANCHORS, 1, bias=False)\n",
    "        self.O4_cls = nn.Conv2d(256, N_ANCHORS*N_CLASSES, 1, bias=False)\n",
    "        self.O5_box = nn.Conv2d(256, 4*N_ANCHORS, 1, bias=False)\n",
    "        self.O5_cls = nn.Conv2d(256, N_ANCHORS*N_CLASSES, 1, bias=False)\n",
    "        self.O6_box = nn.Conv2d(256, 4*N_ANCHORS, 1, bias=False)\n",
    "        self.O6_cls = nn.Conv2d(256, N_ANCHORS*N_CLASSES, 1, bias=False)\n",
    "        self.O7_box = nn.Conv2d(256, 4*N_ANCHORS, 1, bias=False)\n",
    "        self.O7_cls = nn.Conv2d(256, N_ANCHORS*N_CLASSES, 1, bias=False)\n",
    "        self.O8_box = nn.Conv2d(256, 4*N_ANCHORS, 1, bias=False)\n",
    "        self.O8_cls = nn.Conv2d(256, N_ANCHORS*N_CLASSES, 1, bias=False)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _make_anchors(self, fmaps):\n",
    "        anchors_corner, anchors_center = CreateAnchors(fmaps)\n",
    "        dev = fmaps[0].device\n",
    "        dt = fmaps[0].dtype\n",
    "        return anchors_corner.to(dev, dt), anchors_center.to(dev, dt)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f3, f4, f5, f6, f7, f8 = self.backbone(x) # [f3, f4, f5, f6, f7, f8]\n",
    "        O3_box_out = self.O3_box(f3) # Center form\n",
    "        O3_cls_out = self.O3_cls(f3)\n",
    "        O4_box_out = self.O4_box(f4)\n",
    "        O4_cls_out = self.O4_cls(f4)\n",
    "        O5_box_out = self.O5_box(f5)\n",
    "        O5_cls_out = self.O5_cls(f5)\n",
    "        O6_box_out = self.O6_box(f6)\n",
    "        O6_cls_out = self.O6_cls(f6)\n",
    "        O7_box_out = self.O7_box(f7)\n",
    "        O7_cls_out = self.O7_cls(f7)\n",
    "        O8_box_out = self.O8_box(f8)\n",
    "        O8_cls_out = self.O8_cls(f8)\n",
    "\n",
    "        BS = O3_box_out.shape[0]\n",
    "        t_pred = torch.concatenate([O3_box_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 4), # Contiguous realigns memory for new order.\n",
    "                                    O4_box_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 4),\n",
    "                                    O5_box_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 4),\n",
    "                                    O6_box_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 4),\n",
    "                                    O7_box_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 4),\n",
    "                                    O8_box_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 4)],\n",
    "                                    dim=1)\n",
    "        \n",
    "        cls_logits = torch.concatenate([O3_cls_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 2),\n",
    "                                        O4_cls_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 2),\n",
    "                                        O5_cls_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 2),\n",
    "                                        O6_cls_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 2),\n",
    "                                        O7_cls_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 2),\n",
    "                                        O8_cls_out.permute(0, 2, 3, 1).contiguous().view(BS, -1, 2)],\n",
    "                                        dim=1)\n",
    "        \n",
    "        anchors_corner, anchors_center = self._make_anchors([f3, f4, f5, f6, f7, f8])\n",
    "        return t_pred, cls_logits, anchors_center, anchors_corner\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6ecb43e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:42.582635Z",
     "iopub.status.busy": "2025-09-17T03:19:42.582332Z",
     "iopub.status.idle": "2025-09-17T03:19:42.592634Z",
     "shell.execute_reply": "2025-09-17T03:19:42.591930Z"
    },
    "papermill": {
     "duration": 0.022101,
     "end_time": "2025-09-17T03:19:42.593888",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.571787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def box_iou(axyxy, bxyxy):\n",
    "    \"\"\"\n",
    "    IoU between two sets of boxes in CORNER form.\n",
    "\n",
    "    axyxy: [N, 4] = (x1,y1,x2,y2) for N boxes\n",
    "    bxyxy: [M, 4] = (x1,y1,x2,y2) for M boxes\n",
    "\n",
    "    Returns [N, M], where iou[i, j] is IoU(a[i], b[j])\n",
    "    \"\"\"\n",
    "    ax1, ay1, ax2, ay2 = axyxy[:,0], axyxy[:,1], axyxy[:,2], axyxy[:,3] # [N]\n",
    "    bx1, by1, bx2, by2 = bxyxy[:,0], bxyxy[:,1], bxyxy[:,2], bxyxy[:,3] # [M]\n",
    "    # Intersection box corners (broadcast to [N, M])\n",
    "    # ax1[:,None] is [N,1], bx1[None] is [1,M] --> result [N,M]\n",
    "    inter_x1 = torch.max(ax1[:,None], bx1[None]) # [N, 1] and [1, M] --> [N, M]\n",
    "    inter_y1 = torch.max(ay1[:,None], by1[None]) # [N, 1] and [1, M] --> [N, M]\n",
    "    inter_x2 = torch.min(ax2[:,None], bx2[None]) # [N, 1] and [1, M] --> [N, M]\n",
    "    inter_y2 = torch.min(ay2[:,None], by2[None]) # [N, 1] and [1, M] --> [N, M]\n",
    "    # Clamp negative width/height because non-overlapping boxes could be negative.\n",
    "    inter_w = (inter_x2 - inter_x1).clamp(min=0)\n",
    "    inter_h = (inter_y2 - inter_y1).clamp(min=0)\n",
    "    inter = inter_w * inter_h # Intersection area matrix [N, M]\n",
    "    # Areas of individual boxes, clamp just incase\n",
    "    area_a = (ax2-ax1).clamp(min=0) * (ay2-ay1).clamp(min=0) # [N]\n",
    "    area_b = (bx2-bx1).clamp(min=0) * (by2-by1).clamp(min=0) # [M]\n",
    "    # Union = area(a) + area(b) - intersection, broadcast to [N, M]\n",
    "    union = area_a[:,None] + area_b[None] - inter # [N, 1] and [1, M] --> [N, M]\n",
    "    return inter / union.clamp(min=1e-8) # [N, M], [i, j] = IoU between a[i] and b[j]\n",
    "\n",
    "def encoding(A, GT, sigma_xy=0.1, sigma_wh=0.2):\n",
    "    \"\"\"\n",
    "    Encode GT boxes relative to anchors in CENTER form.\n",
    "\n",
    "    A: [..., 4] anchors in center form (x, y, w, h)\n",
    "    GT: [..., 4] GT boxes in center form (x, y, w, h)\n",
    "    sigma_xy: scale for center offsets\n",
    "    sigma_wh: scale for log-scale width/height\n",
    "\n",
    "    returns t_true: [..., 4] encoded targets\n",
    "    \"\"\"\n",
    "    A_x, A_y, A_w, A_h = A.unbind(-1)\n",
    "    GT_x, GT_y, GT_w, GT_h = GT.unbind(-1)\n",
    "    # Clamp to prevent 0 widths and heights\n",
    "    A_w = A_w.clamp(min=1e-8)\n",
    "    A_h = A_h.clamp(min=1e-8)\n",
    "    GT_w = GT_w.clamp(min=1e-8)\n",
    "    GT_h = GT_h.clamp(min=1e-8)\n",
    "    t_x = (GT_x - A_x) / A_w / sigma_xy\n",
    "    t_y = (GT_y - A_y) / A_h / sigma_xy\n",
    "    t_w = torch.log(GT_w / A_w) / sigma_wh\n",
    "    t_h = torch.log(GT_h / A_h) / sigma_wh\n",
    "    return torch.stack([t_x, t_y, t_w, t_h], dim=-1)\n",
    "\n",
    "def decoding(A, t_hat, sigma_xy=0.1, sigma_wh=0.2): # Must be in center form\n",
    "    \"\"\"\n",
    "    Decode predicted offsets back to CENTER form boxes.\n",
    "\n",
    "    A: [..., 4] anchors in center form (x, y, w, h)\n",
    "    t_hat: [..., 4] predicted offsets\n",
    "    sigma_xy: scale for center offsets\n",
    "    sigma_wh: scale for log-scale width/height\n",
    "\n",
    "    returns boxes: [..., 4] decoded boxes in center form (x, y, w, h)\n",
    "    \"\"\"\n",
    "    A_x, A_y, A_w, A_h = A.unbind(-1)\n",
    "    T_x, T_y, T_w, T_h = t_hat.unbind(-1)\n",
    "    x = T_x * sigma_xy * A_w + A_x\n",
    "    y = T_y * sigma_xy * A_h + A_y\n",
    "    w = torch.exp(T_w * sigma_wh) * A_w\n",
    "    h = torch.exp(T_h * sigma_wh) * A_h\n",
    "    return torch.stack([x, y, w, h], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4616ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:19:42.614348Z",
     "iopub.status.busy": "2025-09-17T03:19:42.613798Z",
     "iopub.status.idle": "2025-09-17T03:30:12.965985Z",
     "shell.execute_reply": "2025-09-17T03:30:12.964964Z"
    },
    "papermill": {
     "duration": 630.364272,
     "end_time": "2025-09-17T03:30:12.967828",
     "exception": false,
     "start_time": "2025-09-17T03:19:42.603556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/3440624971.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(scale_list, device=device, dtype=dtype)\n",
      "/tmp/ipykernel_19/3440624971.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r = torch.tensor(ratio_list, device=device, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1107.0129899978638\n",
      "Best (loss=1107.0130). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 1 Loss: 859.3170013427734\n",
      "Best (loss=859.3170). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 2 Loss: 777.937463760376\n",
      "Best (loss=777.9375). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 3 Loss: 750.4309635162354\n",
      "Best (loss=750.4310). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 4 Loss: 719.0877952575684\n",
      "Best (loss=719.0878). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 5 Loss: 668.9105920791626\n",
      "Best (loss=668.9106). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 6 Loss: 639.8297758102417\n",
      "Best (loss=639.8298). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 7 Loss: 603.8889722824097\n",
      "Best (loss=603.8890). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 8 Loss: 626.3037338256836\n",
      "Epoch 9 Loss: 598.8333263397217\n",
      "Best (loss=598.8333). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 10 Loss: 599.3805074691772\n",
      "Epoch 11 Loss: 578.3773255348206\n",
      "Best (loss=578.3773). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 12 Loss: 540.4611225128174\n",
      "Best (loss=540.4611). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 13 Loss: 546.8383569717407\n",
      "Epoch 14 Loss: 529.8344497680664\n",
      "Best (loss=529.8344). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 15 Loss: 515.4502906799316\n",
      "Best (loss=515.4503). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 16 Loss: 519.7710008621216\n",
      "Epoch 17 Loss: 501.44309997558594\n",
      "Best (loss=501.4431). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 18 Loss: 520.7716341018677\n",
      "Epoch 19 Loss: 512.4578366279602\n",
      "Epoch 20 Loss: 499.4056668281555\n",
      "Best (loss=499.4057). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 21 Loss: 480.2193112373352\n",
      "Best (loss=480.2193). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 22 Loss: 491.8152070045471\n",
      "Epoch 23 Loss: 482.55333375930786\n",
      "Epoch 24 Loss: 485.2233729362488\n",
      "Epoch 25 Loss: 485.5116367340088\n",
      "Epoch 26 Loss: 472.0554461479187\n",
      "Best (loss=472.0554). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 27 Loss: 441.8058223724365\n",
      "Best (loss=441.8058). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 28 Loss: 477.68496894836426\n",
      "Epoch 29 Loss: 426.8887896537781\n",
      "Best (loss=426.8888). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 30 Loss: 420.7088613510132\n",
      "Best (loss=420.7089). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 31 Loss: 402.9547462463379\n",
      "Best (loss=402.9547). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 32 Loss: 419.8471345901489\n",
      "Epoch 33 Loss: 390.6971769332886\n",
      "Best (loss=390.6972). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 34 Loss: 405.012131690979\n",
      "Epoch 35 Loss: 390.8327703475952\n",
      "Epoch 36 Loss: 399.8443036079407\n",
      "Epoch 37 Loss: 395.9241371154785\n",
      "Epoch 38 Loss: 382.61865067481995\n",
      "Best (loss=382.6187). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 39 Loss: 349.2345275878906\n",
      "Best (loss=349.2345). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 40 Loss: 357.04894971847534\n",
      "Epoch 41 Loss: 384.8253631591797\n",
      "Epoch 42 Loss: 367.1791000366211\n",
      "Epoch 43 Loss: 368.8969683647156\n",
      "Epoch 44 Loss: 355.40921211242676\n",
      "Epoch 45 Loss: 381.60243940353394\n",
      "Epoch 46 Loss: 383.56980085372925\n",
      "Epoch 47 Loss: 359.8572187423706\n",
      "Epoch 48 Loss: 336.1324157714844\n",
      "Best (loss=336.1324). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 49 Loss: 363.7374267578125\n",
      "Epoch 50 Loss: 352.66089630126953\n",
      "Epoch 51 Loss: 319.5600461959839\n",
      "Best (loss=319.5600). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 52 Loss: 310.9332330226898\n",
      "Best (loss=310.9332). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 53 Loss: 331.3831639289856\n",
      "Epoch 54 Loss: 324.8132252693176\n",
      "Epoch 55 Loss: 332.9170455932617\n",
      "Epoch 56 Loss: 334.0039200782776\n",
      "Epoch 57 Loss: 315.77240777015686\n",
      "Epoch 58 Loss: 329.37876892089844\n",
      "Epoch 59 Loss: 328.1392569541931\n",
      "Epoch 60 Loss: 361.1679496765137\n",
      "Epoch 61 Loss: 355.299654006958\n",
      "Epoch 62 Loss: 359.34108448028564\n",
      "Epoch 63 Loss: 365.86713314056396\n",
      "Epoch 64 Loss: 361.15987062454224\n",
      "Epoch 65 Loss: 314.28750562667847\n",
      "Epoch 66 Loss: 308.78149127960205\n",
      "Best (loss=308.7815). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 67 Loss: 323.94651889801025\n",
      "Epoch 68 Loss: 295.0778398513794\n",
      "Best (loss=295.0778). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 69 Loss: 309.0056486129761\n",
      "Epoch 70 Loss: 295.0500421524048\n",
      "Best (loss=295.0500). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 71 Loss: 304.81095004081726\n",
      "Epoch 72 Loss: 295.89432287216187\n",
      "Epoch 73 Loss: 290.3300108909607\n",
      "Best (loss=290.3300). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 74 Loss: 277.8718023300171\n",
      "Best (loss=277.8718). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 75 Loss: 286.8953483104706\n",
      "Epoch 76 Loss: 284.9566388130188\n",
      "Epoch 77 Loss: 272.67440724372864\n",
      "Best (loss=272.6744). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 78 Loss: 280.83460998535156\n",
      "Epoch 79 Loss: 280.71781158447266\n",
      "Epoch 80 Loss: 288.543643951416\n",
      "Epoch 81 Loss: 279.2566213607788\n",
      "Epoch 82 Loss: 278.10491394996643\n",
      "Epoch 83 Loss: 262.5962824821472\n",
      "Best (loss=262.5963). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 84 Loss: 255.0789029598236\n",
      "Best (loss=255.0789). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 85 Loss: 266.69732308387756\n",
      "Epoch 86 Loss: 256.70494079589844\n",
      "Epoch 87 Loss: 257.6634120941162\n",
      "Epoch 88 Loss: 257.15506887435913\n",
      "Epoch 89 Loss: 255.55761623382568\n",
      "Epoch 90 Loss: 281.8355498313904\n",
      "Epoch 91 Loss: 259.0599126815796\n",
      "Epoch 92 Loss: 251.59759378433228\n",
      "Best (loss=251.5976). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 93 Loss: 251.71555280685425\n",
      "Epoch 94 Loss: 244.645920753479\n",
      "Best (loss=244.6459). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 95 Loss: 239.7616844177246\n",
      "Best (loss=239.7617). Saved to ssd_resnet34_pennfudan_best.pt\n",
      "Epoch 96 Loss: 254.97564029693604\n",
      "Epoch 97 Loss: 251.8667709827423\n",
      "Epoch 98 Loss: 265.41031551361084\n",
      "Epoch 99 Loss: 272.4781036376953\n"
     ]
    }
   ],
   "source": [
    "model = SSD().to(device)\n",
    "cls_loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "box_loss_fn = nn.SmoothL1Loss(reduction='sum', beta=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "negative_ratio = 4\n",
    "\n",
    "epochs = 100\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "# No validation set because I don't have many images to go off of.\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        t_pred, cls_logits, anchors_center, anchors_corner = model(xb)\n",
    "\n",
    "        total_box = torch.tensor(0.0, device=device)\n",
    "        total_cls = torch.tensor(0.0, device=device)\n",
    "        for b in range(xb.shape[0]):\n",
    "            gt_boxes = yb[b]['boxes'].to(device)\n",
    "            scale = torch.tensor([300, 300, 300, 300], device=device, dtype=gt_boxes.dtype)\n",
    "            gt_boxes = gt_boxes / scale\n",
    "            #y_true = yb[b]['labels'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                iou = box_iou(anchors_corner, gt_boxes)\n",
    "                max_iou, max_iou_ind = torch.max(iou, dim=1)\n",
    "                _, max_gt_iou_ind = torch.max(iou, dim=0)\n",
    "                pos = max_iou >= 0.5\n",
    "                pos[max_gt_iou_ind] = True\n",
    "\n",
    "                cls_t = torch.zeros(anchors_corner.shape[0], dtype=torch.long, device=device)\n",
    "                cls_t[pos] = 1\n",
    "\n",
    "                t_true = torch.zeros(anchors_corner.shape[0], 4, device=device, dtype=t_pred.dtype)\n",
    "                t_true[pos] = encoding(anchors_center[pos], corner_to_center(gt_boxes[max_iou_ind[pos]]))\n",
    "                \n",
    "            pos_count = max(1, int(pos.sum().item()))\n",
    "\n",
    "            box_loss = box_loss_fn(t_pred[b][pos], t_true[pos]) / pos_count\n",
    "\n",
    "            cls = cls_loss_fn(cls_logits[b], cls_t)\n",
    "            cls_pos = cls[pos]\n",
    "            cls_neg = cls[~pos]\n",
    "\n",
    "            k = min(negative_ratio * pos_count, cls_neg.numel()) # How many negatives to keep.\n",
    "            cls_neg_topk, _ = torch.topk(cls_neg, k)\n",
    "                \n",
    "            total_cls += (cls_pos.sum() + cls_neg_topk.sum()) / pos_count\n",
    "            total_box += box_loss\n",
    "\n",
    "        loss = total_box + total_cls\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} Loss: {total_loss}\")\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        torch.save({\"epoch\": epoch,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"best_loss\": best_loss},\n",
    "                   \"ssd_resnet34_pennfudan_best.pt\")\n",
    "        print(f\"Best (loss={best_loss:.4f}). Saved to ssd_resnet34_pennfudan_best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33367e",
   "metadata": {},
   "source": [
    "Viewing predictions on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.ops as ops\n",
    "from torchvision.utils import draw_bounding_boxes, make_grid\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = SSD().to(device)\n",
    "ckpt = torch.load(\"ssd_resnet34_pennfudan_best.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "img_path = 'Test2.png'\n",
    "img = read_image(img_path) / 255.0\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for i in range(20):\n",
    "    img, label = train_ds[i]\n",
    "\n",
    "    img_uint8 = (img * 255).to(torch.uint8)  \n",
    "    H, W = img.shape[1], img.shape[2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = img.unsqueeze(0).to(next(model.parameters()).device)\n",
    "        t_pred, cls_logits, anchors_center, anchors_corner = model(x) \n",
    "\n",
    "        fg_scores = cls_logits.softmax(dim=-1)[0, :, 1]\n",
    "\n",
    "        conf_thresh = 0.5\n",
    "        keep = fg_scores > conf_thresh\n",
    "        if keep.sum() == 0:\n",
    "            keep = torch.zeros_like(fg_scores, dtype=torch.bool)\n",
    "            keep[fg_scores.argmax()] = True\n",
    "\n",
    "        scores = fg_scores[keep]\n",
    "        a_sel = anchors_center[keep]\n",
    "        t_sel = t_pred[0, keep]\n",
    "\n",
    "        boxes_cxcywh = decoding(a_sel, t_sel)\n",
    "        boxes_xyxy = center_to_corner(boxes_cxcywh)\n",
    "\n",
    "        scale = torch.tensor([W, H, W, H], device=boxes_xyxy.device, dtype=boxes_xyxy.dtype)\n",
    "        boxes_xyxy = boxes_xyxy * scale\n",
    "\n",
    "        boxes_xyxy[:, [0, 2]] = boxes_xyxy[:, [0, 2]].clamp(0, W - 1)\n",
    "        boxes_xyxy[:, [1, 3]] = boxes_xyxy[:, [1, 3]].clamp(0, H - 1)\n",
    "\n",
    "        iou_thresh = 0.45\n",
    "        keep_nms = ops.nms(boxes_xyxy, scores, iou_thresh)\n",
    "\n",
    "        boxes_xyxy = boxes_xyxy[keep_nms].cpu()\n",
    "        scores = scores[keep_nms].cpu()\n",
    "\n",
    "        topk = 20\n",
    "        if boxes_xyxy.shape[0] > topk:\n",
    "            idx = scores.topk(topk).indices\n",
    "            boxes_xyxy = boxes_xyxy[idx]\n",
    "            scores = scores[idx]\n",
    "\n",
    "        img_uint8 = (img * 255).to(torch.uint8)\n",
    "        labels = [f\"{s.item():.2f}\" for s in scores]\n",
    "        out = draw_bounding_boxes(img_uint8, boxes_xyxy, colors=\"red\", width=2, labels=labels)\n",
    "\n",
    "        imgs.append(out)\n",
    "\n",
    "grid = make_grid(imgs, nrow=5)\n",
    "pil_grid = TF.to_pil_image(grid)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pil_grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12790a0f",
   "metadata": {},
   "source": [
    "![Predicted](figures/Predicted.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa026d",
   "metadata": {},
   "source": [
    "Testing on a random image in my camera roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'figures/Test.png'\n",
    "img = read_image(img_path) / 255.0\n",
    "\n",
    "H, W = img.shape[1], img.shape[2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = img.unsqueeze(0).to(next(model.parameters()).device)\n",
    "    t_pred, cls_logits, anchors_center, anchors_corner = model(x)\n",
    "\n",
    "    value, indice = torch.max(cls_logits, dim=1)\n",
    "    predicted = decoding(anchors_center[indice[0, 1]], t_pred[0, indice[0, 1]])\n",
    "    \n",
    "img_uint8 = (img * 255).to(torch.uint8)  \n",
    "out = draw_bounding_boxes(img_uint8, boxes=center_to_corner(predicted.unsqueeze(0) * 300), colors=\"red\", width=2)\n",
    "\n",
    "pil = TF.to_pil_image(out)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pil)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "center_to_corner(predicted.unsqueeze(0) * 300).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162c0e4",
   "metadata": {},
   "source": [
    "![Predicted](figures/Test_prediction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876365e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8285955,
     "sourceId": 13082746,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 654.427333,
   "end_time": "2025-09-17T03:30:14.875922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-17T03:19:20.448589",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
