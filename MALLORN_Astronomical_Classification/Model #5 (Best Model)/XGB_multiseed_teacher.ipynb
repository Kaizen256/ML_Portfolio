{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e8445a",
   "metadata": {},
   "source": [
    "# Model 5: Teacher Features + Feature Selection + Optuna (OOF F1) + Multiseed XGBoost\n",
    "\n",
    "This notebook contains Model 5 for the MALLORN challenge. Highest performing model out of all of them.\n",
    "\n",
    "Model 5 builds on the teacher-stacking concept from Model 4, but pushes further in three directions:\n",
    "1) Richer time-series and cross-band physics-inspired features\n",
    "2) Feature selection using XGBoost gain importance\n",
    "3) Direct optimization for OOF F1 using split-aware Optuna, followed by multiseed training/inference\n",
    "\n",
    "## Results\n",
    "\n",
    "Best parameters:\n",
    "- n_estimators: 9476\n",
    "- learning_rate: 0.0024306289953670325\n",
    "- max_depth: 7\n",
    "- min_child_weight: 6\n",
    "- subsample: 0.5344962939912224\n",
    "- colsample_bytree: 0.464696420753079\n",
    "- colsample_bylevel: 0.8146569410634974\n",
    "- colsample_bynode: 0.7285475291884695\n",
    "- max_bin: 181\n",
    "- gamma: 8.476938947246458\n",
    "- reg_alpha: 0.44957196104419117\n",
    "- reg_lambda: 5.23806334613521\n",
    "- max_delta_step: 0\n",
    "- grow_policy: depthwise\n",
    "\n",
    "OOF multiseed best threshold: 0.419  \n",
    "OOF multiseed best F1: 0.6243705941591138  \n",
    "OOF AP (aucpr-ish): 0.5164263302782434  \n",
    "\n",
    "| Submission | Public LB F1 | Private LB F1 |\n",
    "|-------------|--------------|----------------|\n",
    "| 1 | 0.6222 | 0.6231 |\n",
    "| 2 | 0.6358 | 0.6413 |\n",
    "| 3 | 0.5830 | 0.5962 |\n",
    "| 4 | 0.6304 | 0.6491 |\n",
    "| 5 | 0.5840 | 0.6100 |\n",
    "\n",
    "\n",
    "Key upgrades vs Model 4:\n",
    "- Expanded feature set (seasonality, structure function, SED slope fits, Bazin fits + rise/fall asymmetry)\n",
    "- SpecType teacher grouping expanded (includes SLSN, SNII)\n",
    "- Missing-value indicators added (`*_isnan`)\n",
    "- Gain-based Top-K feature selection before Optuna\n",
    "- Optuna objective returns OOF F1 (not AP/logloss)\n",
    "- Final prediction is multiseed averaged XGB + global OOF-optimized threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40374204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "from extinction import fitzpatrick99\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "from scipy.optimize import curve_fit\n",
    "import xgboost as xgb\n",
    "\n",
    "FILTERS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "EFF_WL_AA = {\n",
    "    \"u\": 3641.0,\n",
    "    \"g\": 4704.0,\n",
    "    \"r\": 6155.0,\n",
    "    \"i\": 7504.0,\n",
    "    \"z\": 8695.0,\n",
    "    \"y\": 10056.0,\n",
    "}\n",
    "\n",
    "R_V = 3.1\n",
    "PRE_BASE_FRAC = 0.20\n",
    "MIN_BAND_POINTS = 5\n",
    "PEAK_SIGMA_K = 3.0\n",
    "REBRIGHT_FRAC = 0.30\n",
    "EPS = 1e-8\n",
    "\n",
    "SEASON_GAP_DAYS = 90.0\n",
    "\n",
    "SF_LAGS = [5.0, 10.0, 20.0, 50.0, 100.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036fa8a",
   "metadata": {},
   "source": [
    "\n",
    "## Helper Functions\n",
    "\n",
    "This notebook reuses the same helper functions from prior models (robust stats, slopes, chi2, Stetson J, de-extinction, peak logic, etc).\n",
    "\n",
    "New helper functions introduced in Model 5:\n",
    "- `seasonality_features`\n",
    "- `structure_function_lags`\n",
    "- `peak_vs_wavelength_slope`\n",
    "- `sed_logflux_vs_loglambda_at_time`\n",
    "- Bazin fit functions (`fit_bazin`, etc.)\n",
    "\n",
    "Docstring generated by AI after completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b306342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(x, default=np.nan):\n",
    "    \"\"\"Convert value to float safely.\n",
    "\n",
    "    Returns default if x is None, NaN, or cannot be converted.\n",
    "    Prevents crashes from bad metadata inputs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if x is None:\n",
    "            return default\n",
    "        x = float(x)\n",
    "        if np.isnan(x):\n",
    "            return default\n",
    "        return x\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def trapz_safe(y, x):\n",
    "    \"\"\"Compute trapezoidal integral safely.\n",
    "\n",
    "    Uses numpy.trapezoid if available, otherwise manual formula.\n",
    "    Returns NaN if fewer than two x points.\n",
    "    \"\"\"\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return float(np.trapezoid(y, x))\n",
    "    y = np.asarray(y)\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    return float(np.sum((x[1:] - x[:-1]) * (y[1:] + y[:-1]) * 0.5))\n",
    "\n",
    "\n",
    "def median_abs_dev(x):\n",
    "    \"\"\"Median absolute deviation (robust spread measure).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    med = np.median(x)\n",
    "    return float(np.median(np.abs(x - med)))\n",
    "\n",
    "\n",
    "def iqr(x):\n",
    "    \"\"\"Interquartile range (Q75 − Q25).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    q75, q25 = np.percentile(x, [75, 25])\n",
    "    return float(q75 - q25)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    \"\"\"Compute skewness (distribution asymmetry).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m3 = np.mean((x - mu) ** 3)\n",
    "    return float(m3 / (s ** 3))\n",
    "\n",
    "\n",
    "def kurtosis_excess(x):\n",
    "    \"\"\"Compute excess kurtosis (tail heaviness vs normal).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m4 = np.mean((x - mu) ** 4)\n",
    "    return float(m4 / (s ** 4) - 3.0)\n",
    "\n",
    "\n",
    "def von_neumann_eta(x):\n",
    "    \"\"\"Von Neumann variability ratio using successive differences.\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    v = np.var(x)\n",
    "    if v < 1e-12:\n",
    "        return 0.0\n",
    "    dif = np.diff(x)\n",
    "    return float(np.mean(dif ** 2) / v)\n",
    "\n",
    "\n",
    "def max_slope(t, f):\n",
    "    \"\"\"Maximum absolute slope between consecutive points.\"\"\"\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.max(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def median_abs_slope(t, f):\n",
    "    \"\"\"Median absolute slope between consecutive points.\"\"\"\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.median(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def linear_slope(t, f):\n",
    "    \"\"\"Slope of best-fit line f(t) using linear regression.\"\"\"\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    try:\n",
    "        a, b = np.polyfit(t, f, 1)\n",
    "        return float(a)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def chi2_to_constant(f, ferr):\n",
    "    \"\"\"Reduced chi-square vs constant (median) flux model.\"\"\"\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.median(f)\n",
    "    denom = (ferr + EPS) ** 2\n",
    "    chi2 = np.sum((f - mu) ** 2 / denom)\n",
    "    dof = max(1, n - 1)\n",
    "    return float(chi2 / dof)\n",
    "\n",
    "\n",
    "def interp_flux_at_time(tb, fb, t0):\n",
    "    \"\"\"Linearly interpolate flux at time t0 within range.\"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan\n",
    "    if (t0 < tb.min()) or (t0 > tb.max()):\n",
    "        return np.nan\n",
    "    return float(np.interp(t0, tb, fb))\n",
    "\n",
    "\n",
    "def interp_err_at_time(tb, eb, t0):\n",
    "    \"\"\"Linearly interpolate flux error at time t0.\"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    eb = np.asarray(eb)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan\n",
    "    if (t0 < tb.min()) or (t0 > tb.max()):\n",
    "        return np.nan\n",
    "    return float(np.interp(t0, tb, eb))\n",
    "\n",
    "\n",
    "def fractional_variability(f, ferr):\n",
    "    \"\"\"Fractional intrinsic variability after error correction.\"\"\"\n",
    "    f = np.asarray(f, float)\n",
    "    ferr = np.asarray(ferr, float)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.mean(f)\n",
    "    if np.abs(mu) < 1e-8:\n",
    "        return np.nan\n",
    "    s2 = np.var(f, ddof=1)\n",
    "    mean_err2 = np.mean(ferr ** 2)\n",
    "    excess = max(0.0, s2 - mean_err2)\n",
    "    return float(np.sqrt(excess) / np.abs(mu))\n",
    "\n",
    "\n",
    "def stetson_J_consecutive(t, f, ferr):\n",
    "    \"\"\"Stetson J variability index using consecutive pairs.\"\"\"\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(t)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "    mu = np.mean(f)\n",
    "    scale = np.sqrt(n / max(1, n - 1))\n",
    "    delta = scale * (f - mu) / (ferr + EPS)\n",
    "    vals = []\n",
    "    for i in range(n - 1):\n",
    "        P = delta[i] * delta[i + 1]\n",
    "        vals.append(np.sign(P) * np.sqrt(np.abs(P)))\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "\n",
    "def pre_peak_baseline(tb, fb, eb, frac=PRE_BASE_FRAC):\n",
    "    \"\"\"Estimate baseline stats from early light-curve segment.\"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(tb)\n",
    "    if n < 3:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    k = max(2, int(np.ceil(frac * n)))\n",
    "    k = min(k, n)\n",
    "    base = float(np.median(fb[:k]))\n",
    "    mad_pre = median_abs_dev(fb[:k])\n",
    "    mederr_pre = float(np.median(eb[:k])) if k > 0 else np.nan\n",
    "    return base, mad_pre, mederr_pre\n",
    "\n",
    "\n",
    "def count_significant_peaks(tb, fb, eb, baseline_pre, k_sigma=PEAK_SIGMA_K):\n",
    "    \"\"\"Count local peaks above noise-scaled baseline threshold.\"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(fb)\n",
    "    if n < 5:\n",
    "        return 0\n",
    "    mederr = float(np.median(eb)) if np.isfinite(np.median(eb)) else 0.0\n",
    "    thresh = baseline_pre + k_sigma * mederr\n",
    "    peaks = 0\n",
    "    for i in range(1, n - 1):\n",
    "        if (fb[i] > fb[i - 1]) and (fb[i] > fb[i + 1]) and (fb[i] > thresh):\n",
    "            peaks += 1\n",
    "    return int(peaks)\n",
    "\n",
    "\n",
    "def postpeak_monotonicity(tb, fb, pidx):\n",
    "    \"\"\"Fraction of negative slopes after peak index.\"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return np.nan\n",
    "    t2 = tb[pidx:]\n",
    "    f2 = fb[pidx:]\n",
    "    if len(f2) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t2)\n",
    "    df = np.diff(f2)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    return float(np.mean((df[good] / dt[good]) < 0))\n",
    "\n",
    "\n",
    "def count_rebrighten(tb, fb, baseline_pre, amp, pidx, frac=REBRIGHT_FRAC):\n",
    "    \"\"\"Count post-peak upward crossings above fractional amplitude.\"\"\"\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return 0\n",
    "    level = baseline_pre + frac * amp\n",
    "    post = fb[pidx:]\n",
    "    if len(post) < 3:\n",
    "        return 0\n",
    "    above = post > level\n",
    "    crossings = np.sum((~above[:-1]) & (above[1:]))\n",
    "    return int(crossings)\n",
    "\n",
    "\n",
    "def fall_time_to_level(tb, fb, baseline_pre, amp, pidx, frac):\n",
    "    \"\"\"Time from peak to decay below fractional amplitude.\"\"\"\n",
    "    if amp <= 0 or pidx is None:\n",
    "        return np.nan\n",
    "    level = baseline_pre + frac * amp\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "    if len(f_dec) < 2:\n",
    "        return np.nan\n",
    "    idx = np.where(f_dec <= level)[0]\n",
    "    if len(idx) == 0:\n",
    "        return np.nan\n",
    "    return float(t_dec[idx[0]] - t_dec[0])\n",
    "\n",
    "\n",
    "def rise_time_to_level(tb, fb, baseline_pre, amp, pidx, frac):\n",
    "    \"\"\"Time to rise from level to peak.\"\"\"\n",
    "    if amp <= 0 or pidx is None or pidx < 2:\n",
    "        return np.nan\n",
    "    level = baseline_pre + frac * amp\n",
    "    t_pre = tb[:pidx + 1]\n",
    "    f_pre = fb[:pidx + 1]\n",
    "    idx = np.where(f_pre >= level)[0]\n",
    "    if len(idx) == 0:\n",
    "        return np.nan\n",
    "    return float(t_pre[-1] - t_pre[idx[0]])\n",
    "\n",
    "\n",
    "def decay_powerlaw_fit(tb, fb, baseline_pre, pidx, tmax=300.0):\n",
    "    \"\"\"Fit post-peak decay to power-law in log-log space.\n",
    "\n",
    "    Returns slope, R², and number of points used.\n",
    "    \"\"\"\n",
    "    if pidx is None or pidx >= len(fb) - 3:\n",
    "        return np.nan, np.nan, 0\n",
    "    t0 = tb[pidx]\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "    dt = t_dec - t0\n",
    "    m = (dt > 0.0) & (dt <= tmax)\n",
    "    dt = dt[m]\n",
    "    fd = f_dec[m] - baseline_pre\n",
    "    m2 = fd > 0.0\n",
    "    dt = dt[m2]\n",
    "    fd = fd[m2]\n",
    "    if len(dt) < 4:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "    x = np.log(dt + EPS)\n",
    "    y = np.log(fd + EPS)\n",
    "    try:\n",
    "        b, a = np.polyfit(x, y, 1)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "    yhat = a + b * x\n",
    "    ss_res = float(np.sum((y - yhat) ** 2))\n",
    "    ss_tot = float(np.sum((y - np.mean(y)) ** 2)) + EPS\n",
    "    r2 = 1.0 - ss_res / ss_tot\n",
    "    return float(b), float(r2), int(len(dt))\n",
    "\n",
    "\n",
    "def signed_log1p(x):\n",
    "    \"\"\"Signed log(1+|x|) transform for scale compression.\"\"\"\n",
    "    x = float(x)\n",
    "    return float(np.sign(x) * np.log1p(np.abs(x)))\n",
    "\n",
    "\n",
    "def deextinct_band(flux, flux_err, ebv, band, r_v=R_V):\n",
    "    \"\"\"Apply dust extinction correction to one band.\"\"\"\n",
    "    if ebv is None or (isinstance(ebv, float) and np.isnan(ebv)):\n",
    "        return flux, flux_err\n",
    "    A_V = float(ebv) * float(r_v)\n",
    "    wave = np.array([EFF_WL_AA[band]], dtype=float)\n",
    "    A_lambda = float(fitzpatrick99(wave, A_V, r_v=r_v, unit=\"aa\")[0])\n",
    "    fac = 10.0 ** (0.4 * A_lambda)\n",
    "    return flux * fac, flux_err * fac\n",
    "\n",
    "\n",
    "def deextinct_lightcurve(lc, ebv):\n",
    "    \"\"\"Apply extinction correction across all filters.\"\"\"\n",
    "    flux = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    ferr = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "    flux_corr = flux.copy()\n",
    "    ferr_corr = ferr.copy()\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        flux_corr[m], ferr_corr[m] = deextinct_band(flux_corr[m], ferr_corr[m], ebv, b)\n",
    "    return flux_corr, ferr_corr\n",
    "\n",
    "\n",
    "def band_corr(tt_a, ff_a, tt_b, ff_b, n_grid=30):\n",
    "    \"\"\"Correlation between two bands via interpolated grid.\"\"\"\n",
    "    tt_a = np.asarray(tt_a, float)\n",
    "    ff_a = np.asarray(ff_a, float)\n",
    "    tt_b = np.asarray(tt_b, float)\n",
    "    ff_b = np.asarray(ff_b, float)\n",
    "\n",
    "    if len(tt_a) < 3 or len(tt_b) < 3:\n",
    "        return np.nan\n",
    "\n",
    "    tmin = max(tt_a.min(), tt_b.min())\n",
    "    tmax = min(tt_a.max(), tt_b.max())\n",
    "    if (tmax - tmin) < 5.0:\n",
    "        return np.nan\n",
    "\n",
    "    grid = np.linspace(tmin, tmax, n_grid)\n",
    "    fa = np.interp(grid, tt_a, ff_a)\n",
    "    fb = np.interp(grid, tt_b, ff_b)\n",
    "\n",
    "    sa = np.std(fa)\n",
    "    sb = np.std(fb)\n",
    "    if sa < 1e-12 or sb < 1e-12:\n",
    "        return 0.0\n",
    "    return float(np.corrcoef(fa, fb)[0, 1])\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -60.0, 60.0)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def bazin_stable(t, A, t0, trise, tfall, B, eps=EPS):\n",
    "    \"\"\"\n",
    "    Numerically stable Bazin-like function:\n",
    "        f(t) = A * exp(-(t-t0)/tfall) * sigmoid((t-t0)/trise) + B\n",
    "    \"\"\"\n",
    "    trise = np.maximum(trise, eps)\n",
    "    tfall = np.maximum(tfall, eps)\n",
    "\n",
    "    x = (t - t0) / trise\n",
    "    exp_term = np.exp(np.clip(-(t - t0) / tfall, -60.0, 60.0))\n",
    "    return A * exp_term * sigmoid(x) + B\n",
    "\n",
    "\n",
    "def should_fit_bazin(tb, fb, eb, min_points=8, amp_sigma=3.0):\n",
    "    \"\"\"\n",
    "    Gate: only fit when there is enough data and a detectable transient-like signal.\n",
    "    \"\"\"\n",
    "    tb = np.asarray(tb, float)\n",
    "    fb = np.asarray(fb, float)\n",
    "    eb = np.asarray(eb, float)\n",
    "\n",
    "    if len(tb) < min_points:\n",
    "        return False\n",
    "\n",
    "    mederr = float(np.median(eb)) if np.isfinite(np.median(eb)) else np.inf\n",
    "    if not np.isfinite(mederr) or mederr <= 0:\n",
    "        return False\n",
    "\n",
    "    amp = float(np.percentile(fb, 95) - np.percentile(fb, 5))\n",
    "    if not np.isfinite(amp) or amp < amp_sigma * mederr:\n",
    "        return False\n",
    "\n",
    "    if float(np.std(fb)) < 1e-10:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def fit_bazin(tb, fb, eb):\n",
    "    \"\"\"\n",
    "    Returns (A, t0, trise, tfall, B, chi2_red) on success.\n",
    "    Returns (nan...nan) on failure or gate fails.\n",
    "    \"\"\"\n",
    "    nan_out = (np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "\n",
    "    tb = np.asarray(tb, float)\n",
    "    fb = np.asarray(fb, float)\n",
    "    eb = np.asarray(eb, float)\n",
    "\n",
    "    order = np.argsort(tb)\n",
    "    t = tb[order]\n",
    "    f = fb[order]\n",
    "    e = eb[order]\n",
    "\n",
    "    m = np.isfinite(t) & np.isfinite(f) & np.isfinite(e)\n",
    "    t, f, e = t[m], f[m], e[m]\n",
    "    if len(t) < 3:\n",
    "        return nan_out\n",
    "\n",
    "    e = np.maximum(e, 1e-6)\n",
    "\n",
    "    if not should_fit_bazin(t, f, e, min_points=8, amp_sigma=3.0):\n",
    "        return nan_out\n",
    "\n",
    "    B0 = float(np.median(f))\n",
    "    A0 = float(max(1e-6, np.percentile(f, 95) - B0))\n",
    "    t0_0 = float(t[int(np.argmax(f))])\n",
    "    tr0 = 20.0\n",
    "    tf0 = 60.0\n",
    "    p0 = [A0, t0_0, tr0, tf0, B0]\n",
    "\n",
    "    tmin, tmax = float(t.min()), float(t.max())\n",
    "    iqr = float(np.percentile(f, 75) - np.percentile(f, 25))\n",
    "    amp = float(max(1e-6, np.percentile(f, 95) - np.percentile(f, 5)))\n",
    "\n",
    "    lo = [0.0, tmin - 50.0, 0.5, 1.0, B0 - 5.0 * (iqr + 1e-6)]\n",
    "    hi = [10.0 * amp, tmax + 50.0, 200.0, 600.0, B0 + 5.0 * (iqr + 1e-6)]\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(\n",
    "            bazin_stable, t, f,\n",
    "            p0=p0,\n",
    "            sigma=e,\n",
    "            absolute_sigma=True,\n",
    "            bounds=(lo, hi),\n",
    "            maxfev=5000\n",
    "        )\n",
    "\n",
    "        fhat = bazin_stable(t, *popt)\n",
    "        resid = (f - fhat) / e\n",
    "        chi2 = float(np.sum(resid * resid))\n",
    "        dof = max(1, len(t) - len(popt))\n",
    "        chi2_red = chi2 / dof\n",
    "\n",
    "        A, t0, trise, tfall, B = [float(x) for x in popt]\n",
    "        return (A, t0, trise, tfall, B, float(chi2_red))\n",
    "\n",
    "    except Exception:\n",
    "        return nan_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e4df81",
   "metadata": {},
   "source": [
    "## Seasonality Features\n",
    "\n",
    "Astronomical time-series often have seasonal observing gaps.\n",
    "\n",
    "This computes:\n",
    "- `n_seasons`: number of observing segments separated by gaps > `SEASON_GAP_DAYS`\n",
    "- `season_maxspan`: max time span of any segment\n",
    "- `season_meanspan`: mean time span of segments\n",
    "\n",
    "These are useful because:\n",
    "- Some classes are observed continuously, others only show up in narrow windows\n",
    "- Seasonal gaps can distort measured peak width/shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fa589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality_features(tb):\n",
    "    tb = np.asarray(tb, float)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    dt = np.diff(tb)\n",
    "    breaks = np.where(dt > SEASON_GAP_DAYS)[0]\n",
    "    seg_starts = [0] + (breaks + 1).tolist()\n",
    "    seg_ends = breaks.tolist() + [len(tb) - 1]\n",
    "    spans = []\n",
    "    for s, e in zip(seg_starts, seg_ends):\n",
    "        spans.append(tb[e] - tb[s])\n",
    "    spans = np.asarray(spans, float)\n",
    "    n_seasons = float(len(spans))\n",
    "    return n_seasons, float(np.max(spans)), float(np.mean(spans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97bf37f",
   "metadata": {},
   "source": [
    "## Structure Function Features (Variability vs Timescale)\n",
    "\n",
    "A structure function measures typical variability amplitude at different time lags.\n",
    "\n",
    "For each lag in `SF_LAGS`, compute:\n",
    "- `sf_medabs_<lag>`: median absolute flux difference for pairs separated by that lag (± tolerance)\n",
    "- `sf_n_<lag>`: number of qualifying pairs\n",
    "\n",
    "This helps differentiate:\n",
    "- smooth transients vs noisy stochastic variability (AGN-like)\n",
    "- fast vs slow-changing behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8271b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_function_lags(tb, fb, lags=SF_LAGS):\n",
    "    tb = np.asarray(tb, float)\n",
    "    fb = np.asarray(fb, float)\n",
    "    n = len(tb)\n",
    "    out = {}\n",
    "    if n < 6:\n",
    "        for lag in lags:\n",
    "            out[f\"sf_medabs_{int(lag)}\"] = np.nan\n",
    "            out[f\"sf_n_{int(lag)}\"] = 0.0\n",
    "        return out\n",
    "\n",
    "    for lag in lags:\n",
    "        tol = max(2.0, 0.2 * lag)\n",
    "        vals = []\n",
    "        for i in range(n - 1):\n",
    "            dt = tb[i + 1:] - tb[i]\n",
    "            m = (dt >= (lag - tol)) & (dt <= (lag + tol))\n",
    "            if np.any(m):\n",
    "                dif = np.abs(fb[i + 1:][m] - fb[i])\n",
    "                vals.extend(dif.tolist())\n",
    "        if len(vals) == 0:\n",
    "            out[f\"sf_medabs_{int(lag)}\"] = np.nan\n",
    "            out[f\"sf_n_{int(lag)}\"] = 0.0\n",
    "        else:\n",
    "            out[f\"sf_medabs_{int(lag)}\"] = float(np.median(vals))\n",
    "            out[f\"sf_n_{int(lag)}\"] = float(len(vals))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4236a2",
   "metadata": {},
   "source": [
    "## Peak Timing / Peak Flux vs Wavelength (Rest-frame)\n",
    "\n",
    "This fits a simple linear trend across filters in rest-frame wavelength:\n",
    "\n",
    "- For a per-band value `v_b` (ex: peak flux), use x = λ_rest and fit y = slope*x + intercept\n",
    "- Returns slope/intercept and fit quality (R²)\n",
    "\n",
    "This captures chromatic behavior:\n",
    "- some classes peak earlier in blue, later in red\n",
    "- peak flux distribution across wavelength is a crude \"temperature/SED\" proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcac446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_vs_wavelength_slope(tpeak_by_band, val_by_band, z=0.0):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in FILTERS:\n",
    "        v = val_by_band.get(b, np.nan)\n",
    "        t = tpeak_by_band.get(b, np.nan)\n",
    "        if np.isfinite(v):\n",
    "            lam = float(EFF_WL_AA[b] / (1.0 + float(z)))\n",
    "            xs.append(lam)\n",
    "            ys.append(float(v))\n",
    "    xs = np.asarray(xs, float)\n",
    "    ys = np.asarray(ys, float)\n",
    "    if len(xs) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    try:\n",
    "        slope, intercept = np.polyfit(xs, ys, 1)\n",
    "        yhat = slope * xs + intercept\n",
    "        ss_res = float(np.sum((ys - yhat) ** 2))\n",
    "        ss_tot = float(np.sum((ys - np.mean(ys)) ** 2)) + EPS\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "        return float(slope), float(intercept), float(r2)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62de438",
   "metadata": {},
   "source": [
    "## SED Slope: log(flux) vs log(wavelength) at Specific Times\n",
    "\n",
    "At a chosen time `t0` (ex: r-band peak), interpolate flux and error per band.\n",
    "\n",
    "Then fit:\n",
    "- x = log(λ_rest)\n",
    "- y = log(flux)\n",
    "with weights based on relative flux uncertainty.\n",
    "\n",
    "Outputs:\n",
    "- `sed_logflux_loglambda_slope_*`: spectral slope proxy\n",
    "- `sed_*_r2`: fit quality\n",
    "- `sed_*_nbands`: how many bands contributed\n",
    "\n",
    "This is basically a compact color/SED summary without needing full SED modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4293c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sed_logflux_vs_loglambda_at_time(band_tb, band_fb, band_eb, t0, z=0.0):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ws = []\n",
    "    for b in FILTERS:\n",
    "        tb = band_tb.get(b, None)\n",
    "        fb = band_fb.get(b, None)\n",
    "        eb = band_eb.get(b, None)\n",
    "        if tb is None or fb is None or eb is None:\n",
    "            continue\n",
    "        f = interp_flux_at_time(tb, fb, t0)\n",
    "        e = interp_err_at_time(tb, eb, t0)\n",
    "        if not np.isfinite(f) or not np.isfinite(e):\n",
    "            continue\n",
    "        if f <= 0:\n",
    "            continue\n",
    "        lam_rest = float(EFF_WL_AA[b] / (1.0 + float(z)))\n",
    "        xs.append(np.log(lam_rest + EPS))\n",
    "        ys.append(np.log(f + EPS))\n",
    "        ws.append(1.0 / ((e / (f + EPS)) ** 2 + EPS))  # weight by relative error\n",
    "    if len(xs) < 2:\n",
    "        return np.nan, np.nan, np.nan, float(len(xs))\n",
    "    xs = np.asarray(xs, float)\n",
    "    ys = np.asarray(ys, float)\n",
    "    ws = np.asarray(ws, float)\n",
    "\n",
    "    try:\n",
    "        W = np.sum(ws)\n",
    "        xbar = np.sum(ws * xs) / (W + EPS)\n",
    "        ybar = np.sum(ws * ys) / (W + EPS)\n",
    "        cov = np.sum(ws * (xs - xbar) * (ys - ybar))\n",
    "        var = np.sum(ws * (xs - xbar) ** 2) + EPS\n",
    "        slope = cov / var\n",
    "        intercept = ybar - slope * xbar\n",
    "        yhat = slope * xs + intercept\n",
    "        ss_res = float(np.sum(ws * (ys - yhat) ** 2))\n",
    "        ss_tot = float(np.sum(ws * (ys - ybar) ** 2)) + EPS\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "        return float(slope), float(intercept), float(r2), float(len(xs))\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, np.nan, float(len(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d8739",
   "metadata": {},
   "source": [
    "# Model 5: Raw-vs-deextinct deltas + seasonality/structure-function + Bazin fits + richer cross-band + upgraded SpecType teacher\n",
    "\n",
    "Differences vs Model 4:\n",
    " - Adds raw (un-corrected) flux stats alongside de-extincted stats, plus \"delta\" features (deextinct minus raw).\n",
    " - Adds observation seasonality features globally and per band (counts of seasons, gap fractions, season spans).\n",
    " - Adds structure function features per band at multiple time lags (captures variability vs timescale).\n",
    " - Always fits Bazin per band (A, t0, trise/tfall, B, chi2red) and adds rest-frame versions of rise/fall.\n",
    " - Adds rise-time metrics (to 20%/50%) and asymmetry ratios (fall/rise) per band.\n",
    " - Adds cross-band ratios (amp_pre, AUC, width50, asym50) and band-to-band correlations (g-r, r-i, i-z).\n",
    " - Adds wavelength-trend features (peak time vs lambda, peak flux vs lambda) and SED slope fits at r-peak and +20d.\n",
    " - SpecType teacher: richer label mapping (splits out SLSN and SNII), uses missing-value flags, exposes spec_topprob.\n",
    "\n",
    "There are ~562 train features, so tables will be kept very compact.\n",
    "\n",
    "## New Features\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|---------|----------|--------------|\n",
    "| `flux_mean_raw` | Mean flux before dust de-extinction correction | Lets the model compare raw vs corrected brightness scale and learn dust-impact patterns |\n",
    "| `flux_std_raw` | Standard deviation of raw flux | Captures variability before correction to detect dust-driven distortions |\n",
    "| `snr_max_raw` | Maximum SNR using raw flux and raw error | Measures best raw detection strength as a correction sanity check |\n",
    "| `fvar_raw` | Fractional variability using raw flux and error | Provides intrinsic-variability proxy before dust adjustment |\n",
    "| `flux_mean_deext_minus_raw` | Difference between corrected and raw mean flux | Direct signal of how strongly extinction correction shifts brightness |\n",
    "| `snrmax_deext_minus_raw` | Difference between corrected and raw max SNR | Measures how much detectability improves after correction |\n",
    "| `n_seasons_global` | Number of observing seasons inferred from large time gaps | Separates single-season vs multi-season coverage patterns |\n",
    "| `gap_frac_gt90` | Fraction of time gaps greater than 90 days | Flags strongly seasonal sampling |\n",
    "| `gap_frac_gt30` | Fraction of time gaps greater than 30 days | Captures moderate sampling fragmentation |\n",
    "| `n_seasons_{b}` | Number of observing seasons in band b | Band-specific sampling structure can differ by class |\n",
    "| `season_maxspan_{b}` | Longest continuous season span in band b | Measures longest uninterrupted coverage window |\n",
    "| `season_meanspan_{b}` | Mean season span in band b | Captures typical continuous coverage length |\n",
    "| `sf_medabs_5_{b}` | Median absolute flux difference at ~5-day lag | Measures short-timescale variability strength |\n",
    "| `sf_n_5_{b}` | Number of pairs used for 5-day lag SF | Reliability indicator for short-lag estimate |\n",
    "| `sf_medabs_10_{b}` | Median absolute flux difference at ~10-day lag | Captures slightly longer-timescale changes |\n",
    "| `sf_n_10_{b}` | Pair count for 10-day lag SF | Reliability indicator |\n",
    "| `sf_medabs_20_{b}` | Median absolute flux difference at ~20-day lag | Mid-scale variability measure |\n",
    "| `sf_n_20_{b}` | Pair count for 20-day lag SF | Reliability indicator |\n",
    "| `sf_medabs_50_{b}` | Median absolute flux difference at ~50-day lag | Long-timescale variability proxy |\n",
    "| `sf_n_50_{b}` | Pair count for 50-day lag SF | Reliability indicator |\n",
    "| `sf_medabs_100_{b}` | Median absolute flux difference at ~100-day lag | Very long-timescale variability proxy |\n",
    "| `sf_n_100_{b}` | Pair count for 100-day lag SF | Reliability indicator |\n",
    "| `bazin_A_{b}` | Bazin model amplitude parameter | Smooth transient strength estimate |\n",
    "| `bazin_t0_{b}_obs` | Bazin peak-time parameter (observed frame) | Parametric peak timing estimate |\n",
    "| `bazin_trise_{b}_obs` | Bazin rise timescale (observed frame) | Encodes rise speed |\n",
    "| `bazin_tfall_{b}_obs` | Bazin decay timescale (observed frame) | Encodes decay speed |\n",
    "| `bazin_B_{b}` | Bazin baseline parameter | Estimates underlying baseline level |\n",
    "| `bazin_chi2red_{b}_obs` | Reduced chi-square of Bazin fit | Fit quality indicator |\n",
    "| `bazin_trise_{b}_rest` | Bazin rise timescale (rest frame) | Intrinsic rise speed |\n",
    "| `bazin_tfall_{b}_rest` | Bazin fall timescale (rest frame) | Intrinsic decay speed |\n",
    "| `t_rise50_{b}_obs` | Time from baseline to 50% amplitude (observed) | Measures rise speed |\n",
    "| `t_rise20_{b}_obs` | Time from baseline to 20% amplitude (observed) | Early-rise behavior |\n",
    "| `t_rise50_{b}_rest` | Rise time to 50% amplitude (rest frame) | Intrinsic rise speed |\n",
    "| `t_rise20_{b}_rest` | Rise time to 20% amplitude (rest frame) | Intrinsic early-rise behavior |\n",
    "| `asym50_{b}_obs` | Fall50 / Rise50 ratio (observed) | Captures peak asymmetry |\n",
    "| `asym50_{b}_rest` | Fall50 / Rise50 ratio (rest frame) | Intrinsic asymmetry measure |\n",
    "| `amppreratio_{a}{b}` | Ratio of pre-baseline amplitudes between bands | Color-dependent peak strength comparison |\n",
    "| `aucratio_{a}{b}_obs` | Ratio of positive AUC between bands | Relative emitted-energy proxy |\n",
    "| `width50ratio_{a}{b}_obs` | Ratio of 50% widths between bands | Cross-band duration contrast |\n",
    "| `asym50ratio_{a}{b}_obs` | Ratio of asymmetry metrics between bands | Cross-band shape contrast |\n",
    "| `corr_gr_obs` | Correlation between g and r band lightcurves | Measures multi-band coherence |\n",
    "| `corr_ri_obs` | Correlation between r and i bands | Same, redder wavelengths |\n",
    "| `corr_iz_obs` | Correlation between i and z bands | Same, further red |\n",
    "| `tpeak_vs_lambda_slope_obs` | Slope of peak-time vs wavelength fit | Detects chromatic timing trends |\n",
    "| `tpeak_vs_lambda_intercept_obs` | Intercept of that regression | Baseline timing offset |\n",
    "| `tpeak_vs_lambda_r2_obs` | R² of peak-time vs wavelength fit | Reliability of chromatic timing trend |\n",
    "| `peakflux_vs_lambda_slope` | Slope of peak-flux vs wavelength fit | Spectral energy trend |\n",
    "| `peakflux_vs_lambda_intercept` | Intercept of flux–wavelength fit | Baseline spectral level |\n",
    "| `peakflux_vs_lambda_r2` | R² of flux–wavelength fit | Reliability of spectral slope |\n",
    "| `sed_logflux_loglambda_slope_rpeak` | Slope of log(flux) vs log(wavelength) at r-peak | Spectral slope at peak |\n",
    "| `sed_logflux_loglambda_r2_rpeak` | R² of SED fit at r-peak | Fit reliability |\n",
    "| `sed_logflux_loglambda_nbands_rpeak` | Number of bands used in SED fit | Coverage reliability |\n",
    "| `sed_slope_rpeak_p20` | SED slope at r-peak + 20 days | Spectral evolution rate |\n",
    "| `sed_r2_rpeak_p20` | R² of SED fit at +20 days | Reliability indicator |\n",
    "| `sed_nbands_rpeak_p20` | Bands used at +20 days | Coverage indicator |\n",
    "| `spec_topprob` | Maximum teacher-model class probability | Teacher confidence summary for meta-learning |\n",
    "\n",
    "## Features from other models\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|---------|----------|--------------|\n",
    "| `n_obs` | Total number of observations across all filters | Coverage proxy; some classes are observed more densely |\n",
    "| `total_time_obs` | Total observed duration (max time − min time) | Separates long-timescale variability from short transients |\n",
    "| `total_time_rest` | Duration corrected by (1+z) time dilation | Makes durations comparable across redshift |\n",
    "| `flux_mean` | Mean dust-corrected flux | Overall brightness level |\n",
    "| `flux_median` | Median dust-corrected flux | Robust brightness estimate |\n",
    "| `flux_std` | Standard deviation of corrected flux | Overall variability strength |\n",
    "| `flux_min` | Minimum corrected flux | Captures deep dips / noise floor |\n",
    "| `flux_max` | Maximum corrected flux | Captures peak brightness |\n",
    "| `flux_mad` | Median absolute deviation | Robust variability measure |\n",
    "| `flux_iqr` | Interquartile range | Robust spread measure |\n",
    "| `flux_skew` | Skewness of flux distribution | Detects asymmetric burst-like shapes |\n",
    "| `flux_kurt_excess` | Excess kurtosis | Detects heavy-tailed spike behavior |\n",
    "| `flux_p5` | 5th percentile flux | Robust low level |\n",
    "| `flux_p25` | 25th percentile flux | Lower quartile |\n",
    "| `flux_p75` | 75th percentile flux | Upper quartile |\n",
    "| `flux_p95` | 95th percentile flux | Robust high level |\n",
    "| `robust_amp_global` | p95 − p5 | Stable global amplitude proxy |\n",
    "| `neg_flux_frac` | Fraction of flux values below zero | Noise-dominated vs real detection signal |\n",
    "| `snr_median` | Median signal-to-noise ratio | Typical detection quality |\n",
    "| `snr_max` | Maximum signal-to-noise ratio | Strongest detection strength |\n",
    "| `median_dt` | Median time gap between observations | Sampling cadence proxy |\n",
    "| `max_gap` | Largest time gap | Detects large seasonal breaks |\n",
    "| `eta_von_neumann` | Von Neumann eta statistic | Smoothness vs randomness indicator |\n",
    "| `chi2_const_global` | Chi-square vs constant model | Detects variability vs flat signal |\n",
    "| `stetsonJ_global_obs` | Stetson J index (observed frame) | Robust correlated variability measure |\n",
    "| `stetsonJ_global_rest` | Stetson J index (rest frame) | Intrinsic variability measure |\n",
    "| `max_slope_global_obs` | Maximum absolute slope (observed) | Fastest brightness change |\n",
    "| `max_slope_global_rest` | Maximum slope (rest frame) | Intrinsic fastest change |\n",
    "| `med_abs_slope_global_obs` | Median absolute slope (observed) | Typical change rate |\n",
    "| `med_abs_slope_global_rest` | Median absolute slope (rest) | Intrinsic change rate |\n",
    "| `slope_global_obs` | Linear trend slope (observed) | Long-term drift indicator |\n",
    "| `slope_global_rest` | Linear trend slope (rest) | Intrinsic drift |\n",
    "| `fvar_global` | Fractional variability | Noise-corrected variability strength |\n",
    "| `Z` | Redshift | Distance and time-dilation proxy |\n",
    "| `log1pZ` | log(1+Z) | Stabilized redshift scale |\n",
    "| `Z_err` | Redshift uncertainty | Reliability of distance estimate |\n",
    "| `log1pZerr` | log(1+Z_err) | Stabilized uncertainty scale |\n",
    "| `EBV` | Dust extinction value | Measures dust impact |\n",
    "| `n_filters_present` | Number of filters with data | Multi-band coverage indicator |\n",
    "| `total_obs` | Total observations across bands | Coverage strength |\n",
    "| `n_{b}` | Number of observations in band b | Band completeness differs by class |\n",
    "| `p5_{b}` | 5th percentile flux in band b | Robust low level |\n",
    "| `p25_{b}` | 25th percentile | Lower quartile |\n",
    "| `p75_{b}` | 75th percentile | Upper quartile |\n",
    "| `p95_{b}` | 95th percentile | Robust high level |\n",
    "| `robust_amp_{b}` | p95 − p5 in band b | Stable band amplitude |\n",
    "| `mad_{b}` | Median absolute deviation | Robust variability |\n",
    "| `iqr_{b}` | Interquartile range | Robust spread |\n",
    "| `mad_over_std_{b}` | MAD / std ratio | Outlier sensitivity indicator |\n",
    "| `eta_{b}` | Von Neumann eta | Smoothness vs noise |\n",
    "| `chi2_const_{b}` | Chi-square vs constant | Variability detector |\n",
    "| `stetsonJ_{b}_obs` | Stetson J (observed) | Correlated variability |\n",
    "| `stetsonJ_{b}_rest` | Stetson J (rest) | Intrinsic correlated variability |\n",
    "| `fvar_{b}` | Fractional variability | Normalized variability strength |\n",
    "| `snrmax_{b}` | Maximum SNR | Best detection strength |\n",
    "| `baseline_pre_{b}` | Estimated pre-peak baseline | Reference level for amplitude |\n",
    "| `amp_{b}` | Peak − median flux | Simple amplitude |\n",
    "| `amp_pre_{b}` | Peak − pre-peak baseline | Cleaner transient amplitude |\n",
    "| `tpeak_{b}_obs` | Peak time (observed frame) | Band timing behavior |\n",
    "| `tpeak_{b}_rest` | Peak time (rest frame) | Intrinsic timing |\n",
    "| `peak_dominance_{b}` | Peak / baseline noise scale | Peak significance |\n",
    "| `std_ratio_prepost_{b}` | Pre/post peak std ratio | Stability vs post-peak chaos |\n",
    "| `width50_{b}_obs` | Width above 50% amplitude (obs) | Duration at mid level |\n",
    "| `width80_{b}_obs` | Width above 80% amplitude (obs) | Peak sharpness |\n",
    "| `width50_{b}_rest` | Width50 (rest) | Intrinsic duration |\n",
    "| `width80_{b}_rest` | Width80 (rest) | Intrinsic peak shape |\n",
    "| `t_fall50_{b}_obs` | Fall time to 50% (obs) | Decay speed |\n",
    "| `t_fall20_{b}_obs` | Fall time to 20% (obs) | Late decay |\n",
    "| `t_fall50_{b}_rest` | Fall50 (rest) | Intrinsic decay |\n",
    "| `t_fall20_{b}_rest` | Fall20 (rest) | Intrinsic late decay |\n",
    "| `sharp50_{b}_obs` | Amplitude / width50 (obs) | Spike sharpness |\n",
    "| `sharp50_{b}_rest` | Amplitude / width50 (rest) | Intrinsic sharpness |\n",
    "| `postpeak_monotone_frac_{b}` | Fraction monotonic after peak | Smooth decay vs noisy |\n",
    "| `n_peaks_{b}` | Significant peak count | Multi-peak vs single transient |\n",
    "| `n_rebrighten_{b}` | Rebrightening count | Secondary bump behavior |\n",
    "| `decay_pl_slope_{b}_obs` | Power-law decay slope (obs) | Decay steepness |\n",
    "| `decay_pl_r2_{b}_obs` | Fit R² (obs) | Fit reliability |\n",
    "| `decay_pl_npts_{b}_obs` | Points used (obs) | Support size |\n",
    "| `decay_pl_slope_{b}_rest` | Power-law slope (rest) | Intrinsic decay |\n",
    "| `decay_pl_r2_{b}_rest` | Fit R² (rest) | Reliability |\n",
    "| `decay_pl_npts_{b}_rest` | Points used (rest) | Support size |\n",
    "| `tpeak_std_obs` | Std of peak times across bands (obs) | Peak alignment indicator |\n",
    "| `tpeak_std_rest` | Std of peak times (rest) | Intrinsic alignment |\n",
    "| `tpeakdiff_{a}{b}_obs` | Peak time difference (obs) | Chromatic lag signal |\n",
    "| `tpeakdiff_{a}{b}_rest` | Peak time difference (rest) | Intrinsic lag |\n",
    "| `peakratio_{a}{b}` | Peak flux ratio | Peak color proxy |\n",
    "| `color_gr_at_rpeak_obs` | g−r color at r-peak | Spectral color at peak |\n",
    "| `color_ri_at_rpeak_obs` | r−i color at r-peak | Red color proxy |\n",
    "| `color_gr_rpeak_p20_obs` | g−r at +20d | Color evolution |\n",
    "| `color_ri_rpeak_p20_obs` | r−i at +20d | Color evolution |\n",
    "| `color_gr_rpeak_p40_obs` | g−r at +40d | Slower evolution |\n",
    "| `color_ri_rpeak_p40_obs` | r−i at +40d | Slower evolution |\n",
    "| `color_gr_slope20_obs` | g−r slope over 20d | Early color change rate |\n",
    "| `color_ri_slope20_obs` | r−i slope over 20d | Early color change |\n",
    "| `color_gr_slope40_obs` | g−r slope over 40d | Longer color trend |\n",
    "| `color_ri_slope40_obs` | r−i slope over 40d | Longer trend |\n",
    "| `p_spec_{c}` | Teacher probability for class c | Soft-label prior signal |\n",
    "| `spec_entropy` | Entropy of teacher probs | Teacher uncertainty |\n",
    "| `spec_topprob` | Max teacher probability | Teacher confidence summary |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9427600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_object(lc_raw, z, z_err, ebv):\n",
    "    feats = {}\n",
    "\n",
    "    # Sort observations by time so time-based calculations make sense\n",
    "    lc = lc_raw.sort_values(\"Time (MJD)\").reset_index(drop=True)\n",
    "\n",
    "    # Extract time values and filter (band) labels\n",
    "    t = lc[\"Time (MJD)\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    # If there are no observations, return minimal info\n",
    "    if len(t) == 0:\n",
    "        feats[\"n_obs\"] = 0\n",
    "        return feats\n",
    "\n",
    "    # Make sure metadata fields are valid numbers (avoid NaNs / strings / missing values)\n",
    "    z = safe_float(z, default=0.0)                     # redshift (distance proxy)\n",
    "    z_err = safe_float(z_err, default=0.0)             # redshift uncertainty\n",
    "    ebv = safe_float(ebv, default=np.nan)              # dust amount (can be missing)\n",
    "\n",
    "    # Convert time to start at 0 (relative time axis)\n",
    "    t_rel = t - t.min()\n",
    "\n",
    "    # Convert observed time to intrinsic time of the object\n",
    "    # Distant objects appear to evolve slower, so divide by (1 + z)\n",
    "    t_rest = t_rel / (1.0 + z)\n",
    "\n",
    "    # Extract raw flux/error (before dust correction)\n",
    "    flux_raw = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    err_raw = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "\n",
    "    # Correct brightness values for dust in the Milky Way\n",
    "    # (dust makes objects look dimmer than they really are)\n",
    "    flux_corr, err_corr = deextinct_lightcurve(lc, ebv)\n",
    "\n",
    "    # Basic observation statistics\n",
    "    feats[\"n_obs\"] = int(len(t))                                  # total number of measurements\n",
    "    feats[\"total_time_obs\"] = float(t_rel.max() - t_rel.min())    # total observed duration\n",
    "    feats[\"total_time_rest\"] = float(t_rest.max() - t_rest.min()) # duration corrected for distance effects\n",
    "\n",
    "    # Global brightness statistics (after dust correction)\n",
    "    feats[\"flux_mean\"] = float(np.mean(flux_corr))                # average brightness level\n",
    "    feats[\"flux_median\"] = float(np.median(flux_corr))            # robust typical brightness\n",
    "    feats[\"flux_std\"] = float(np.std(flux_corr))                  # overall variability\n",
    "    feats[\"flux_min\"] = float(np.min(flux_corr))                  # dimmest point\n",
    "    feats[\"flux_max\"] = float(np.max(flux_corr))                  # brightest point\n",
    "\n",
    "    # Robust statistics that are less sensitive to outliers\n",
    "    feats[\"flux_mad\"] = median_abs_dev(flux_corr)                 # median absolute deviation\n",
    "    feats[\"flux_iqr\"] = iqr(flux_corr)                            # interquartile range (Q3 - Q1)\n",
    "\n",
    "    # Distribution shape features\n",
    "    feats[\"flux_skew\"] = skewness(flux_corr)                      # asymmetry of values\n",
    "    feats[\"flux_kurt_excess\"] = kurtosis_excess(flux_corr)        # tail heaviness / spikiness\n",
    "\n",
    "    # Robust amplitude using percentiles (stable against a few extreme points)\n",
    "    p5, p25, p75, p95 = np.percentile(flux_corr, [5, 25, 75, 95])\n",
    "    feats[\"flux_p5\"] = float(p5)\n",
    "    feats[\"flux_p25\"] = float(p25)\n",
    "    feats[\"flux_p75\"] = float(p75)\n",
    "    feats[\"flux_p95\"] = float(p95)\n",
    "    feats[\"robust_amp_global\"] = float(p95 - p5)                  # robust amplitude proxy\n",
    "\n",
    "    # Fraction of measurements that are below zero\n",
    "    # (often indicates noise-dominated detections)\n",
    "    feats[\"neg_flux_frac\"] = float(np.mean(flux_corr < 0))\n",
    "\n",
    "    # Signal-to-noise ratio summaries (dust-corrected)\n",
    "    snr = np.abs(flux_corr) / (err_corr + EPS)\n",
    "    feats[\"snr_median\"] = float(np.median(snr))                   # typical signal quality\n",
    "    feats[\"snr_max\"] = float(np.max(snr))                         # strongest detection\n",
    "\n",
    "    # Raw-flux sanity features (captures how dust correction shifts statistics)\n",
    "    feats[\"flux_mean_raw\"] = float(np.mean(flux_raw))             # mean before de-extinction\n",
    "    feats[\"flux_std_raw\"] = float(np.std(flux_raw))               # variability before de-extinction\n",
    "    feats[\"snr_max_raw\"] = float(np.max(np.abs(flux_raw) / (err_raw + EPS)))  # raw best SNR\n",
    "    feats[\"fvar_raw\"] = fractional_variability(flux_raw, err_raw) # raw fractional variability\n",
    "\n",
    "    # Dust-correction “delta” features (how much correction changes the signal)\n",
    "    feats[\"flux_mean_deext_minus_raw\"] = float(feats[\"flux_mean\"] - feats[\"flux_mean_raw\"])\n",
    "    feats[\"snrmax_deext_minus_raw\"] = float(feats[\"snr_max\"] - feats[\"snr_max_raw\"])\n",
    "\n",
    "    # Observation timing + seasonality (global)\n",
    "    if len(t_rel) >= 2:\n",
    "        dt = np.diff(t_rel)\n",
    "        feats[\"median_dt\"] = float(np.median(dt))                 # typical time between observations\n",
    "        feats[\"max_gap\"] = float(np.max(dt))                      # largest observation gap\n",
    "\n",
    "        # Seasonality proxies: large gaps often indicate separate observing seasons\n",
    "        feats[\"n_seasons_global\"] = float(np.sum(dt > SEASON_GAP_DAYS) + 1)  # number of seasons\n",
    "        feats[\"gap_frac_gt90\"] = float(np.mean(dt > SEASON_GAP_DAYS))        # fraction of gaps > 90d\n",
    "        feats[\"gap_frac_gt30\"] = float(np.mean(dt > 30.0))                  # fraction of gaps > 30d\n",
    "    else:\n",
    "        feats[\"median_dt\"] = np.nan\n",
    "        feats[\"max_gap\"] = np.nan\n",
    "        feats[\"n_seasons_global\"] = np.nan\n",
    "        feats[\"gap_frac_gt90\"] = np.nan\n",
    "        feats[\"gap_frac_gt30\"] = np.nan\n",
    "\n",
    "    # Global time-series variability diagnostics\n",
    "    feats[\"eta_von_neumann\"] = von_neumann_eta(flux_corr)              # smoothness vs noise proxy\n",
    "    feats[\"chi2_const_global\"] = chi2_to_constant(flux_corr, err_corr) # variability vs constant model\n",
    "\n",
    "    feats[\"stetsonJ_global_obs\"] = stetson_J_consecutive(t_rel, flux_corr, err_corr)\n",
    "    feats[\"stetsonJ_global_rest\"] = stetson_J_consecutive(t_rest, flux_corr, err_corr)\n",
    "\n",
    "    # Global slope features (obs + rest frame)\n",
    "    feats[\"max_slope_global_obs\"] = max_slope(t_rel, flux_corr)        # fastest brightness change (obs)\n",
    "    feats[\"max_slope_global_rest\"] = max_slope(t_rest, flux_corr)      # fastest brightness change (rest)\n",
    "\n",
    "    feats[\"med_abs_slope_global_obs\"] = median_abs_slope(t_rel, flux_corr)   # typical change rate (obs)\n",
    "    feats[\"med_abs_slope_global_rest\"] = median_abs_slope(t_rest, flux_corr) # typical change rate (rest)\n",
    "\n",
    "    feats[\"slope_global_obs\"] = linear_slope(t_rel, flux_corr)         # best-fit linear trend (obs)\n",
    "    feats[\"slope_global_rest\"] = linear_slope(t_rest, flux_corr)       # best-fit linear trend (rest)\n",
    "\n",
    "    feats[\"fvar_global\"] = fractional_variability(flux_corr, err_corr) # noise-corrected variability\n",
    "\n",
    "    # Metadata features\n",
    "    feats[\"Z\"] = float(z)                           # distance proxy (redshift)\n",
    "    feats[\"log1pZ\"] = float(np.log1p(max(0.0, z)))  # compressed redshift scale\n",
    "    feats[\"Z_err\"] = float(max(0.0, z_err))         # clamp negative uncertainty to 0\n",
    "    feats[\"log1pZerr\"] = float(np.log1p(max(0.0, feats[\"Z_err\"])))     # compressed uncertainty scale\n",
    "    feats[\"EBV\"] = ebv                              # dust amount\n",
    "\n",
    "    # Counters for band coverage\n",
    "    feats[\"n_filters_present\"] = 0                  # how many bands have >= 1 observation\n",
    "    feats[\"total_obs\"] = 0                          # total observations across all bands\n",
    "\n",
    "    # Storage for cross-band timing/color/SED features later\n",
    "    band_tpeak_obs = {}                             # per-band peak time (obs frame)\n",
    "    band_tpeak_rest = {}                            # per-band peak time (rest frame)\n",
    "    band_peak_flux = {}                             # per-band peak flux\n",
    "\n",
    "    band_tb_obs = {}                                # per-band time arrays (obs frame)\n",
    "    band_tb_rest = {}                               # per-band time arrays (rest frame)\n",
    "    band_fb = {}                                    # per-band flux arrays (dust-corrected)\n",
    "    band_eb = {}                                    # per-band error arrays (dust-corrected)\n",
    "\n",
    "    # Loop over each wavelength band (u, g, r, i, z, y)\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        nb = int(np.sum(m))\n",
    "\n",
    "        # Number of observations in this band\n",
    "        feats[f\"n_{b}\"] = nb\n",
    "        feats[\"total_obs\"] += nb\n",
    "\n",
    "        # Initialize all band features as missing by default\n",
    "        keys = [\n",
    "            f\"amp_{b}\", f\"amp_pre_{b}\", f\"baseline_pre_{b}\", f\"robust_amp_{b}\",\n",
    "            f\"tpeak_{b}_obs\", f\"tpeak_{b}_rest\",\n",
    "            f\"width50_{b}_obs\", f\"width50_{b}_rest\",\n",
    "            f\"width80_{b}_obs\", f\"width80_{b}_rest\",\n",
    "            f\"auc_pos_{b}_obs\", f\"auc_pos_{b}_rest\",\n",
    "            f\"snrmax_{b}\", f\"eta_{b}\", f\"chi2_const_{b}\",\n",
    "            f\"slope_{b}_obs\", f\"slope_{b}_rest\",\n",
    "            f\"maxslope_{b}_obs\", f\"maxslope_{b}_rest\",\n",
    "            f\"stetsonJ_{b}_obs\", f\"stetsonJ_{b}_rest\",\n",
    "            f\"p5_{b}\", f\"p25_{b}\", f\"p75_{b}\", f\"p95_{b}\",\n",
    "            f\"mad_{b}\", f\"iqr_{b}\", f\"mad_over_std_{b}\", f\"fvar_{b}\",\n",
    "            f\"t_fall50_{b}_obs\", f\"t_fall20_{b}_obs\", f\"t_fall50_{b}_rest\", f\"t_fall20_{b}_rest\",\n",
    "            f\"t_rise50_{b}_obs\", f\"t_rise20_{b}_obs\", f\"t_rise50_{b}_rest\", f\"t_rise20_{b}_rest\",\n",
    "            f\"asym50_{b}_obs\", f\"asym50_{b}_rest\",\n",
    "            f\"sharp50_{b}_obs\", f\"sharp50_{b}_rest\",\n",
    "            f\"peak_dominance_{b}\", f\"std_ratio_prepost_{b}\",\n",
    "            f\"n_peaks_{b}\", f\"postpeak_monotone_frac_{b}\", f\"n_rebrighten_{b}\",\n",
    "            f\"decay_pl_slope_{b}_obs\", f\"decay_pl_r2_{b}_obs\", f\"decay_pl_npts_{b}_obs\",\n",
    "            f\"decay_pl_slope_{b}_rest\", f\"decay_pl_r2_{b}_rest\", f\"decay_pl_npts_{b}_rest\",\n",
    "\n",
    "            # Seasonality and structure function per band\n",
    "            f\"n_seasons_{b}\", f\"season_maxspan_{b}\", f\"season_meanspan_{b}\",\n",
    "            f\"sf_medabs_5_{b}\", f\"sf_n_5_{b}\",\n",
    "            f\"sf_medabs_10_{b}\", f\"sf_n_10_{b}\",\n",
    "            f\"sf_medabs_20_{b}\", f\"sf_n_20_{b}\",\n",
    "            f\"sf_medabs_50_{b}\", f\"sf_n_50_{b}\",\n",
    "            f\"sf_medabs_100_{b}\", f\"sf_n_100_{b}\",\n",
    "\n",
    "            # Bazin shape fit parameters (obs + rest)\n",
    "            f\"bazin_A_{b}\", f\"bazin_t0_{b}_obs\",\n",
    "            f\"bazin_trise_{b}_obs\", f\"bazin_tfall_{b}_obs\",\n",
    "            f\"bazin_B_{b}\", f\"bazin_chi2red_{b}_obs\",\n",
    "            f\"bazin_trise_{b}_rest\", f\"bazin_tfall_{b}_rest\",\n",
    "        ]\n",
    "        for k in keys:\n",
    "            feats[k] = np.nan\n",
    "\n",
    "        # Skip bands with no data\n",
    "        if nb == 0:\n",
    "            continue\n",
    "\n",
    "        feats[\"n_filters_present\"] += 1\n",
    "\n",
    "        # Extract time, brightness, and error for this band\n",
    "        tb_obs = t_rel[m]\n",
    "        fb = flux_corr[m]\n",
    "        eb = err_corr[m]\n",
    "\n",
    "        # Sort observations within the band by time\n",
    "        order = np.argsort(tb_obs)\n",
    "        tb_obs = tb_obs[order]\n",
    "        fb = fb[order]\n",
    "        eb = eb[order]\n",
    "\n",
    "        # Convert to intrinsic time scale\n",
    "        tb_rest = tb_obs / (1.0 + z)\n",
    "\n",
    "        # Cache arrays for later cross-band operations (correlations, SED fits, colors)\n",
    "        band_tb_obs[b] = tb_obs\n",
    "        band_tb_rest[b] = tb_rest\n",
    "        band_fb[b] = fb\n",
    "        band_eb[b] = eb\n",
    "\n",
    "        # Seasonality features for this band\n",
    "        ns, maxsp, meansp = seasonality_features(tb_obs)\n",
    "        feats[f\"n_seasons_{b}\"] = ns\n",
    "        feats[f\"season_maxspan_{b}\"] = maxsp\n",
    "        feats[f\"season_meanspan_{b}\"] = meansp\n",
    "\n",
    "        # Structure function features\n",
    "        sf = structure_function_lags(tb_obs, fb, lags=SF_LAGS)\n",
    "        for lag in SF_LAGS:\n",
    "            feats[f\"sf_medabs_{int(lag)}_{b}\"] = sf.get(f\"sf_medabs_{int(lag)}\", np.nan)\n",
    "            feats[f\"sf_n_{int(lag)}_{b}\"] = sf.get(f\"sf_n_{int(lag)}\", 0.0)\n",
    "\n",
    "        # Robust per-band amplitude using percentiles (stable against outliers)\n",
    "        p5b, p25b, p75b, p95b = np.percentile(fb, [5, 25, 75, 95])\n",
    "        feats[f\"p5_{b}\"] = float(p5b)\n",
    "        feats[f\"p25_{b}\"] = float(p25b)\n",
    "        feats[f\"p75_{b}\"] = float(p75b)\n",
    "        feats[f\"p95_{b}\"] = float(p95b)\n",
    "        feats[f\"robust_amp_{b}\"] = float(p95b - p5b)\n",
    "\n",
    "        # Robust variability summaries\n",
    "        feats[f\"mad_{b}\"] = median_abs_dev(fb)\n",
    "        feats[f\"iqr_{b}\"] = iqr(fb)\n",
    "        stdb = float(np.std(fb))\n",
    "        feats[f\"mad_over_std_{b}\"] = float(feats[f\"mad_{b}\"] / (stdb + EPS))\n",
    "\n",
    "        # Estimate a pre-peak baseline using early observations\n",
    "        base_pre, mad_pre, mederr_pre = pre_peak_baseline(tb_obs, fb, eb, frac=PRE_BASE_FRAC)\n",
    "        feats[f\"baseline_pre_{b}\"] = float(base_pre) if np.isfinite(base_pre) else np.nan\n",
    "\n",
    "        # Identify peak flux and peak time\n",
    "        pidx = int(np.argmax(fb))\n",
    "        peak_flux = float(fb[pidx])\n",
    "        tpeak_obs = float(tb_obs[pidx])\n",
    "        tpeak_rest = float(tb_rest[pidx])\n",
    "\n",
    "        # Amplitude relative to two different baselines\n",
    "        amp_median = peak_flux - float(np.median(fb))                             # peak relative to median\n",
    "        amp_pre = peak_flux - base_pre if np.isfinite(base_pre) else np.nan       # peak relative to pre-peak baseline\n",
    "\n",
    "        feats[f\"amp_{b}\"] = float(amp_median)\n",
    "        feats[f\"amp_pre_{b}\"] = float(amp_pre) if np.isfinite(amp_pre) else np.nan\n",
    "\n",
    "        feats[f\"tpeak_{b}_obs\"] = tpeak_obs\n",
    "        feats[f\"tpeak_{b}_rest\"] = tpeak_rest\n",
    "        feats[f\"snrmax_{b}\"] = float(np.max(np.abs(fb) / (eb + EPS)))              # best detection quality\n",
    "\n",
    "        # Band-level variability diagnostics\n",
    "        feats[f\"eta_{b}\"] = von_neumann_eta(fb)\n",
    "        feats[f\"chi2_const_{b}\"] = chi2_to_constant(fb, eb)\n",
    "\n",
    "        feats[f\"slope_{b}_obs\"] = linear_slope(tb_obs, fb)\n",
    "        feats[f\"slope_{b}_rest\"] = linear_slope(tb_rest, fb)\n",
    "\n",
    "        feats[f\"maxslope_{b}_obs\"] = max_slope(tb_obs, fb)\n",
    "        feats[f\"maxslope_{b}_rest\"] = max_slope(tb_rest, fb)\n",
    "\n",
    "        feats[f\"stetsonJ_{b}_obs\"] = stetson_J_consecutive(tb_obs, fb, eb)\n",
    "        feats[f\"stetsonJ_{b}_rest\"] = stetson_J_consecutive(tb_rest, fb, eb)\n",
    "\n",
    "        feats[f\"fvar_{b}\"] = fractional_variability(fb, eb)\n",
    "\n",
    "        # Bazin fit parameters (smooth transient model)\n",
    "        A, t0, trise, tfall, B, chi2 = fit_bazin(tb_obs, fb, eb)\n",
    "        feats[f\"bazin_A_{b}\"] = A\n",
    "        feats[f\"bazin_t0_{b}_obs\"] = t0\n",
    "        feats[f\"bazin_trise_{b}_obs\"] = trise\n",
    "        feats[f\"bazin_tfall_{b}_obs\"] = tfall\n",
    "        feats[f\"bazin_B_{b}\"] = B\n",
    "        feats[f\"bazin_chi2red_{b}_obs\"] = chi2\n",
    "\n",
    "        # Convert Bazin times to rest frame (distance/time-dilation corrected)\n",
    "        feats[f\"bazin_trise_{b}_rest\"] = trise / (1.0 + z) if np.isfinite(trise) else np.nan\n",
    "        feats[f\"bazin_tfall_{b}_rest\"] = tfall / (1.0 + z) if np.isfinite(tfall) else np.nan\n",
    "\n",
    "        # Peak morphology features only make sense if we have a positive transient above baseline\n",
    "        if np.isfinite(amp_pre) and amp_pre > 0:\n",
    "            feats[f\"peak_dominance_{b}\"] = float(amp_pre / (mad_pre + EPS))        # peak relative to baseline noise\n",
    "\n",
    "            # Pre vs post variability ratio\n",
    "            pre_seg = fb[:max(2, pidx)]\n",
    "            post_seg = fb[pidx:]\n",
    "            std_pre = float(np.std(pre_seg)) if len(pre_seg) >= 2 else np.nan\n",
    "            std_post = float(np.std(post_seg)) if len(post_seg) >= 2 else np.nan\n",
    "            if np.isfinite(std_pre) and np.isfinite(std_post):\n",
    "                feats[f\"std_ratio_prepost_{b}\"] = float(std_pre / (std_post + EPS))\n",
    "\n",
    "            feats[f\"postpeak_monotone_frac_{b}\"] = float(postpeak_monotonicity(tb_obs, fb, pidx))\n",
    "            feats[f\"n_peaks_{b}\"] = float(count_significant_peaks(tb_obs, fb, eb, base_pre, k_sigma=PEAK_SIGMA_K))\n",
    "            feats[f\"n_rebrighten_{b}\"] = float(count_rebrighten(tb_obs, fb, base_pre, amp_pre, pidx, frac=REBRIGHT_FRAC))\n",
    "\n",
    "            # Fall times from peak to given fractional levels (observed + rest frame)\n",
    "            feats[f\"t_fall50_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "            feats[f\"t_fall50_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "\n",
    "            # Rise times from baseline to given fractional levels\n",
    "            feats[f\"t_rise50_{b}_obs\"] = float(rise_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_rise20_{b}_obs\"] = float(rise_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "            feats[f\"t_rise50_{b}_rest\"] = float(rise_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_rise20_{b}_rest\"] = float(rise_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "\n",
    "            # Asymmetry: fall / rise (large means slow decay compared to rise)\n",
    "            tr50o = feats[f\"t_rise50_{b}_obs\"]\n",
    "            tf50o = feats[f\"t_fall50_{b}_obs\"]\n",
    "            tr50r = feats[f\"t_rise50_{b}_rest\"]\n",
    "            tf50r = feats[f\"t_fall50_{b}_rest\"]\n",
    "            feats[f\"asym50_{b}_obs\"] = float(tf50o / (tr50o + EPS)) if np.isfinite(tf50o) and np.isfinite(tr50o) else np.nan\n",
    "            feats[f\"asym50_{b}_rest\"] = float(tf50r / (tr50r + EPS)) if np.isfinite(tf50r) and np.isfinite(tr50r) else np.nan\n",
    "\n",
    "            # Area under the curve above the pre-peak baseline (positive only)\n",
    "            feats[f\"auc_pos_{b}_obs\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_obs))\n",
    "            feats[f\"auc_pos_{b}_rest\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_rest))\n",
    "\n",
    "            # Width at a given fractional level (simple “time above threshold” proxy)\n",
    "            def width_at_level(tt, ff, base, amp, frac):\n",
    "                if amp <= 0 or len(ff) < 3:\n",
    "                    return np.nan\n",
    "                level = base + frac * amp\n",
    "                above = ff >= level\n",
    "                if not np.any(above):\n",
    "                    return np.nan\n",
    "                idx = np.where(above)[0]\n",
    "                return float(tt[idx[-1]] - tt[idx[0]])\n",
    "\n",
    "            w50_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.80)\n",
    "            w50_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.80)\n",
    "\n",
    "            feats[f\"width50_{b}_obs\"] = w50_obs\n",
    "            feats[f\"width80_{b}_obs\"] = w80_obs\n",
    "            feats[f\"width50_{b}_rest\"] = w50_rest\n",
    "            feats[f\"width80_{b}_rest\"] = w80_rest\n",
    "\n",
    "            # Sharpness: high amplitude + short width means a “spiky” transient\n",
    "            feats[f\"sharp50_{b}_obs\"] = float(amp_pre / (w50_obs + EPS)) if np.isfinite(w50_obs) else np.nan\n",
    "            feats[f\"sharp50_{b}_rest\"] = float(amp_pre / (w50_rest + EPS)) if np.isfinite(w50_rest) else np.nan\n",
    "\n",
    "            # Fit a simple power-law decay model post-peak (captures decay steepness)\n",
    "            b_obs, r2_obs, npts_obs = decay_powerlaw_fit(tb_obs, fb, base_pre, pidx, tmax=300.0)\n",
    "            b_rest, r2_rest, npts_rest = decay_powerlaw_fit(tb_rest, fb, base_pre, pidx, tmax=300.0)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_obs\"] = b_obs\n",
    "            feats[f\"decay_pl_r2_{b}_obs\"] = r2_obs\n",
    "            feats[f\"decay_pl_npts_{b}_obs\"] = float(npts_obs)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_rest\"] = b_rest\n",
    "            feats[f\"decay_pl_r2_{b}_rest\"] = r2_rest\n",
    "            feats[f\"decay_pl_npts_{b}_rest\"] = float(npts_rest)\n",
    "\n",
    "        # Store values for cross-band comparisons and wavelength-trend features\n",
    "        band_tpeak_obs[b] = tpeak_obs\n",
    "        band_tpeak_rest[b] = tpeak_rest\n",
    "        band_peak_flux[b] = peak_flux\n",
    "\n",
    "    # Peak-time dispersion across filters (how synchronized the bands are)\n",
    "    tpeaks_obs = np.array([band_tpeak_obs.get(b, np.nan) for b in FILTERS], float)\n",
    "    tpeaks_rest = np.array([band_tpeak_rest.get(b, np.nan) for b in FILTERS], float)\n",
    "    tpeaks_obs = np.array([x for x in tpeaks_obs if np.isfinite(x)], float)\n",
    "    tpeaks_rest = np.array([x for x in tpeaks_rest if np.isfinite(x)], float)\n",
    "    feats[\"tpeak_std_obs\"] = float(np.std(tpeaks_obs)) if len(tpeaks_obs) >= 2 else np.nan\n",
    "    feats[\"tpeak_std_rest\"] = float(np.std(tpeaks_rest)) if len(tpeaks_rest) >= 2 else np.nan\n",
    "\n",
    "    # Cross-band peak-time lags and peak ratios for adjacent filters\n",
    "    pairs = [(\"u\", \"g\"), (\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\"), (\"z\", \"y\")]\n",
    "    for a, b in pairs:\n",
    "        ta_obs = band_tpeak_obs.get(a, np.nan)\n",
    "        tb_obs2 = band_tpeak_obs.get(b, np.nan)\n",
    "        ta_rest = band_tpeak_rest.get(a, np.nan)\n",
    "        tb_rest2 = band_tpeak_rest.get(b, np.nan)\n",
    "        pa = band_peak_flux.get(a, np.nan)\n",
    "        pb = band_peak_flux.get(b, np.nan)\n",
    "\n",
    "        feats[f\"tpeakdiff_{a}{b}_obs\"] = (ta_obs - tb_obs2) if (np.isfinite(ta_obs) and np.isfinite(tb_obs2)) else np.nan\n",
    "        feats[f\"tpeakdiff_{a}{b}_rest\"] = (ta_rest - tb_rest2) if (np.isfinite(ta_rest) and np.isfinite(tb_rest2)) else np.nan\n",
    "        feats[f\"peakratio_{a}{b}\"] = (pa / (pb + EPS)) if (np.isfinite(pa) and np.isfinite(pb)) else np.nan\n",
    "\n",
    "    # Helper for safe ratio features\n",
    "    def ratio_feature(name, num, den):\n",
    "        if np.isfinite(num) and np.isfinite(den):\n",
    "            feats[name] = float(num / (den + EPS))\n",
    "        else:\n",
    "            feats[name] = np.nan\n",
    "\n",
    "    # Cross-band ratios (adjacent filters)\n",
    "    # (helps capture “relative shape” instead of absolute scale)\n",
    "    for a, b in pairs:\n",
    "        ratio_feature(f\"amppreratio_{a}{b}\", feats.get(f\"amp_pre_{a}\", np.nan), feats.get(f\"amp_pre_{b}\", np.nan))\n",
    "        ratio_feature(f\"aucratio_{a}{b}_obs\", feats.get(f\"auc_pos_{a}_obs\", np.nan), feats.get(f\"auc_pos_{b}_obs\", np.nan))\n",
    "        ratio_feature(f\"width50ratio_{a}{b}_obs\", feats.get(f\"width50_{a}_obs\", np.nan), feats.get(f\"width50_{b}_obs\", np.nan))\n",
    "        ratio_feature(f\"asym50ratio_{a}{b}_obs\", feats.get(f\"asym50_{a}_obs\", np.nan), feats.get(f\"asym50_{b}_obs\", np.nan))\n",
    "\n",
    "    # Band-to-band correlations (captures whether bands rise/fall together)\n",
    "    for a, b in [(\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\")]:\n",
    "        if (a in band_tb_obs) and (b in band_tb_obs):\n",
    "            feats[f\"corr_{a}{b}_obs\"] = band_corr(\n",
    "                band_tb_obs[a], band_fb[a],\n",
    "                band_tb_obs[b], band_fb[b]\n",
    "            )\n",
    "        else:\n",
    "            feats[f\"corr_{a}{b}_obs\"] = np.nan\n",
    "\n",
    "    # Wavelength-trend features (peak time vs wavelength, peak flux vs wavelength)\n",
    "    # NOTE: these helpers likely internally use FILTER effective wavelengths, so they can regress vs lambda\n",
    "    slope_t, intercept_t, r2_t = peak_vs_wavelength_slope(band_tpeak_obs, band_tpeak_obs, z=z)\n",
    "    feats[\"tpeak_vs_lambda_slope_obs\"] = slope_t\n",
    "    feats[\"tpeak_vs_lambda_intercept_obs\"] = intercept_t\n",
    "    feats[\"tpeak_vs_lambda_r2_obs\"] = r2_t\n",
    "\n",
    "    slope_pf, intercept_pf, r2_pf = peak_vs_wavelength_slope(band_tpeak_obs, band_peak_flux, z=z)\n",
    "    feats[\"peakflux_vs_lambda_slope\"] = slope_pf\n",
    "    feats[\"peakflux_vs_lambda_intercept\"] = intercept_pf\n",
    "    feats[\"peakflux_vs_lambda_r2\"] = r2_pf\n",
    "\n",
    "    # Color features anchored at r-band peak time\n",
    "    tpr_obs = feats.get(\"tpeak_r_obs\", np.nan)\n",
    "    if np.isfinite(tpr_obs):\n",
    "        def colors_at_time(t0):\n",
    "            fr = interp_flux_at_time(band_tb_obs.get(\"r\", np.array([])), band_fb.get(\"r\", np.array([])), t0)\n",
    "            fg = interp_flux_at_time(band_tb_obs.get(\"g\", np.array([])), band_fb.get(\"g\", np.array([])), t0)\n",
    "            fi = interp_flux_at_time(band_tb_obs.get(\"i\", np.array([])), band_fb.get(\"i\", np.array([])), t0)\n",
    "\n",
    "            # signed_log1p allows negative flux without crashing log transforms\n",
    "            cgr = (signed_log1p(fg) - signed_log1p(fr)) if (np.isfinite(fg) and np.isfinite(fr)) else np.nan\n",
    "            cri = (signed_log1p(fr) - signed_log1p(fi)) if (np.isfinite(fr) and np.isfinite(fi)) else np.nan\n",
    "            return cgr, cri\n",
    "\n",
    "        # Colors at peak\n",
    "        cgr0, cri0 = colors_at_time(tpr_obs)\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = cgr0\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = cri0\n",
    "\n",
    "        # Colors at +20d and +40d to capture slower spectral evolution\n",
    "        cgr20, cri20 = colors_at_time(tpr_obs + 20.0)\n",
    "        cgr40, cri40 = colors_at_time(tpr_obs + 40.0)\n",
    "\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = cgr20\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = cri20\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = cgr40\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = cri40\n",
    "\n",
    "        # Simple finite-difference slopes (color change per day)\n",
    "        def slope(c1, c2, dt):\n",
    "            if np.isfinite(c1) and np.isfinite(c2):\n",
    "                return float((c2 - c1) / dt)\n",
    "            return np.nan\n",
    "\n",
    "        feats[\"color_gr_slope20_obs\"] = slope(cgr0, cgr20, 20.0)\n",
    "        feats[\"color_ri_slope20_obs\"] = slope(cri0, cri20, 20.0)\n",
    "        feats[\"color_gr_slope40_obs\"] = slope(cgr0, cgr40, 40.0)\n",
    "        feats[\"color_ri_slope40_obs\"] = slope(cri0, cri40, 40.0)\n",
    "    else:\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope20_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope20_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope40_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope40_obs\"] = np.nan\n",
    "\n",
    "    # SED slope features:\n",
    "    if np.isfinite(tpr_obs):\n",
    "        sed_slope, sed_int, sed_r2, sed_n = sed_logflux_vs_loglambda_at_time(\n",
    "            band_tb_obs, band_fb, band_eb, tpr_obs, z=z\n",
    "        )\n",
    "        feats[\"sed_logflux_loglambda_slope_rpeak\"] = sed_slope\n",
    "        feats[\"sed_logflux_loglambda_r2_rpeak\"] = sed_r2\n",
    "        feats[\"sed_logflux_loglambda_nbands_rpeak\"] = sed_n\n",
    "\n",
    "        sed_slope20, sed_int20, sed_r2_20, sed_n20 = sed_logflux_vs_loglambda_at_time(\n",
    "            band_tb_obs, band_fb, band_eb, tpr_obs + 20.0, z=z\n",
    "        )\n",
    "        feats[\"sed_slope_rpeak_p20\"] = sed_slope20\n",
    "        feats[\"sed_r2_rpeak_p20\"] = sed_r2_20\n",
    "        feats[\"sed_nbands_rpeak_p20\"] = sed_n20\n",
    "    else:\n",
    "        feats[\"sed_logflux_loglambda_slope_rpeak\"] = np.nan\n",
    "        feats[\"sed_logflux_loglambda_r2_rpeak\"] = np.nan\n",
    "        feats[\"sed_logflux_loglambda_nbands_rpeak\"] = np.nan\n",
    "        feats[\"sed_slope_rpeak_p20\"] = np.nan\n",
    "        feats[\"sed_r2_rpeak_p20\"] = np.nan\n",
    "        feats[\"sed_nbands_rpeak_p20\"] = np.nan\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def build_lightcurve_cache(splits, base_dir, kind=\"train\"):\n",
    "    base_dir = Path(base_dir)\n",
    "    lc_cache = {}\n",
    "    idx_cache = {}\n",
    "    for s in splits:\n",
    "        path = base_dir / str(s) / f\"{kind}_full_lightcurves.csv\"\n",
    "        lc = pd.read_csv(path)\n",
    "        lc[\"object_id\"] = lc[\"object_id\"].astype(str)\n",
    "        groups = lc.groupby(\"object_id\").indices\n",
    "        lc_cache[s] = lc\n",
    "        idx_cache[s] = groups\n",
    "    return lc_cache, idx_cache\n",
    "\n",
    "\n",
    "def get_lightcurve(lc_cache, idx_cache, split, object_id):\n",
    "    object_id = str(object_id)\n",
    "    idx = idx_cache[split].get(object_id, None)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    return lc_cache[split].iloc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd79af",
   "metadata": {},
   "source": [
    "## Feature Table Construction + Photo-z Augmentation\n",
    "\n",
    "Build a row per object by pulling its lightcurve from the cached split CSV.\n",
    "\n",
    "Augmentation:\n",
    "- For each training object, create `N_AUG` additional rows\n",
    "- Sample `sigma` from test `Z_err` pool\n",
    "- Set `z_sim = z0 + Normal(0, sigma)`\n",
    "- Mark augmented rows with `photoz_aug = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_table(\n",
    "    log_df,\n",
    "    lc_cache,\n",
    "    idx_cache,\n",
    "    augment_photoz=False,\n",
    "    test_zerr_pool=None,\n",
    "    n_aug=2,\n",
    "    seed=6\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    if test_zerr_pool is not None:\n",
    "        test_zerr_pool = np.asarray(test_zerr_pool, float)\n",
    "        test_zerr_pool = test_zerr_pool[np.isfinite(test_zerr_pool)]\n",
    "        test_zerr_pool = test_zerr_pool[test_zerr_pool > 0]\n",
    "\n",
    "    for i in range(len(log_df)):\n",
    "        r = log_df.iloc[i]\n",
    "        obj = str(r[\"object_id\"])\n",
    "        split = r[\"split\"]\n",
    "\n",
    "        lc = get_lightcurve(lc_cache, idx_cache, split, obj)\n",
    "        if lc is None:\n",
    "            feats = {\"n_obs\": 0}\n",
    "            feats[\"object_id\"] = obj\n",
    "            feats[\"split\"] = split\n",
    "            feats[\"photoz_aug\"] = 0\n",
    "            if \"target\" in log_df.columns:\n",
    "                feats[\"target\"] = int(r[\"target\"])\n",
    "            rows.append(feats)\n",
    "            continue\n",
    "\n",
    "        feats = extract_features_for_object(\n",
    "            lc_raw=lc,\n",
    "            z=r[\"Z\"],\n",
    "            z_err=r.get(\"Z_err\", 0.0),\n",
    "            ebv=r[\"EBV\"],\n",
    "        )\n",
    "        feats[\"object_id\"] = obj\n",
    "        feats[\"split\"] = split\n",
    "        feats[\"photoz_aug\"] = 0\n",
    "        if \"target\" in log_df.columns:\n",
    "            feats[\"target\"] = int(r[\"target\"])\n",
    "        rows.append(feats)\n",
    "\n",
    "        if augment_photoz and (\"target\" in log_df.columns) and (test_zerr_pool is not None) and (len(test_zerr_pool) > 0):\n",
    "            z0 = safe_float(r[\"Z\"], default=0.0)\n",
    "            for _ in range(n_aug):\n",
    "                sigma = float(rng.choice(test_zerr_pool))\n",
    "                z_sim = max(0.0, z0 + float(rng.normal(0.0, sigma)))\n",
    "                feats2 = extract_features_for_object(\n",
    "                    lc_raw=lc,\n",
    "                    z=z_sim,\n",
    "                    z_err=sigma,\n",
    "                    ebv=r[\"EBV\"],\n",
    "                )\n",
    "                feats2[\"object_id\"] = obj\n",
    "                feats2[\"split\"] = split\n",
    "                feats2[\"target\"] = int(r[\"target\"])\n",
    "                feats2[\"photoz_aug\"] = 1\n",
    "                rows.append(feats2)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc011e",
   "metadata": {},
   "source": [
    "## Feature Cleaning (with Missing Flags)\n",
    "\n",
    "Unlike earlier models that median-filled immediately, Model 5 keeps NaNs and adds explicit missing indicators.\n",
    "\n",
    "`clean_features(..., add_missing_flags=True)`:\n",
    "- replaces ±inf with NaN\n",
    "- appends `feature_isnan` columns\n",
    "\n",
    "This helps tree models learn patterns like:\n",
    "- \"missing band features\"\n",
    "- \"failed Bazin fit\"\n",
    "- \"no valid SED bands at peak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11049916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(df, drop_cols, add_missing_flags=True):\n",
    "    X = df.drop(columns=drop_cols).copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if add_missing_flags:\n",
    "        miss = X.isna().astype(np.uint8)\n",
    "        miss.columns = [c + \"_isnan\" for c in miss.columns]\n",
    "        X = pd.concat([X, miss], axis=1)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27129862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold_f1(y_true, probs):\n",
    "    ths = np.linspace(0.01, 0.99, 401)\n",
    "    f1s = [f1_score(y_true, probs > t, zero_division=0) for t in ths]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return float(ths[j]), float(f1s[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36a6e3",
   "metadata": {},
   "source": [
    "## SpecType Teacher Features (Expanded Classes)\n",
    "\n",
    "Model 5 repeats the legal teacher stacking approach, but expands:\n",
    "- Adds SLSN and SNII\n",
    "- Adds `spec_topprob` (max class probability) as a confidence feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10, seed=6):\n",
    "\n",
    "    # Join spectroscopic labels onto the feature table\n",
    "    df = train_feat.merge(train_log[[\"object_id\", \"SpecType\"]], on=\"object_id\", how=\"left\")\n",
    "    spec = df[\"SpecType\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "    # Map detailed SpecType strings into a richer set of coarse groups\n",
    "    # (separates SLSN and SNII explicitly vs earlier models)\n",
    "    def map_group(s):\n",
    "        s2 = s.strip()\n",
    "        if s2 == \"TDE\":\n",
    "            return \"TDE\"\n",
    "        if s2 == \"AGN\":\n",
    "            return \"AGN\"\n",
    "        if \"SLSN\" in s2:\n",
    "            return \"SLSN\"\n",
    "        if s2 == \"SN Ia\" or s2.startswith(\"SN Ia\"):\n",
    "            return \"SNIa\"\n",
    "        if s2.startswith(\"SN II\") or (\"SN II\" in s2):\n",
    "            return \"SNII\"\n",
    "        if s2.startswith(\"SN\"):\n",
    "            return \"SNother\"\n",
    "        return \"Other\"\n",
    "\n",
    "    spec_group = spec.map(map_group).astype(str)\n",
    "\n",
    "    # Encode group labels into integers for multiclass training\n",
    "    classes = sorted(spec_group.unique())\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_mc = spec_group.map(class_to_idx).to_numpy()\n",
    "\n",
    "    # Build train/test matrices using only shared columns\n",
    "    # add_missing_flags=True exposes “isnan” indicators as extra features\n",
    "    X_tr = clean_features(df, drop_cols=[\"object_id\", \"split\", \"target\", \"SpecType\"], add_missing_flags=True)\n",
    "    X_te = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"], add_missing_flags=True)\n",
    "\n",
    "    # Use split folders as groups to prevent leakage across simulated splits\n",
    "    groups = df[\"split\"].to_numpy()\n",
    "\n",
    "    # Splitter\n",
    "    splitter = StratifiedGroupKFold(n_splits, shuffle=True, random_state=seed)\n",
    "    split_iter = splitter.split(X_tr, y_mc, groups)\n",
    "\n",
    "    # Out-of-fold predicted probabilities for the teacher\n",
    "    oof = np.zeros((len(X_tr), len(classes)), dtype=float)\n",
    "\n",
    "    # LightGBM multiclass teacher configuration\n",
    "    base = dict(\n",
    "        objective=\"multiclass\",\n",
    "        num_class=len(classes),\n",
    "        metric=\"multi_logloss\",\n",
    "        n_estimators=20000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=63,\n",
    "        min_child_samples=5,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        verbosity=-1,\n",
    "        force_col_wise=True\n",
    "    )\n",
    "\n",
    "    # Train teacher in CV and collect OOF probabilities (legal stacking)\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        model = LGBMClassifier(**base)\n",
    "        model.fit(\n",
    "            X_tr.iloc[tr_idx], y_mc[tr_idx],\n",
    "            eval_set=[(X_tr.iloc[va_idx], y_mc[va_idx])],\n",
    "            eval_metric=\"multi_logloss\",\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "        )\n",
    "\n",
    "        # Predict probabilities on the validation fold using the best iteration\n",
    "        oof[va_idx] = model.predict_proba(\n",
    "            X_tr.iloc[va_idx],\n",
    "            num_iteration=model.best_iteration_\n",
    "        )\n",
    "\n",
    "    # Fit on full training data and predict teacher probabilities for test\n",
    "    model_full = LGBMClassifier(**base)\n",
    "    model_full.fit(X_tr, y_mc)\n",
    "    p_test = model_full.predict_proba(X_te)\n",
    "\n",
    "    # Entropy summary: high entropy means the teacher is uncertain\n",
    "    def entropy(p):\n",
    "        p = np.clip(p, 1e-12, 1.0)\n",
    "        return -np.sum(p * np.log(p), axis=1)\n",
    "\n",
    "    # Append per-class probabilities as new features\n",
    "    for i, c in enumerate(classes):\n",
    "        train_feat[f\"p_spec_{c}\"] = oof[:, i]\n",
    "        test_feat[f\"p_spec_{c}\"] = p_test[:, i]\n",
    "\n",
    "    # Append teacher uncertainty + top-probability confidence\n",
    "    train_feat[\"spec_entropy\"] = entropy(oof)\n",
    "    test_feat[\"spec_entropy\"] = entropy(p_test)\n",
    "\n",
    "    train_feat[\"spec_topprob\"] = np.max(oof, axis=1)   # teacher confidence (train OOF)\n",
    "    test_feat[\"spec_topprob\"] = np.max(p_test, axis=1) # teacher confidence (test)\n",
    "\n",
    "    return train_feat, test_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa79b9",
   "metadata": {},
   "source": [
    "## Feature Selection (Top-K)\n",
    "\n",
    "Before Optuna, Model 5 performs feature selection:\n",
    "\n",
    "1) Train a reasonably strong XGB baseline per fold\n",
    "2) Extract feature importance via `importance_type=\"gain\"`\n",
    "3) Sum gains across folds\n",
    "4) Keep Top-K features (`FS_TOPK`)\n",
    "\n",
    "This reduces search space and helps Optuna focus on the best subset. I don't have the computational power in order to test performance without feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_gain_topk(train_feat, k=350, n_splits=10, seed=6):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "    X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "\n",
    "    splitter = StratifiedGroupKFold(n_splits, shuffle=True, random_state=seed)\n",
    "    split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "    gains = {c: 0.0 for c in X.columns}\n",
    "\n",
    "    base_params = dict(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"aucpr\",\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        n_estimators=6000,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=6,\n",
    "        min_child_weight=10,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=2.0,\n",
    "        reg_lambda=2.0,\n",
    "        gamma=0.0,\n",
    "        max_bin=256,\n",
    "    )\n",
    "\n",
    "    for tr_idx, va_idx in split_iter:\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        spw = float(neg / max(1, pos))\n",
    "\n",
    "        model = XGBClassifier(**{**base_params, \"scale_pos_weight\": spw})\n",
    "        model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "        score = model.get_booster().get_score(importance_type=\"gain\")\n",
    "        for feat, g in score.items():\n",
    "            if feat in gains:\n",
    "                gains[feat] += float(g)\n",
    "\n",
    "    ranked = sorted(gains.items(), key=lambda x: x[1], reverse=True)\n",
    "    top = [f for f, _ in ranked[:k]]\n",
    "    return top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd739c",
   "metadata": {},
   "source": [
    "## Optuna XGBoost Hyperparameter Tuning\n",
    "\n",
    "This function runs Optuna using:\n",
    "- Out-of-fold (OOF) F1 score as the optimization target\n",
    "- StratifiedGroupKFold to prevent split/group leakage\n",
    "- Selected feature subset only\n",
    "- Early stopping + Optuna pruning\n",
    "\n",
    "Key behavior:\n",
    "- Cleans features and adds missing-value indicator flags\n",
    "- Uses split labels as groups to keep objects from the same split together\n",
    "- Computes `scale_pos_weight` per fold for class imbalance\n",
    "- Tunes tree structure, sampling, regularization, and binning parameters\n",
    "- Stores results in a persistent Optuna SQLite study\n",
    "- Returns the best parameter set found within the time limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20251e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_xgb_f1(train_feat, feature_cols, n_folds_tune=10, timeout_sec=28800, seed=6):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X_all = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "    X = X_all[feature_cols].copy()\n",
    "\n",
    "    splitter = StratifiedGroupKFold(n_folds_tune, shuffle=True, random_state=seed)\n",
    "    split_iter_all = list(splitter.split(X, y, groups))\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"aucpr\",\n",
    "            \"random_state\": seed,\n",
    "            \"n_jobs\": -1,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1500, 14000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.002, 0.08, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 80),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "            \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "            \"max_bin\": trial.suggest_int(\"max_bin\", 128, 512),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 12.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 35.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.05, 50.0),\n",
    "            \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "            \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "        }\n",
    "\n",
    "        if params[\"grow_policy\"] == \"lossguide\":\n",
    "            params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 512)\n",
    "\n",
    "        oof = np.zeros(len(X), dtype=float)\n",
    "        f1_progress = []\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(split_iter_all, 1):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "            neg = np.sum(y_tr == 0)\n",
    "            pos = np.sum(y_tr == 1)\n",
    "            spw = float(neg / max(1, pos))\n",
    "\n",
    "            model = XGBClassifier(**{**params, \"scale_pos_weight\": spw})\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_va, y_va)],\n",
    "                verbose=False,\n",
    "                callbacks=[xgb.callback.EarlyStopping(rounds=200, save_best=True)]\n",
    "            )\n",
    "\n",
    "            oof[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "            th_fold, f1_fold = best_threshold_f1(y_va, oof[va_idx])\n",
    "            f1_progress.append(f1_fold)\n",
    "\n",
    "            trial.report(float(np.mean(f1_progress)), step=fold)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        th, f1 = best_threshold_f1(y, oof)\n",
    "        return float(f1)\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=seed, multivariate=True, group=True)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=40, n_warmup_steps=3)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name=\"xgb_oof_f1_splitcv_gpu_selected\",\n",
    "        storage=\"sqlite:///optuna_xgb_oof_f1_gpu_selected.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=999999, timeout=timeout_sec)\n",
    "\n",
    "    print(\"\\nOptuna best OOF F1:\", study.best_value)\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(k, \"=\", v)\n",
    "\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad3562",
   "metadata": {},
   "source": [
    "## Multiseed XGBoost OOF Ensemble\n",
    "\n",
    "This function trains a multiseed XGBoost ensemble using split-aware cross validation\n",
    "\n",
    "- Cleans features and adds missing flags\n",
    "- Uses selected feature subset only\n",
    "- StratifiedGroupKFold prevents group leakage\n",
    "- Multiple seeds per fold for prediction stability\n",
    "- Averages seed predictions for OOF and test (Should have tested without averaging seeds as some said it lowered performance)\n",
    "- Computes best OOF F1 threshold and AP\n",
    "- Trains final multiseed models on full data for test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_xgb_multiseed(train_feat, test_feat, best_params, feature_cols, n_splits_oof=20, seeds=(6, 67, 6767)):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X_all = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "    X_test_all = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"], add_missing_flags=True)\n",
    "\n",
    "    X = X_all[feature_cols].copy()\n",
    "    X_test = X_test_all[feature_cols].copy()\n",
    "\n",
    "    splitter = StratifiedGroupKFold(n_splits=n_splits_oof, shuffle=True, random_state=6)\n",
    "    split_iter = list(splitter.split(X, y, groups))\n",
    "\n",
    "    oof = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "        \n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        spw = float(neg / max(1, pos))\n",
    "\n",
    "        probs_va = []\n",
    "        for sd in seeds:\n",
    "            model = XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                eval_metric=\"logloss\",\n",
    "                random_state=sd,\n",
    "                n_jobs=-1,\n",
    "                tree_method=\"hist\",\n",
    "                device=\"cuda\",\n",
    "                scale_pos_weight=spw,\n",
    "                **best_params,\n",
    "            )\n",
    "            model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False,\n",
    "            )\n",
    "            probs_va.append(model.predict_proba(X_va)[:, 1])\n",
    "\n",
    "        oof[va_idx] = np.mean(probs_va, axis=0)\n",
    "\n",
    "    best_th, best_f1 = best_threshold_f1(y, oof)\n",
    "    ap = average_precision_score(y, oof)\n",
    "    print(\"\\nOOF multiseed best threshold:\", best_th)\n",
    "    print(\"OOF multiseed best F1:\", best_f1)\n",
    "    print(\"OOF AP (aucpr-ish):\", ap)\n",
    "\n",
    "    probs_test = []\n",
    "    neg_full = np.sum(y == 0)\n",
    "    pos_full = np.sum(y == 1)\n",
    "    spw_full = float(neg_full / max(1, pos_full))\n",
    "\n",
    "    for sd in seeds:\n",
    "        model = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"aucpr\",\n",
    "            random_state=sd,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\",\n",
    "            device=\"cuda\",\n",
    "            scale_pos_weight=spw_full,\n",
    "            **best_params,\n",
    "        )\n",
    "        model.fit(X, y, verbose=False)\n",
    "        probs_test.append(model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    p_test = np.mean(probs_test, axis=0)\n",
    "    return p_test, best_th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b70d8d",
   "metadata": {},
   "source": [
    "## Data Init and Feature Table Build\n",
    "\n",
    "Outputs train/test feature tables and prints their shapes (Not including SpecType features and before feature selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2aaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (9129, 559)\n",
      "test shape: (7135, 558)\n"
     ]
    }
   ],
   "source": [
    "N_AUG = 2\n",
    "FS_TOPK = 380\n",
    "FS_FOLDS = 10\n",
    "OPTUNA_FOLDS = 10\n",
    "OPTUNA_TIMEOUT_SEC = 28800\n",
    "FINAL_OOF_FOLDS = 20\n",
    "SEEDS = (6, 67, 6767)\n",
    "\n",
    "ROOT = Path.cwd().parents[0]\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "train_log = pd.read_csv(DATA_DIR / \"train_log.csv\")\n",
    "test_log  = pd.read_csv(DATA_DIR / \"test_log.csv\")\n",
    "\n",
    "train_log[\"object_id\"] = train_log[\"object_id\"].astype(str)\n",
    "test_log[\"object_id\"] = test_log[\"object_id\"].astype(str)\n",
    "\n",
    "train_log[\"Z_err\"] = train_log[\"Z_err\"].fillna(0.0)\n",
    "test_log[\"Z_err\"] = test_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "train_splits = sorted(train_log[\"split\"].unique())\n",
    "test_splits = sorted(test_log[\"split\"].unique())\n",
    "\n",
    "train_lc_cache, train_idx_cache = build_lightcurve_cache(train_splits, base_dir=DATA_DIR, kind=\"train\")\n",
    "test_lc_cache, test_idx_cache = build_lightcurve_cache(test_splits, base_dir=DATA_DIR, kind=\"test\")\n",
    "\n",
    "test_zerr_pool = test_log[\"Z_err\"].dropna().values\n",
    "\n",
    "train_feat = build_feature_table(\n",
    "    train_log, train_lc_cache, train_idx_cache,\n",
    "    augment_photoz=True,\n",
    "    test_zerr_pool=test_zerr_pool,\n",
    "    n_aug=N_AUG,\n",
    "    seed=6\n",
    ")\n",
    "\n",
    "test_feat = build_feature_table(\n",
    "    test_log, test_lc_cache, test_idx_cache,\n",
    "    augment_photoz=False\n",
    ")\n",
    "print(f\"train shape: {train_feat.shape}\")\n",
    "print(f\"test shape: {test_feat.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422509de",
   "metadata": {},
   "source": [
    "## SpecType Features and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1d75763",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat, test_feat = add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10, seed=6)\n",
    "selected_cols = feature_select_gain_topk(train_feat, k=FS_TOPK, n_splits=FS_FOLDS, seed=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = run_optuna_xgb_f1(\n",
    "    train_feat,\n",
    "    feature_cols=selected_cols,\n",
    "    n_folds_tune=OPTUNA_FOLDS,\n",
    "    timeout_sec=OPTUNA_TIMEOUT_SEC,\n",
    "    seed=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f16b9b",
   "metadata": {},
   "source": [
    "Best OOF F1: **0.6093264248704663**\n",
    "\n",
    "Best Parameters:\n",
    "```json\n",
    "{\n",
    "  \"n_estimators\": 9476,\n",
    "  \"learning_rate\": 0.0024306289953670325,\n",
    "  \"max_depth\": 7,\n",
    "  \"min_child_weight\": 6,\n",
    "  \"subsample\": 0.5344962939912224,\n",
    "  \"colsample_bytree\": 0.464696420753079,\n",
    "  \"colsample_bylevel\": 0.8146569410634974,\n",
    "  \"colsample_bynode\": 0.7285475291884695,\n",
    "  \"max_bin\": 181,\n",
    "  \"gamma\": 8.476938947246458,\n",
    "  \"reg_alpha\": 0.44957196104419117,\n",
    "  \"reg_lambda\": 5.23806334613521,\n",
    "  \"max_delta_step\": 0,\n",
    "  \"grow_policy\": \"depthwise\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efdfec",
   "metadata": {},
   "source": [
    "Reinitializing best_params because kernel reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee18984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 9476,\n",
    "               'learning_rate': 0.0024306289953670325,\n",
    "               'max_depth': 7, 'min_child_weight': 6,\n",
    "               'subsample': 0.5344962939912224,\n",
    "               'colsample_bytree': 0.464696420753079,\n",
    "               'colsample_bylevel': 0.8146569410634974,\n",
    "               'colsample_bynode': 0.7285475291884695,\n",
    "               'max_bin': 181, 'gamma': 8.476938947246458,\n",
    "               'reg_alpha': 0.44957196104419117,\n",
    "               'reg_lambda': 5.23806334613521,\n",
    "               'max_delta_step': 0,\n",
    "               'grow_policy': 'depthwise'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a738b14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OOF multiseed best threshold: 0.419\n",
      "OOF multiseed best F1: 0.6243705941591138\n",
      "OOF AP (aucpr-ish): 0.5164263302782434\n"
     ]
    }
   ],
   "source": [
    "p_test, best_th = predict_xgb_multiseed(\n",
    "    train_feat,\n",
    "    test_feat,\n",
    "    best_params,\n",
    "    feature_cols=selected_cols,\n",
    "    n_splits_oof=min(FINAL_OOF_FOLDS, len(train_splits)),\n",
    "    seeds=(99, 999, 909)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bad79a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGB_multiseed_teacher5.csv  threshold: 0.419\n"
     ]
    }
   ],
   "source": [
    "test_pred = (p_test > best_th).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"object_id\": test_feat[\"object_id\"].values,\n",
    "    \"target\": test_pred\n",
    "})\n",
    "out_name = \"XGB_multiseed_teacher5.csv\"\n",
    "sub.to_csv(out_name, index=False)\n",
    "print(\"Saved\", out_name, \" threshold:\", best_th)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
