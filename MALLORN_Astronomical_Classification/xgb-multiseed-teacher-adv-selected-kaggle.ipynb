{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475a8c1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:38.372952Z",
     "iopub.status.busy": "2026-01-25T03:03:38.372178Z",
     "iopub.status.idle": "2026-01-25T03:03:50.390977Z",
     "shell.execute_reply": "2026-01-25T03:03:50.390089Z"
    },
    "papermill": {
     "duration": 12.026379,
     "end_time": "2026-01-25T03:03:50.392836",
     "exception": false,
     "start_time": "2026-01-25T03:03:38.366457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m627.8/627.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "!pip -q install extinction\n",
    "from extinction import fitzpatrick99\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "from scipy.optimize import curve_fit\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "FILTERS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "EFF_WL_AA = {\n",
    "    \"u\": 3641.0,\n",
    "    \"g\": 4704.0,\n",
    "    \"r\": 6155.0,\n",
    "    \"i\": 7504.0,\n",
    "    \"z\": 8695.0,\n",
    "    \"y\": 10056.0,\n",
    "}\n",
    "\n",
    "R_V = 3.1\n",
    "PRE_BASE_FRAC = 0.20\n",
    "MIN_BAND_POINTS = 5\n",
    "PEAK_SIGMA_K = 3.0\n",
    "REBRIGHT_FRAC = 0.30\n",
    "EPS = 1e-8\n",
    "\n",
    "SEASON_GAP_DAYS = 90.0\n",
    "\n",
    "SF_LAGS = [5.0, 10.0, 20.0, 50.0, 100.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c953d9f",
   "metadata": {
    "papermill": {
     "duration": 0.003516,
     "end_time": "2026-01-25T03:03:50.400048",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.396532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Kaggle has XGBoost 3.1.0 which messes with quite a few of the hyperparameters I used like early stopping rounds and callbacks. I learned that the hard way after hours wasted on GPU's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020c170d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.408682Z",
     "iopub.status.busy": "2026-01-25T03:03:50.407533Z",
     "iopub.status.idle": "2026-01-25T03:03:50.412957Z",
     "shell.execute_reply": "2026-01-25T03:03:50.412199Z"
    },
    "papermill": {
     "duration": 0.011134,
     "end_time": "2026-01-25T03:03:50.414373",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.403239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xgb_fit_kaggle(model, X_tr, y_tr, X_va=None, y_va=None, verbose=False):\n",
    "    if X_va is not None and y_va is not None:\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=verbose)\n",
    "    else:\n",
    "        model.fit(X_tr, y_tr, verbose=verbose)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71e1119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.422964Z",
     "iopub.status.busy": "2026-01-25T03:03:50.422458Z",
     "iopub.status.idle": "2026-01-25T03:03:50.428110Z",
     "shell.execute_reply": "2026-01-25T03:03:50.427449Z"
    },
    "papermill": {
     "duration": 0.011633,
     "end_time": "2026-01-25T03:03:50.429423",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.417790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "\n",
    "def _filter_kwargs(func, kwargs: dict):\n",
    "    sig = inspect.signature(func)\n",
    "    return {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
    "\n",
    "def make_model_safe(ModelClass, **kwargs):\n",
    "    init_kwargs = _filter_kwargs(ModelClass.__init__, kwargs)\n",
    "    return ModelClass(**init_kwargs)\n",
    "\n",
    "def fit_safe(model, X, y, *, X_va=None, y_va=None, early_stopping_rounds=200, verbose=False):\n",
    "    fit_kwargs = {}\n",
    "\n",
    "    if X_va is not None and y_va is not None:\n",
    "        fit_kwargs[\"eval_set\"] = [(X_va, y_va)]\n",
    "\n",
    "    fit_kwargs[\"verbose\"] = verbose\n",
    "    fit_kwargs[\"early_stopping_rounds\"] = early_stopping_rounds\n",
    "\n",
    "    fit_kwargs = _filter_kwargs(model.fit, fit_kwargs)\n",
    "\n",
    "    model.fit(X, y, **fit_kwargs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbec236c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.438042Z",
     "iopub.status.busy": "2026-01-25T03:03:50.437631Z",
     "iopub.status.idle": "2026-01-25T03:03:50.472914Z",
     "shell.execute_reply": "2026-01-25T03:03:50.472230Z"
    },
    "papermill": {
     "duration": 0.041471,
     "end_time": "2026-01-25T03:03:50.474434",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.432963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return default\n",
    "        x = float(x)\n",
    "        if np.isnan(x):\n",
    "            return default\n",
    "        return x\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def trapz_safe(y, x):\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return float(np.trapezoid(y, x))\n",
    "    y = np.asarray(y)\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    return float(np.sum((x[1:] - x[:-1]) * (y[1:] + y[:-1]) * 0.5))\n",
    "\n",
    "\n",
    "def median_abs_dev(x):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    med = np.median(x)\n",
    "    return float(np.median(np.abs(x - med)))\n",
    "\n",
    "\n",
    "def iqr(x):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    q75, q25 = np.percentile(x, [75, 25])\n",
    "    return float(q75 - q25)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m3 = np.mean((x - mu) ** 3)\n",
    "    return float(m3 / (s ** 3))\n",
    "\n",
    "\n",
    "def kurtosis_excess(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m4 = np.mean((x - mu) ** 4)\n",
    "    return float(m4 / (s ** 4) - 3.0)\n",
    "\n",
    "\n",
    "def von_neumann_eta(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    v = np.var(x)\n",
    "    if v < 1e-12:\n",
    "        return 0.0\n",
    "    dif = np.diff(x)\n",
    "    return float(np.mean(dif ** 2) / v)\n",
    "\n",
    "\n",
    "def max_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.max(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def median_abs_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.median(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def linear_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    try:\n",
    "        a, b = np.polyfit(t, f, 1)\n",
    "        return float(a)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def chi2_to_constant(f, ferr):\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.median(f)\n",
    "    denom = (ferr + EPS) ** 2\n",
    "    chi2 = np.sum((f - mu) ** 2 / denom)\n",
    "    dof = max(1, n - 1)\n",
    "    return float(chi2 / dof)\n",
    "\n",
    "\n",
    "def interp_flux_at_time(tb, fb, t0):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan\n",
    "    if (t0 < tb.min()) or (t0 > tb.max()):\n",
    "        return np.nan\n",
    "    return float(np.interp(t0, tb, fb))\n",
    "\n",
    "\n",
    "def interp_err_at_time(tb, eb, t0):\n",
    "    tb = np.asarray(tb)\n",
    "    eb = np.asarray(eb)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan\n",
    "    if (t0 < tb.min()) or (t0 > tb.max()):\n",
    "        return np.nan\n",
    "    return float(np.interp(t0, tb, eb))\n",
    "\n",
    "\n",
    "def fractional_variability(f, ferr):\n",
    "    f = np.asarray(f, float)\n",
    "    ferr = np.asarray(ferr, float)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.mean(f)\n",
    "    if np.abs(mu) < 1e-8:\n",
    "        return np.nan\n",
    "    s2 = np.var(f, ddof=1)\n",
    "    mean_err2 = np.mean(ferr ** 2)\n",
    "    excess = max(0.0, s2 - mean_err2)\n",
    "    return float(np.sqrt(excess) / np.abs(mu))\n",
    "\n",
    "\n",
    "def stetson_J_consecutive(t, f, ferr):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(t)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "    mu = np.mean(f)\n",
    "    scale = np.sqrt(n / max(1, n - 1))\n",
    "    delta = scale * (f - mu) / (ferr + EPS)\n",
    "    vals = []\n",
    "    for i in range(n - 1):\n",
    "        P = delta[i] * delta[i + 1]\n",
    "        vals.append(np.sign(P) * np.sqrt(np.abs(P)))\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "\n",
    "def pre_peak_baseline(tb, fb, eb, frac=PRE_BASE_FRAC):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(tb)\n",
    "    if n < 3:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    k = max(2, int(np.ceil(frac * n)))\n",
    "    k = min(k, n)\n",
    "    base = float(np.median(fb[:k]))\n",
    "    mad_pre = median_abs_dev(fb[:k])\n",
    "    mederr_pre = float(np.median(eb[:k])) if k > 0 else np.nan\n",
    "    return base, mad_pre, mederr_pre\n",
    "\n",
    "\n",
    "def count_significant_peaks(tb, fb, eb, baseline_pre, k_sigma=PEAK_SIGMA_K):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(fb)\n",
    "    if n < 5:\n",
    "        return 0\n",
    "    mederr = float(np.median(eb)) if np.isfinite(np.median(eb)) else 0.0\n",
    "    thresh = baseline_pre + k_sigma * mederr\n",
    "    peaks = 0\n",
    "    for i in range(1, n - 1):\n",
    "        if (fb[i] > fb[i - 1]) and (fb[i] > fb[i + 1]) and (fb[i] > thresh):\n",
    "            peaks += 1\n",
    "    return int(peaks)\n",
    "\n",
    "\n",
    "def postpeak_monotonicity(tb, fb, pidx):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return np.nan\n",
    "    t2 = tb[pidx:]\n",
    "    f2 = fb[pidx:]\n",
    "    if len(f2) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t2)\n",
    "    df = np.diff(f2)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    frac_neg = float(np.mean((df[good] / dt[good]) < 0))\n",
    "    return frac_neg\n",
    "\n",
    "\n",
    "def count_rebrighten(tb, fb, baseline_pre, amp, pidx, frac=REBRIGHT_FRAC):\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return 0\n",
    "    level = baseline_pre + frac * amp\n",
    "    post = fb[pidx:]\n",
    "    if len(post) < 3:\n",
    "        return 0\n",
    "    above = post > level\n",
    "    crossings = np.sum((~above[:-1]) & (above[1:]))\n",
    "    return int(crossings)\n",
    "\n",
    "\n",
    "def fall_time_to_level(tb, fb, baseline_pre, amp, pidx, frac):\n",
    "    if amp <= 0 or pidx is None:\n",
    "        return np.nan\n",
    "    level = baseline_pre + frac * amp\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "    if len(f_dec) < 2:\n",
    "        return np.nan\n",
    "    idx = np.where(f_dec <= level)[0]\n",
    "    if len(idx) == 0:\n",
    "        return np.nan\n",
    "    return float(t_dec[idx[0]] - t_dec[0])\n",
    "\n",
    "\n",
    "def rise_time_to_level(tb, fb, baseline_pre, amp, pidx, frac):\n",
    "    if amp <= 0 or pidx is None or pidx < 2:\n",
    "        return np.nan\n",
    "    level = baseline_pre + frac * amp\n",
    "    t_pre = tb[:pidx + 1]\n",
    "    f_pre = fb[:pidx + 1]\n",
    "    idx = np.where(f_pre >= level)[0]\n",
    "    if len(idx) == 0:\n",
    "        return np.nan\n",
    "    return float(t_pre[-1] - t_pre[idx[0]])\n",
    "\n",
    "\n",
    "def decay_powerlaw_fit(tb, fb, baseline_pre, pidx, tmax=300.0):\n",
    "    if pidx is None or pidx >= len(fb) - 3:\n",
    "        return np.nan, np.nan, 0\n",
    "    t0 = tb[pidx]\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "    dt = t_dec - t0\n",
    "    m = (dt > 0.0) & (dt <= tmax)\n",
    "    dt = dt[m]\n",
    "    fd = f_dec[m] - baseline_pre\n",
    "    m2 = fd > 0.0\n",
    "    dt = dt[m2]\n",
    "    fd = fd[m2]\n",
    "    if len(dt) < 4:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "    x = np.log(dt + EPS)\n",
    "    y = np.log(fd + EPS)\n",
    "    try:\n",
    "        b, a = np.polyfit(x, y, 1)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "    yhat = a + b * x\n",
    "    ss_res = float(np.sum((y - yhat) ** 2))\n",
    "    ss_tot = float(np.sum((y - np.mean(y)) ** 2)) + EPS\n",
    "    r2 = 1.0 - ss_res / ss_tot\n",
    "    return float(b), float(r2), int(len(dt))\n",
    "\n",
    "\n",
    "def signed_log1p(x):\n",
    "    x = float(x)\n",
    "    return float(np.sign(x) * np.log1p(np.abs(x)))\n",
    "\n",
    "\n",
    "def deextinct_band(flux, flux_err, ebv, band, r_v=R_V):\n",
    "    if ebv is None or (isinstance(ebv, float) and np.isnan(ebv)):\n",
    "        return flux, flux_err\n",
    "    A_V = float(ebv) * float(r_v)\n",
    "    wave = np.array([EFF_WL_AA[band]], dtype=float)\n",
    "    A_lambda = float(fitzpatrick99(wave, A_V, r_v=r_v, unit=\"aa\")[0])\n",
    "    fac = 10.0 ** (0.4 * A_lambda)\n",
    "    return flux * fac, flux_err * fac\n",
    "\n",
    "\n",
    "def deextinct_lightcurve(lc, ebv):\n",
    "    flux = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    ferr = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "    flux_corr = flux.copy()\n",
    "    ferr_corr = ferr.copy()\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        flux_corr[m], ferr_corr[m] = deextinct_band(flux_corr[m], ferr_corr[m], ebv, b)\n",
    "    return flux_corr, ferr_corr\n",
    "\n",
    "\n",
    "def band_corr(tt_a, ff_a, tt_b, ff_b, n_grid=30):\n",
    "    tt_a = np.asarray(tt_a, float)\n",
    "    ff_a = np.asarray(ff_a, float)\n",
    "    tt_b = np.asarray(tt_b, float)\n",
    "    ff_b = np.asarray(ff_b, float)\n",
    "\n",
    "    if len(tt_a) < 3 or len(tt_b) < 3:\n",
    "        return np.nan\n",
    "\n",
    "    tmin = max(tt_a.min(), tt_b.min())\n",
    "    tmax = min(tt_a.max(), tt_b.max())\n",
    "    if (tmax - tmin) < 5.0:\n",
    "        return np.nan\n",
    "\n",
    "    grid = np.linspace(tmin, tmax, n_grid)\n",
    "    fa = np.interp(grid, tt_a, ff_a)\n",
    "    fb = np.interp(grid, tt_b, ff_b)\n",
    "\n",
    "    sa = np.std(fa)\n",
    "    sb = np.std(fb)\n",
    "    if sa < 1e-12 or sb < 1e-12:\n",
    "        return 0.0\n",
    "    return float(np.corrcoef(fa, fb)[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674fcb16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.483213Z",
     "iopub.status.busy": "2026-01-25T03:03:50.482651Z",
     "iopub.status.idle": "2026-01-25T03:03:50.498515Z",
     "shell.execute_reply": "2026-01-25T03:03:50.497855Z"
    },
    "papermill": {
     "duration": 0.022146,
     "end_time": "2026-01-25T03:03:50.500241",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.478095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seasonality_features(tb):\n",
    "    tb = np.asarray(tb, float)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    dt = np.diff(tb)\n",
    "    breaks = np.where(dt > SEASON_GAP_DAYS)[0]\n",
    "    seg_starts = [0] + (breaks + 1).tolist()\n",
    "    seg_ends = breaks.tolist() + [len(tb) - 1]\n",
    "    spans = []\n",
    "    for s, e in zip(seg_starts, seg_ends):\n",
    "        spans.append(tb[e] - tb[s])\n",
    "    spans = np.asarray(spans, float)\n",
    "    n_seasons = float(len(spans))\n",
    "    return n_seasons, float(np.max(spans)), float(np.mean(spans))\n",
    "\n",
    "\n",
    "def structure_function_lags(tb, fb, lags=SF_LAGS):\n",
    "    tb = np.asarray(tb, float)\n",
    "    fb = np.asarray(fb, float)\n",
    "    n = len(tb)\n",
    "    out = {}\n",
    "    if n < 6:\n",
    "        for lag in lags:\n",
    "            out[f\"sf_medabs_{int(lag)}\"] = np.nan\n",
    "            out[f\"sf_n_{int(lag)}\"] = 0.0\n",
    "        return out\n",
    "\n",
    "    for lag in lags:\n",
    "        tol = max(2.0, 0.2 * lag)\n",
    "        vals = []\n",
    "        for i in range(n - 1):\n",
    "            dt = tb[i + 1:] - tb[i]\n",
    "            m = (dt >= (lag - tol)) & (dt <= (lag + tol))\n",
    "            if np.any(m):\n",
    "                dif = np.abs(fb[i + 1:][m] - fb[i])\n",
    "                vals.extend(dif.tolist())\n",
    "        if len(vals) == 0:\n",
    "            out[f\"sf_medabs_{int(lag)}\"] = np.nan\n",
    "            out[f\"sf_n_{int(lag)}\"] = 0.0\n",
    "        else:\n",
    "            out[f\"sf_medabs_{int(lag)}\"] = float(np.median(vals))\n",
    "            out[f\"sf_n_{int(lag)}\"] = float(len(vals))\n",
    "    return out\n",
    "\n",
    "\n",
    "def peak_vs_wavelength_slope(tpeak_by_band, val_by_band, z=0.0):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in FILTERS:\n",
    "        v = val_by_band.get(b, np.nan)\n",
    "        t = tpeak_by_band.get(b, np.nan)\n",
    "        if np.isfinite(v):\n",
    "            lam = float(EFF_WL_AA[b] / (1.0 + float(z)))\n",
    "            xs.append(lam)\n",
    "            ys.append(float(v))\n",
    "    xs = np.asarray(xs, float)\n",
    "    ys = np.asarray(ys, float)\n",
    "    if len(xs) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    try:\n",
    "        slope, intercept = np.polyfit(xs, ys, 1)\n",
    "        yhat = slope * xs + intercept\n",
    "        ss_res = float(np.sum((ys - yhat) ** 2))\n",
    "        ss_tot = float(np.sum((ys - np.mean(ys)) ** 2)) + EPS\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "        return float(slope), float(intercept), float(r2)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "\n",
    "def sed_logflux_vs_loglambda_at_time(band_tb, band_fb, band_eb, t0, z=0.0):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ws = []\n",
    "    for b in FILTERS:\n",
    "        tb = band_tb.get(b, None)\n",
    "        fb = band_fb.get(b, None)\n",
    "        eb = band_eb.get(b, None)\n",
    "        if tb is None or fb is None or eb is None:\n",
    "            continue\n",
    "        f = interp_flux_at_time(tb, fb, t0)\n",
    "        e = interp_err_at_time(tb, eb, t0)\n",
    "        if not np.isfinite(f) or not np.isfinite(e):\n",
    "            continue\n",
    "        if f <= 0:\n",
    "            continue\n",
    "        lam_rest = float(EFF_WL_AA[b] / (1.0 + float(z)))\n",
    "        xs.append(np.log(lam_rest + EPS))\n",
    "        ys.append(np.log(f + EPS))\n",
    "        ws.append(1.0 / ((e / (f + EPS)) ** 2 + EPS))  # weight by relative error\n",
    "    if len(xs) < 2:\n",
    "        return np.nan, np.nan, np.nan, float(len(xs))\n",
    "    xs = np.asarray(xs, float)\n",
    "    ys = np.asarray(ys, float)\n",
    "    ws = np.asarray(ws, float)\n",
    "\n",
    "    try:\n",
    "        W = np.sum(ws)\n",
    "        xbar = np.sum(ws * xs) / (W + EPS)\n",
    "        ybar = np.sum(ws * ys) / (W + EPS)\n",
    "        cov = np.sum(ws * (xs - xbar) * (ys - ybar))\n",
    "        var = np.sum(ws * (xs - xbar) ** 2) + EPS\n",
    "        slope = cov / var\n",
    "        intercept = ybar - slope * xbar\n",
    "        yhat = slope * xs + intercept\n",
    "        ss_res = float(np.sum(ws * (ys - yhat) ** 2))\n",
    "        ss_tot = float(np.sum(ws * (ys - ybar) ** 2)) + EPS\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "        return float(slope), float(intercept), float(r2), float(len(xs))\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, np.nan, float(len(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb779ee",
   "metadata": {
    "papermill": {
     "duration": 0.0033,
     "end_time": "2026-01-25T03:03:50.507130",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.503830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Had to add bounds to Bazin cause it was overflowing. Pain in the ass. Most of the formulas in here are new to me. Astronomy is not my forte. My entire strategy for this competition was to add quality features, which turned into just creating a plethora of features. Hard to manage. But hey if you are reading this you for some reason decided to look through my projects and are probably doubting whether or not I understand all of this. You are correct. I have a very strong understanding of ML, but when it comes to astronomy I am clueless. I followed a lead in a discussion to focus on what classifies a TDE from the SpecType feature available in the train set, but not in the test. My whole model is to create a seperate model to predict SpecType, and then use that to create even more features with the help of Chat GPT barreling through thousands of research papers and finding hundreds of formulas. I then throw XGB and run that shit through optuna overnight to get a decent model. It's worked. haven't ran this one yet, but I currently sit ~130th out of 800. Should move up to top 50 we will see. 2026/01/24 1:57 AM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f92e3eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.515128Z",
     "iopub.status.busy": "2026-01-25T03:03:50.514632Z",
     "iopub.status.idle": "2026-01-25T03:03:50.527971Z",
     "shell.execute_reply": "2026-01-25T03:03:50.527247Z"
    },
    "papermill": {
     "duration": 0.019075,
     "end_time": "2026-01-25T03:03:50.529407",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.510332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -60.0, 60.0)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def bazin_stable(t, A, t0, trise, tfall, B, eps=EPS):\n",
    "    \"\"\"\n",
    "    Numerically stable Bazin-like function:\n",
    "        f(t) = A * exp(-(t-t0)/tfall) * sigmoid((t-t0)/trise) + B\n",
    "    \"\"\"\n",
    "    trise = np.maximum(trise, eps)\n",
    "    tfall = np.maximum(tfall, eps)\n",
    "\n",
    "    x = (t - t0) / trise\n",
    "    exp_term = np.exp(np.clip(-(t - t0) / tfall, -60.0, 60.0))\n",
    "    return A * exp_term * sigmoid(x) + B\n",
    "\n",
    "\n",
    "def should_fit_bazin(tb, fb, eb, min_points=8, amp_sigma=3.0):\n",
    "    \"\"\"\n",
    "    Gate: only fit when there is enough data and a detectable transient-like signal.\n",
    "    \"\"\"\n",
    "    tb = np.asarray(tb, float)\n",
    "    fb = np.asarray(fb, float)\n",
    "    eb = np.asarray(eb, float)\n",
    "\n",
    "    if len(tb) < min_points:\n",
    "        return False\n",
    "\n",
    "    mederr = float(np.median(eb)) if np.isfinite(np.median(eb)) else np.inf\n",
    "    if not np.isfinite(mederr) or mederr <= 0:\n",
    "        return False\n",
    "\n",
    "    amp = float(np.percentile(fb, 95) - np.percentile(fb, 5))\n",
    "    if not np.isfinite(amp) or amp < amp_sigma * mederr:\n",
    "        return False\n",
    "\n",
    "    if float(np.std(fb)) < 1e-10:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def fit_bazin(tb, fb, eb):\n",
    "    \"\"\"\n",
    "    Returns (A, t0, trise, tfall, B, chi2_red) on success.\n",
    "    Returns (nan...nan) on failure or if scipy missing / gate fails.\n",
    "    \"\"\"\n",
    "    nan_out = (np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "\n",
    "    tb = np.asarray(tb, float)\n",
    "    fb = np.asarray(fb, float)\n",
    "    eb = np.asarray(eb, float)\n",
    "\n",
    "    order = np.argsort(tb)\n",
    "    t = tb[order]\n",
    "    f = fb[order]\n",
    "    e = eb[order]\n",
    "\n",
    "    m = np.isfinite(t) & np.isfinite(f) & np.isfinite(e)\n",
    "    t, f, e = t[m], f[m], e[m]\n",
    "    if len(t) < 3:\n",
    "        return nan_out\n",
    "\n",
    "    e = np.maximum(e, 1e-6)\n",
    "\n",
    "    if not should_fit_bazin(t, f, e, min_points=8, amp_sigma=3.0):\n",
    "        return nan_out\n",
    "\n",
    "    B0 = float(np.median(f))\n",
    "    A0 = float(max(1e-6, np.percentile(f, 95) - B0))\n",
    "    t0_0 = float(t[int(np.argmax(f))])\n",
    "    tr0 = 20.0\n",
    "    tf0 = 60.0\n",
    "    p0 = [A0, t0_0, tr0, tf0, B0]\n",
    "\n",
    "    tmin, tmax = float(t.min()), float(t.max())\n",
    "    iqr = float(np.percentile(f, 75) - np.percentile(f, 25))\n",
    "    amp = float(max(1e-6, np.percentile(f, 95) - np.percentile(f, 5)))\n",
    "\n",
    "    lo = [0.0, tmin - 50.0, 0.5, 1.0, B0 - 5.0 * (iqr + 1e-6)]\n",
    "    hi = [10.0 * amp, tmax + 50.0, 200.0, 600.0, B0 + 5.0 * (iqr + 1e-6)]\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(\n",
    "            bazin_stable, t, f,\n",
    "            p0=p0,\n",
    "            sigma=e,\n",
    "            absolute_sigma=True,\n",
    "            bounds=(lo, hi),\n",
    "            maxfev=5000\n",
    "        )\n",
    "\n",
    "        fhat = bazin_stable(t, *popt)\n",
    "        resid = (f - fhat) / e\n",
    "        chi2 = float(np.sum(resid * resid))\n",
    "        dof = max(1, len(t) - len(popt))\n",
    "        chi2_red = chi2 / dof\n",
    "\n",
    "        A, t0, trise, tfall, B = [float(x) for x in popt]\n",
    "        return (A, t0, trise, tfall, B, float(chi2_red))\n",
    "\n",
    "    except Exception:\n",
    "        return nan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e137831a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.538062Z",
     "iopub.status.busy": "2026-01-25T03:03:50.537605Z",
     "iopub.status.idle": "2026-01-25T03:03:50.588458Z",
     "shell.execute_reply": "2026-01-25T03:03:50.587830Z"
    },
    "papermill": {
     "duration": 0.056949,
     "end_time": "2026-01-25T03:03:50.589918",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.532969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features_for_object(lc_raw, z, z_err, ebv):\n",
    "    feats = {}\n",
    "    lc = lc_raw.sort_values(\"Time (MJD)\").reset_index(drop=True)\n",
    "\n",
    "    t = lc[\"Time (MJD)\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    if len(t) == 0:\n",
    "        feats[\"n_obs\"] = 0\n",
    "        return feats\n",
    "\n",
    "    z = safe_float(z, default=0.0)\n",
    "    z_err = safe_float(z_err, default=0.0)\n",
    "    ebv = safe_float(ebv, default=np.nan)\n",
    "\n",
    "    t_rel = t - t.min()\n",
    "    t_rest = t_rel / (1.0 + z)\n",
    "\n",
    "    flux_raw = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    err_raw = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "    flux_corr, err_corr = deextinct_lightcurve(lc, ebv)\n",
    "\n",
    "    feats[\"n_obs\"] = int(len(t))\n",
    "    feats[\"total_time_obs\"] = float(t_rel.max() - t_rel.min())\n",
    "    feats[\"total_time_rest\"] = float(t_rest.max() - t_rest.min())\n",
    "\n",
    "    feats[\"flux_mean\"] = float(np.mean(flux_corr))\n",
    "    feats[\"flux_median\"] = float(np.median(flux_corr))\n",
    "    feats[\"flux_std\"] = float(np.std(flux_corr))\n",
    "    feats[\"flux_min\"] = float(np.min(flux_corr))\n",
    "    feats[\"flux_max\"] = float(np.max(flux_corr))\n",
    "\n",
    "    feats[\"flux_mad\"] = median_abs_dev(flux_corr)\n",
    "    feats[\"flux_iqr\"] = iqr(flux_corr)\n",
    "    feats[\"flux_skew\"] = skewness(flux_corr)\n",
    "    feats[\"flux_kurt_excess\"] = kurtosis_excess(flux_corr)\n",
    "\n",
    "    p5, p25, p75, p95 = np.percentile(flux_corr, [5, 25, 75, 95])\n",
    "    feats[\"flux_p5\"] = float(p5)\n",
    "    feats[\"flux_p25\"] = float(p25)\n",
    "    feats[\"flux_p75\"] = float(p75)\n",
    "    feats[\"flux_p95\"] = float(p95)\n",
    "    feats[\"robust_amp_global\"] = float(p95 - p5)\n",
    "\n",
    "    feats[\"neg_flux_frac\"] = float(np.mean(flux_corr < 0))\n",
    "\n",
    "    snr = np.abs(flux_corr) / (err_corr + EPS)\n",
    "    feats[\"snr_median\"] = float(np.median(snr))\n",
    "    feats[\"snr_max\"] = float(np.max(snr))\n",
    "\n",
    "    feats[\"flux_mean_raw\"] = float(np.mean(flux_raw))\n",
    "    feats[\"flux_std_raw\"] = float(np.std(flux_raw))\n",
    "    feats[\"snr_max_raw\"] = float(np.max(np.abs(flux_raw) / (err_raw + EPS)))\n",
    "    feats[\"fvar_raw\"] = fractional_variability(flux_raw, err_raw)\n",
    "\n",
    "    feats[\"flux_mean_deext_minus_raw\"] = float(feats[\"flux_mean\"] - feats[\"flux_mean_raw\"])\n",
    "    feats[\"snrmax_deext_minus_raw\"] = float(feats[\"snr_max\"] - feats[\"snr_max_raw\"])\n",
    "\n",
    "    if len(t_rel) >= 2:\n",
    "        dt = np.diff(t_rel)\n",
    "        feats[\"median_dt\"] = float(np.median(dt))\n",
    "        feats[\"max_gap\"] = float(np.max(dt))\n",
    "        feats[\"n_seasons_global\"] = float(np.sum(dt > SEASON_GAP_DAYS) + 1)\n",
    "        feats[\"gap_frac_gt90\"] = float(np.mean(dt > SEASON_GAP_DAYS))\n",
    "        feats[\"gap_frac_gt30\"] = float(np.mean(dt > 30.0))\n",
    "    else:\n",
    "        feats[\"median_dt\"] = np.nan\n",
    "        feats[\"max_gap\"] = np.nan\n",
    "        feats[\"n_seasons_global\"] = np.nan\n",
    "        feats[\"gap_frac_gt90\"] = np.nan\n",
    "        feats[\"gap_frac_gt30\"] = np.nan\n",
    "\n",
    "    feats[\"eta_von_neumann\"] = von_neumann_eta(flux_corr)\n",
    "    feats[\"chi2_const_global\"] = chi2_to_constant(flux_corr, err_corr)\n",
    "    feats[\"stetsonJ_global_obs\"] = stetson_J_consecutive(t_rel, flux_corr, err_corr)\n",
    "    feats[\"stetsonJ_global_rest\"] = stetson_J_consecutive(t_rest, flux_corr, err_corr)\n",
    "\n",
    "    feats[\"max_slope_global_obs\"] = max_slope(t_rel, flux_corr)\n",
    "    feats[\"max_slope_global_rest\"] = max_slope(t_rest, flux_corr)\n",
    "\n",
    "    feats[\"med_abs_slope_global_obs\"] = median_abs_slope(t_rel, flux_corr)\n",
    "    feats[\"med_abs_slope_global_rest\"] = median_abs_slope(t_rest, flux_corr)\n",
    "\n",
    "    feats[\"slope_global_obs\"] = linear_slope(t_rel, flux_corr)\n",
    "    feats[\"slope_global_rest\"] = linear_slope(t_rest, flux_corr)\n",
    "\n",
    "    feats[\"fvar_global\"] = fractional_variability(flux_corr, err_corr)\n",
    "\n",
    "    feats[\"Z\"] = float(z)\n",
    "    feats[\"log1pZ\"] = float(np.log1p(max(0.0, z)))\n",
    "    feats[\"Z_err\"] = float(max(0.0, z_err))\n",
    "    feats[\"log1pZerr\"] = float(np.log1p(max(0.0, feats[\"Z_err\"])))\n",
    "    feats[\"EBV\"] = ebv\n",
    "\n",
    "    feats[\"n_filters_present\"] = 0\n",
    "    feats[\"total_obs\"] = 0\n",
    "\n",
    "    band_tpeak_obs = {}\n",
    "    band_tpeak_rest = {}\n",
    "    band_peak_flux = {}\n",
    "\n",
    "    band_tb_obs = {}\n",
    "    band_tb_rest = {}\n",
    "    band_fb = {}\n",
    "    band_eb = {}\n",
    "\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        nb = int(np.sum(m))\n",
    "        feats[f\"n_{b}\"] = nb\n",
    "        feats[\"total_obs\"] += nb\n",
    "\n",
    "        keys = [\n",
    "            f\"amp_{b}\",\n",
    "            f\"amp_pre_{b}\",\n",
    "            f\"baseline_pre_{b}\",\n",
    "            f\"robust_amp_{b}\",\n",
    "            f\"tpeak_{b}_obs\",\n",
    "            f\"tpeak_{b}_rest\",\n",
    "            f\"width50_{b}_obs\",\n",
    "            f\"width50_{b}_rest\",\n",
    "            f\"width80_{b}_obs\",\n",
    "            f\"width80_{b}_rest\",\n",
    "            f\"auc_pos_{b}_obs\",\n",
    "            f\"auc_pos_{b}_rest\",\n",
    "            f\"snrmax_{b}\",\n",
    "            f\"eta_{b}\",\n",
    "            f\"chi2_const_{b}\",\n",
    "            f\"slope_{b}_obs\",\n",
    "            f\"slope_{b}_rest\",\n",
    "            f\"maxslope_{b}_obs\",\n",
    "            f\"maxslope_{b}_rest\",\n",
    "            f\"stetsonJ_{b}_obs\",\n",
    "            f\"stetsonJ_{b}_rest\",\n",
    "            f\"p5_{b}\",\n",
    "            f\"p25_{b}\",\n",
    "            f\"p75_{b}\",\n",
    "            f\"p95_{b}\",\n",
    "            f\"mad_{b}\",\n",
    "            f\"iqr_{b}\",\n",
    "            f\"mad_over_std_{b}\",\n",
    "            f\"fvar_{b}\",\n",
    "            f\"t_fall50_{b}_obs\",\n",
    "            f\"t_fall20_{b}_obs\",\n",
    "            f\"t_fall50_{b}_rest\",\n",
    "            f\"t_fall20_{b}_rest\",\n",
    "            f\"t_rise50_{b}_obs\",\n",
    "            f\"t_rise20_{b}_obs\",\n",
    "            f\"t_rise50_{b}_rest\",\n",
    "            f\"t_rise20_{b}_rest\",\n",
    "            f\"asym50_{b}_obs\",\n",
    "            f\"asym50_{b}_rest\",\n",
    "            f\"sharp50_{b}_obs\",\n",
    "            f\"sharp50_{b}_rest\",\n",
    "            f\"peak_dominance_{b}\",\n",
    "            f\"std_ratio_prepost_{b}\",\n",
    "            f\"n_peaks_{b}\",\n",
    "            f\"postpeak_monotone_frac_{b}\",\n",
    "            f\"n_rebrighten_{b}\",\n",
    "            f\"decay_pl_slope_{b}_obs\",\n",
    "            f\"decay_pl_r2_{b}_obs\",\n",
    "            f\"decay_pl_npts_{b}_obs\",\n",
    "            f\"decay_pl_slope_{b}_rest\",\n",
    "            f\"decay_pl_r2_{b}_rest\",\n",
    "            f\"decay_pl_npts_{b}_rest\",\n",
    "\n",
    "            # seasonality and structure function per band\n",
    "            f\"n_seasons_{b}\",\n",
    "            f\"season_maxspan_{b}\",\n",
    "            f\"season_meanspan_{b}\",\n",
    "            f\"sf_medabs_5_{b}\",\n",
    "            f\"sf_n_5_{b}\",\n",
    "            f\"sf_medabs_10_{b}\",\n",
    "            f\"sf_n_10_{b}\",\n",
    "            f\"sf_medabs_20_{b}\",\n",
    "            f\"sf_n_20_{b}\",\n",
    "            f\"sf_medabs_50_{b}\",\n",
    "            f\"sf_n_50_{b}\",\n",
    "            f\"sf_medabs_100_{b}\",\n",
    "            f\"sf_n_100_{b}\",\n",
    "\n",
    "            # Bazin shape fit\n",
    "            f\"bazin_A_{b}\",\n",
    "            f\"bazin_t0_{b}_obs\",\n",
    "            f\"bazin_trise_{b}_obs\",\n",
    "            f\"bazin_tfall_{b}_obs\",\n",
    "            f\"bazin_B_{b}\",\n",
    "            f\"bazin_chi2red_{b}_obs\",\n",
    "            f\"bazin_trise_{b}_rest\",\n",
    "            f\"bazin_tfall_{b}_rest\",\n",
    "        ]\n",
    "        for k in keys:\n",
    "            feats[k] = np.nan\n",
    "\n",
    "        if nb == 0:\n",
    "            continue\n",
    "\n",
    "        feats[\"n_filters_present\"] += 1\n",
    "\n",
    "        tb_obs = t_rel[m]\n",
    "        fb = flux_corr[m]\n",
    "        eb = err_corr[m]\n",
    "\n",
    "        order = np.argsort(tb_obs)\n",
    "        tb_obs = tb_obs[order]\n",
    "        fb = fb[order]\n",
    "        eb = eb[order]\n",
    "        tb_rest = tb_obs / (1.0 + z)\n",
    "\n",
    "        band_tb_obs[b] = tb_obs\n",
    "        band_tb_rest[b] = tb_rest\n",
    "        band_fb[b] = fb\n",
    "        band_eb[b] = eb\n",
    "\n",
    "        ns, maxsp, meansp = seasonality_features(tb_obs)\n",
    "        feats[f\"n_seasons_{b}\"] = ns\n",
    "        feats[f\"season_maxspan_{b}\"] = maxsp\n",
    "        feats[f\"season_meanspan_{b}\"] = meansp\n",
    "\n",
    "        sf = structure_function_lags(tb_obs, fb, lags=SF_LAGS)\n",
    "        for lag in SF_LAGS:\n",
    "            feats[f\"sf_medabs_{int(lag)}_{b}\"] = sf.get(f\"sf_medabs_{int(lag)}\", np.nan)\n",
    "            feats[f\"sf_n_{int(lag)}_{b}\"] = sf.get(f\"sf_n_{int(lag)}\", 0.0)\n",
    "\n",
    "        p5b, p25b, p75b, p95b = np.percentile(fb, [5, 25, 75, 95])\n",
    "        feats[f\"p5_{b}\"] = float(p5b)\n",
    "        feats[f\"p25_{b}\"] = float(p25b)\n",
    "        feats[f\"p75_{b}\"] = float(p75b)\n",
    "        feats[f\"p95_{b}\"] = float(p95b)\n",
    "        feats[f\"robust_amp_{b}\"] = float(p95b - p5b)\n",
    "\n",
    "        feats[f\"mad_{b}\"] = median_abs_dev(fb)\n",
    "        feats[f\"iqr_{b}\"] = iqr(fb)\n",
    "        stdb = float(np.std(fb))\n",
    "        feats[f\"mad_over_std_{b}\"] = float(feats[f\"mad_{b}\"] / (stdb + EPS))\n",
    "\n",
    "        base_pre, mad_pre, mederr_pre = pre_peak_baseline(tb_obs, fb, eb, frac=PRE_BASE_FRAC)\n",
    "        feats[f\"baseline_pre_{b}\"] = float(base_pre) if np.isfinite(base_pre) else np.nan\n",
    "\n",
    "        pidx = int(np.argmax(fb))\n",
    "        peak_flux = float(fb[pidx])\n",
    "        tpeak_obs = float(tb_obs[pidx])\n",
    "        tpeak_rest = float(tb_rest[pidx])\n",
    "\n",
    "        amp_median = peak_flux - float(np.median(fb))\n",
    "        amp_pre = peak_flux - base_pre if np.isfinite(base_pre) else np.nan\n",
    "\n",
    "        feats[f\"amp_{b}\"] = float(amp_median)\n",
    "        feats[f\"amp_pre_{b}\"] = float(amp_pre) if np.isfinite(amp_pre) else np.nan\n",
    "\n",
    "        feats[f\"tpeak_{b}_obs\"] = tpeak_obs\n",
    "        feats[f\"tpeak_{b}_rest\"] = tpeak_rest\n",
    "        feats[f\"snrmax_{b}\"] = float(np.max(np.abs(fb) / (eb + EPS)))\n",
    "\n",
    "        feats[f\"eta_{b}\"] = von_neumann_eta(fb)\n",
    "        feats[f\"chi2_const_{b}\"] = chi2_to_constant(fb, eb)\n",
    "\n",
    "        feats[f\"slope_{b}_obs\"] = linear_slope(tb_obs, fb)\n",
    "        feats[f\"slope_{b}_rest\"] = linear_slope(tb_rest, fb)\n",
    "\n",
    "        feats[f\"maxslope_{b}_obs\"] = max_slope(tb_obs, fb)\n",
    "        feats[f\"maxslope_{b}_rest\"] = max_slope(tb_rest, fb)\n",
    "\n",
    "        feats[f\"stetsonJ_{b}_obs\"] = stetson_J_consecutive(tb_obs, fb, eb)\n",
    "        feats[f\"stetsonJ_{b}_rest\"] = stetson_J_consecutive(tb_rest, fb, eb)\n",
    "\n",
    "        feats[f\"fvar_{b}\"] = fractional_variability(fb, eb)\n",
    "\n",
    "        A, t0, trise, tfall, B, chi2 = fit_bazin(tb_obs, fb, eb)\n",
    "        feats[f\"bazin_A_{b}\"] = A\n",
    "        feats[f\"bazin_t0_{b}_obs\"] = t0\n",
    "        feats[f\"bazin_trise_{b}_obs\"] = trise\n",
    "        feats[f\"bazin_tfall_{b}_obs\"] = tfall\n",
    "        feats[f\"bazin_B_{b}\"] = B\n",
    "        feats[f\"bazin_chi2red_{b}_obs\"] = chi2\n",
    "\n",
    "        feats[f\"bazin_trise_{b}_rest\"] = trise / (1.0 + z) if np.isfinite(trise) else np.nan\n",
    "        feats[f\"bazin_tfall_{b}_rest\"] = tfall / (1.0 + z) if np.isfinite(tfall) else np.nan\n",
    "\n",
    "        if np.isfinite(amp_pre) and amp_pre > 0:\n",
    "            feats[f\"peak_dominance_{b}\"] = float(amp_pre / (mad_pre + EPS))\n",
    "\n",
    "            pre_seg = fb[:max(2, pidx)]\n",
    "            post_seg = fb[pidx:]\n",
    "            std_pre = float(np.std(pre_seg)) if len(pre_seg) >= 2 else np.nan\n",
    "            std_post = float(np.std(post_seg)) if len(post_seg) >= 2 else np.nan\n",
    "            if np.isfinite(std_pre) and np.isfinite(std_post):\n",
    "                feats[f\"std_ratio_prepost_{b}\"] = float(std_pre / (std_post + EPS))\n",
    "\n",
    "            feats[f\"postpeak_monotone_frac_{b}\"] = float(postpeak_monotonicity(tb_obs, fb, pidx))\n",
    "            feats[f\"n_peaks_{b}\"] = float(count_significant_peaks(tb_obs, fb, eb, base_pre, k_sigma=PEAK_SIGMA_K))\n",
    "            feats[f\"n_rebrighten_{b}\"] = float(count_rebrighten(tb_obs, fb, base_pre, amp_pre, pidx, frac=REBRIGHT_FRAC))\n",
    "\n",
    "            feats[f\"t_fall50_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "            feats[f\"t_fall50_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "\n",
    "            feats[f\"t_rise50_{b}_obs\"] = float(rise_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_rise20_{b}_obs\"] = float(rise_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "            feats[f\"t_rise50_{b}_rest\"] = float(rise_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_rise20_{b}_rest\"] = float(rise_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "\n",
    "            tr50o = feats[f\"t_rise50_{b}_obs\"]\n",
    "            tf50o = feats[f\"t_fall50_{b}_obs\"]\n",
    "            tr50r = feats[f\"t_rise50_{b}_rest\"]\n",
    "            tf50r = feats[f\"t_fall50_{b}_rest\"]\n",
    "            feats[f\"asym50_{b}_obs\"] = float(tf50o / (tr50o + EPS)) if np.isfinite(tf50o) and np.isfinite(tr50o) else np.nan\n",
    "            feats[f\"asym50_{b}_rest\"] = float(tf50r / (tr50r + EPS)) if np.isfinite(tf50r) and np.isfinite(tr50r) else np.nan\n",
    "\n",
    "            feats[f\"auc_pos_{b}_obs\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_obs))\n",
    "            feats[f\"auc_pos_{b}_rest\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_rest))\n",
    "\n",
    "            def width_at_level(tt, ff, base, amp, frac):\n",
    "                if amp <= 0 or len(ff) < 3:\n",
    "                    return np.nan\n",
    "                level = base + frac * amp\n",
    "                above = ff >= level\n",
    "                if not np.any(above):\n",
    "                    return np.nan\n",
    "                idx = np.where(above)[0]\n",
    "                return float(tt[idx[-1]] - tt[idx[0]])\n",
    "\n",
    "            w50_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.80)\n",
    "            w50_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.80)\n",
    "\n",
    "            feats[f\"width50_{b}_obs\"] = w50_obs\n",
    "            feats[f\"width80_{b}_obs\"] = w80_obs\n",
    "            feats[f\"width50_{b}_rest\"] = w50_rest\n",
    "            feats[f\"width80_{b}_rest\"] = w80_rest\n",
    "\n",
    "            feats[f\"sharp50_{b}_obs\"] = float(amp_pre / (w50_obs + EPS)) if np.isfinite(w50_obs) else np.nan\n",
    "            feats[f\"sharp50_{b}_rest\"] = float(amp_pre / (w50_rest + EPS)) if np.isfinite(w50_rest) else np.nan\n",
    "\n",
    "            b_obs, r2_obs, npts_obs = decay_powerlaw_fit(tb_obs, fb, base_pre, pidx, tmax=300.0)\n",
    "            b_rest, r2_rest, npts_rest = decay_powerlaw_fit(tb_rest, fb, base_pre, pidx, tmax=300.0)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_obs\"] = b_obs\n",
    "            feats[f\"decay_pl_r2_{b}_obs\"] = r2_obs\n",
    "            feats[f\"decay_pl_npts_{b}_obs\"] = float(npts_obs)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_rest\"] = b_rest\n",
    "            feats[f\"decay_pl_r2_{b}_rest\"] = r2_rest\n",
    "            feats[f\"decay_pl_npts_{b}_rest\"] = float(npts_rest)\n",
    "\n",
    "        band_tpeak_obs[b] = tpeak_obs\n",
    "        band_tpeak_rest[b] = tpeak_rest\n",
    "        band_peak_flux[b] = peak_flux\n",
    "\n",
    "    tpeaks_obs = np.array([band_tpeak_obs.get(b, np.nan) for b in FILTERS], float)\n",
    "    tpeaks_rest = np.array([band_tpeak_rest.get(b, np.nan) for b in FILTERS], float)\n",
    "    tpeaks_obs = np.array([x for x in tpeaks_obs if np.isfinite(x)], float)\n",
    "    tpeaks_rest = np.array([x for x in tpeaks_rest if np.isfinite(x)], float)\n",
    "    feats[\"tpeak_std_obs\"] = float(np.std(tpeaks_obs)) if len(tpeaks_obs) >= 2 else np.nan\n",
    "    feats[\"tpeak_std_rest\"] = float(np.std(tpeaks_rest)) if len(tpeaks_rest) >= 2 else np.nan\n",
    "\n",
    "    pairs = [(\"u\", \"g\"), (\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\"), (\"z\", \"y\")]\n",
    "    for a, b in pairs:\n",
    "        ta_obs = band_tpeak_obs.get(a, np.nan)\n",
    "        tb_obs2 = band_tpeak_obs.get(b, np.nan)\n",
    "        ta_rest = band_tpeak_rest.get(a, np.nan)\n",
    "        tb_rest2 = band_tpeak_rest.get(b, np.nan)\n",
    "        pa = band_peak_flux.get(a, np.nan)\n",
    "        pb = band_peak_flux.get(b, np.nan)\n",
    "\n",
    "        feats[f\"tpeakdiff_{a}{b}_obs\"] = (ta_obs - tb_obs2) if (np.isfinite(ta_obs) and np.isfinite(tb_obs2)) else np.nan\n",
    "        feats[f\"tpeakdiff_{a}{b}_rest\"] = (ta_rest - tb_rest2) if (np.isfinite(ta_rest) and np.isfinite(tb_rest2)) else np.nan\n",
    "        feats[f\"peakratio_{a}{b}\"] = (pa / (pb + EPS)) if (np.isfinite(pa) and np.isfinite(pb)) else np.nan\n",
    "\n",
    "    def ratio_feature(name, num, den):\n",
    "        if np.isfinite(num) and np.isfinite(den):\n",
    "            feats[name] = float(num / (den + EPS))\n",
    "        else:\n",
    "            feats[name] = np.nan\n",
    "\n",
    "    for a, b in pairs:\n",
    "        ratio_feature(f\"amppreratio_{a}{b}\", feats.get(f\"amp_pre_{a}\", np.nan), feats.get(f\"amp_pre_{b}\", np.nan))\n",
    "        ratio_feature(f\"aucratio_{a}{b}_obs\", feats.get(f\"auc_pos_{a}_obs\", np.nan), feats.get(f\"auc_pos_{b}_obs\", np.nan))\n",
    "        ratio_feature(f\"width50ratio_{a}{b}_obs\", feats.get(f\"width50_{a}_obs\", np.nan), feats.get(f\"width50_{b}_obs\", np.nan))\n",
    "        ratio_feature(f\"asym50ratio_{a}{b}_obs\", feats.get(f\"asym50_{a}_obs\", np.nan), feats.get(f\"asym50_{b}_obs\", np.nan))\n",
    "\n",
    "    for a, b in [(\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\")]:\n",
    "        if (a in band_tb_obs) and (b in band_tb_obs):\n",
    "            feats[f\"corr_{a}{b}_obs\"] = band_corr(\n",
    "                band_tb_obs[a], band_fb[a],\n",
    "                band_tb_obs[b], band_fb[b]\n",
    "            )\n",
    "        else:\n",
    "            feats[f\"corr_{a}{b}_obs\"] = np.nan\n",
    "\n",
    "    slope_t, intercept_t, r2_t = peak_vs_wavelength_slope(band_tpeak_obs, band_tpeak_obs, z=z)\n",
    "    feats[\"tpeak_vs_lambda_slope_obs\"] = slope_t\n",
    "    feats[\"tpeak_vs_lambda_intercept_obs\"] = intercept_t\n",
    "    feats[\"tpeak_vs_lambda_r2_obs\"] = r2_t\n",
    "\n",
    "    slope_pf, intercept_pf, r2_pf = peak_vs_wavelength_slope(band_tpeak_obs, band_peak_flux, z=z)\n",
    "    feats[\"peakflux_vs_lambda_slope\"] = slope_pf\n",
    "    feats[\"peakflux_vs_lambda_intercept\"] = intercept_pf\n",
    "    feats[\"peakflux_vs_lambda_r2\"] = r2_pf\n",
    "\n",
    "    tpr_obs = feats.get(\"tpeak_r_obs\", np.nan)\n",
    "    if np.isfinite(tpr_obs):\n",
    "        def colors_at_time(t0):\n",
    "            fr = interp_flux_at_time(band_tb_obs.get(\"r\", np.array([])), band_fb.get(\"r\", np.array([])), t0)\n",
    "            fg = interp_flux_at_time(band_tb_obs.get(\"g\", np.array([])), band_fb.get(\"g\", np.array([])), t0)\n",
    "            fi = interp_flux_at_time(band_tb_obs.get(\"i\", np.array([])), band_fb.get(\"i\", np.array([])), t0)\n",
    "            cgr = (signed_log1p(fg) - signed_log1p(fr)) if (np.isfinite(fg) and np.isfinite(fr)) else np.nan\n",
    "            cri = (signed_log1p(fr) - signed_log1p(fi)) if (np.isfinite(fr) and np.isfinite(fi)) else np.nan\n",
    "            return cgr, cri\n",
    "\n",
    "        cgr0, cri0 = colors_at_time(tpr_obs)\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = cgr0\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = cri0\n",
    "\n",
    "        cgr20, cri20 = colors_at_time(tpr_obs + 20.0)\n",
    "        cgr40, cri40 = colors_at_time(tpr_obs + 40.0)\n",
    "\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = cgr20\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = cri20\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = cgr40\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = cri40\n",
    "\n",
    "        def slope(c1, c2, dt):\n",
    "            if np.isfinite(c1) and np.isfinite(c2):\n",
    "                return float((c2 - c1) / dt)\n",
    "            return np.nan\n",
    "\n",
    "        feats[\"color_gr_slope20_obs\"] = slope(cgr0, cgr20, 20.0)\n",
    "        feats[\"color_ri_slope20_obs\"] = slope(cri0, cri20, 20.0)\n",
    "        feats[\"color_gr_slope40_obs\"] = slope(cgr0, cgr40, 40.0)\n",
    "        feats[\"color_ri_slope40_obs\"] = slope(cri0, cri40, 40.0)\n",
    "    else:\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope20_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope20_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope40_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope40_obs\"] = np.nan\n",
    "\n",
    "    if np.isfinite(tpr_obs):\n",
    "        sed_slope, sed_int, sed_r2, sed_n = sed_logflux_vs_loglambda_at_time(\n",
    "            band_tb_obs, band_fb, band_eb, tpr_obs, z=z\n",
    "        )\n",
    "        feats[\"sed_logflux_loglambda_slope_rpeak\"] = sed_slope\n",
    "        feats[\"sed_logflux_loglambda_r2_rpeak\"] = sed_r2\n",
    "        feats[\"sed_logflux_loglambda_nbands_rpeak\"] = sed_n\n",
    "\n",
    "        sed_slope20, sed_int20, sed_r2_20, sed_n20 = sed_logflux_vs_loglambda_at_time(\n",
    "            band_tb_obs, band_fb, band_eb, tpr_obs + 20.0, z=z\n",
    "        )\n",
    "        feats[\"sed_slope_rpeak_p20\"] = sed_slope20\n",
    "        feats[\"sed_r2_rpeak_p20\"] = sed_r2_20\n",
    "        feats[\"sed_nbands_rpeak_p20\"] = sed_n20\n",
    "    else:\n",
    "        feats[\"sed_logflux_loglambda_slope_rpeak\"] = np.nan\n",
    "        feats[\"sed_logflux_loglambda_r2_rpeak\"] = np.nan\n",
    "        feats[\"sed_logflux_loglambda_nbands_rpeak\"] = np.nan\n",
    "        feats[\"sed_slope_rpeak_p20\"] = np.nan\n",
    "        feats[\"sed_r2_rpeak_p20\"] = np.nan\n",
    "        feats[\"sed_nbands_rpeak_p20\"] = np.nan\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05869091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.598719Z",
     "iopub.status.busy": "2026-01-25T03:03:50.598266Z",
     "iopub.status.idle": "2026-01-25T03:03:50.609820Z",
     "shell.execute_reply": "2026-01-25T03:03:50.609101Z"
    },
    "papermill": {
     "duration": 0.017784,
     "end_time": "2026-01-25T03:03:50.611319",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.593535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_lightcurve_cache(splits, base_dir=\"data\", kind=\"train\"):\n",
    "    lc_cache = {}\n",
    "    idx_cache = {}\n",
    "    for s in splits:\n",
    "        path = f\"{base_dir}/{s}/{kind}_full_lightcurves.csv\"\n",
    "        lc = pd.read_csv(path)\n",
    "        lc[\"object_id\"] = lc[\"object_id\"].astype(str)\n",
    "        groups = lc.groupby(\"object_id\").indices\n",
    "        lc_cache[s] = lc\n",
    "        idx_cache[s] = groups\n",
    "    return lc_cache, idx_cache\n",
    "\n",
    "\n",
    "def get_lightcurve(lc_cache, idx_cache, split, object_id):\n",
    "    object_id = str(object_id)\n",
    "    idx = idx_cache[split].get(object_id, None)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    return lc_cache[split].iloc[idx]\n",
    "\n",
    "def build_feature_table(\n",
    "    log_df,\n",
    "    lc_cache,\n",
    "    idx_cache,\n",
    "    augment_photoz=False,\n",
    "    test_zerr_pool=None,\n",
    "    n_aug=2,\n",
    "    seed=6\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    if test_zerr_pool is not None:\n",
    "        test_zerr_pool = np.asarray(test_zerr_pool, float)\n",
    "        test_zerr_pool = test_zerr_pool[np.isfinite(test_zerr_pool)]\n",
    "        test_zerr_pool = test_zerr_pool[test_zerr_pool > 0]\n",
    "\n",
    "    for i in range(len(log_df)):\n",
    "        r = log_df.iloc[i]\n",
    "        obj = str(r[\"object_id\"])\n",
    "        split = r[\"split\"]\n",
    "\n",
    "        lc = get_lightcurve(lc_cache, idx_cache, split, obj)\n",
    "        if lc is None:\n",
    "            feats = {\"n_obs\": 0}\n",
    "            feats[\"object_id\"] = obj\n",
    "            feats[\"split\"] = split\n",
    "            feats[\"photoz_aug\"] = 0\n",
    "            if \"target\" in log_df.columns:\n",
    "                feats[\"target\"] = int(r[\"target\"])\n",
    "            rows.append(feats)\n",
    "            continue\n",
    "\n",
    "        feats = extract_features_for_object(\n",
    "            lc_raw=lc,\n",
    "            z=r[\"Z\"],\n",
    "            z_err=r.get(\"Z_err\", 0.0),\n",
    "            ebv=r[\"EBV\"],\n",
    "        )\n",
    "        feats[\"object_id\"] = obj\n",
    "        feats[\"split\"] = split\n",
    "        feats[\"photoz_aug\"] = 0\n",
    "        if \"target\" in log_df.columns:\n",
    "            feats[\"target\"] = int(r[\"target\"])\n",
    "        rows.append(feats)\n",
    "\n",
    "        if augment_photoz and (\"target\" in log_df.columns) and (test_zerr_pool is not None) and (len(test_zerr_pool) > 0):\n",
    "            z0 = safe_float(r[\"Z\"], default=0.0)\n",
    "            for _ in range(n_aug):\n",
    "                sigma = float(rng.choice(test_zerr_pool))\n",
    "                z_sim = max(0.0, z0 + float(rng.normal(0.0, sigma)))\n",
    "                feats2 = extract_features_for_object(\n",
    "                    lc_raw=lc,\n",
    "                    z=z_sim,\n",
    "                    z_err=sigma,\n",
    "                    ebv=r[\"EBV\"],\n",
    "                )\n",
    "                feats2[\"object_id\"] = obj\n",
    "                feats2[\"split\"] = split\n",
    "                feats2[\"target\"] = int(r[\"target\"])\n",
    "                feats2[\"photoz_aug\"] = 1\n",
    "                rows.append(feats2)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def clean_features(df, drop_cols, add_missing_flags=True):\n",
    "    X = df.drop(columns=drop_cols).copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if add_missing_flags:\n",
    "        miss = X.isna().astype(np.uint8)\n",
    "        miss.columns = [c + \"_isnan\" for c in miss.columns]\n",
    "        X = pd.concat([X, miss], axis=1)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6152d80d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.620053Z",
     "iopub.status.busy": "2026-01-25T03:03:50.619444Z",
     "iopub.status.idle": "2026-01-25T03:03:50.623915Z",
     "shell.execute_reply": "2026-01-25T03:03:50.623302Z"
    },
    "papermill": {
     "duration": 0.010136,
     "end_time": "2026-01-25T03:03:50.625291",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.615155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def best_threshold_f1(y_true, probs):\n",
    "    ths = np.linspace(0.01, 0.99, 401)\n",
    "    f1s = [f1_score(y_true, probs > t, zero_division=0) for t in ths]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return float(ths[j]), float(f1s[j])\n",
    "\n",
    "\n",
    "def make_splitter(n_splits, random_state=6):\n",
    "    return StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca17d12c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.633819Z",
     "iopub.status.busy": "2026-01-25T03:03:50.633390Z",
     "iopub.status.idle": "2026-01-25T03:03:50.643954Z",
     "shell.execute_reply": "2026-01-25T03:03:50.643332Z"
    },
    "papermill": {
     "duration": 0.016494,
     "end_time": "2026-01-25T03:03:50.645286",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.628792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10, seed=6):\n",
    "    df = train_feat.merge(train_log[[\"object_id\", \"SpecType\"]], on=\"object_id\", how=\"left\")\n",
    "    spec = df[\"SpecType\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "    def map_group(s):\n",
    "        s2 = s.strip()\n",
    "        if s2 == \"TDE\":\n",
    "            return \"TDE\"\n",
    "        if s2 == \"AGN\":\n",
    "            return \"AGN\"\n",
    "        if \"SLSN\" in s2:\n",
    "            return \"SLSN\"\n",
    "        if s2 == \"SN Ia\" or s2.startswith(\"SN Ia\"):\n",
    "            return \"SNIa\"\n",
    "        if s2.startswith(\"SN II\") or (\"SN II\" in s2):\n",
    "            return \"SNII\"\n",
    "        if s2.startswith(\"SN\"):\n",
    "            return \"SNother\"\n",
    "        return \"Other\"\n",
    "\n",
    "    spec_group = spec.map(map_group).astype(str)\n",
    "\n",
    "    classes = sorted(spec_group.unique())\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_mc = spec_group.map(class_to_idx).to_numpy()\n",
    "\n",
    "    X_tr = clean_features(df, drop_cols=[\"object_id\", \"split\", \"target\", \"SpecType\"], add_missing_flags=True)\n",
    "    X_te = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"], add_missing_flags=True)\n",
    "\n",
    "    groups = df[\"split\"].to_numpy()\n",
    "    splitter = make_splitter(n_splits, random_state=seed)\n",
    "    split_iter = splitter.split(X_tr, y_mc, groups)\n",
    "\n",
    "    oof = np.zeros((len(X_tr), len(classes)), dtype=float)\n",
    "\n",
    "    base = dict(\n",
    "        objective=\"multiclass\",\n",
    "        num_class=len(classes),\n",
    "        metric=\"multi_logloss\",\n",
    "        n_estimators=20000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=63,\n",
    "        min_child_samples=5,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        verbosity=-1,\n",
    "        force_col_wise=True,\n",
    "    )\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        model = LGBMClassifier(**base)\n",
    "        model.fit(\n",
    "            X_tr.iloc[tr_idx], y_mc[tr_idx],\n",
    "            eval_set=[(X_tr.iloc[va_idx], y_mc[va_idx])],\n",
    "            eval_metric=\"multi_logloss\",\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)],\n",
    "        )\n",
    "\n",
    "        best_iter = getattr(model, \"best_iteration_\", None)\n",
    "        if best_iter is None:\n",
    "            oof[va_idx] = model.predict_proba(X_tr.iloc[va_idx])\n",
    "        else:\n",
    "            oof[va_idx] = model.predict_proba(X_tr.iloc[va_idx], num_iteration=best_iter)\n",
    "\n",
    "    model_full = LGBMClassifier(**base)\n",
    "    model_full.fit(X_tr, y_mc)\n",
    "    p_test = model_full.predict_proba(X_te)\n",
    "\n",
    "    def entropy(p):\n",
    "        p = np.clip(p, 1e-12, 1.0)\n",
    "        return -np.sum(p * np.log(p), axis=1)\n",
    "\n",
    "    for i, c in enumerate(classes):\n",
    "        train_feat[f\"p_spec_{c}\"] = oof[:, i]\n",
    "        test_feat[f\"p_spec_{c}\"] = p_test[:, i]\n",
    "\n",
    "    train_feat[\"spec_entropy\"] = entropy(oof)\n",
    "    test_feat[\"spec_entropy\"] = entropy(p_test)\n",
    "\n",
    "    train_feat[\"spec_topprob\"] = np.max(oof, axis=1)\n",
    "    test_feat[\"spec_topprob\"] = np.max(p_test, axis=1)\n",
    "\n",
    "    return train_feat, test_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36019f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.653708Z",
     "iopub.status.busy": "2026-01-25T03:03:50.652988Z",
     "iopub.status.idle": "2026-01-25T03:03:50.660071Z",
     "shell.execute_reply": "2026-01-25T03:03:50.659527Z"
    },
    "papermill": {
     "duration": 0.012721,
     "end_time": "2026-01-25T03:03:50.661408",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.648687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_select_gain_topk(train_feat, k=350, n_splits=10, seed=6):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "    X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "\n",
    "    splitter = make_splitter(n_splits, random_state=seed)\n",
    "    split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "    gains = {c: 0.0 for c in X.columns}\n",
    "\n",
    "    base_params = dict(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"aucpr\",\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        n_estimators=6000,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=6,\n",
    "        min_child_weight=10,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=2.0,\n",
    "        reg_lambda=2.0,\n",
    "        gamma=0.0,\n",
    "        max_bin=256,\n",
    "    )\n",
    "\n",
    "    for tr_idx, va_idx in split_iter:\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "\n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        spw = float(neg / max(1, pos))\n",
    "\n",
    "        model = XGBClassifier(**{**base_params, \"scale_pos_weight\": spw})\n",
    "        xgb_fit_kaggle(model, X_tr, y_tr, verbose=False)\n",
    "\n",
    "        score = model.get_booster().get_score(importance_type=\"gain\")\n",
    "        for feat, g in score.items():\n",
    "            if feat in gains:\n",
    "                gains[feat] += float(g)\n",
    "\n",
    "    ranked = sorted(gains.items(), key=lambda x: x[1], reverse=True)\n",
    "    top = [f for f, _ in ranked[:k]]\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92987b58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.669619Z",
     "iopub.status.busy": "2026-01-25T03:03:50.669105Z",
     "iopub.status.idle": "2026-01-25T03:03:50.680198Z",
     "shell.execute_reply": "2026-01-25T03:03:50.679518Z"
    },
    "papermill": {
     "duration": 0.016691,
     "end_time": "2026-01-25T03:03:50.681555",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.664864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def run_optuna_xgb_f1(train_feat, feature_cols, n_folds_tune=10, timeout_sec=28800, seed=6):\n",
    "    import optuna\n",
    "\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X_all = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "    X = X_all[feature_cols].copy()\n",
    "\n",
    "    splitter = make_splitter(n_folds_tune, random_state=seed)\n",
    "    split_iter_all = list(splitter.split(X, y, groups))\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"aucpr\",\n",
    "            \"random_state\": seed,\n",
    "            \"n_jobs\": -1,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"device\": \"cuda\",\n",
    "\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1500, 14000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.002, 0.08, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 80),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "            \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "            \"max_bin\": trial.suggest_int(\"max_bin\", 128, 512),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 12.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 35.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.05, 50.0),\n",
    "            \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "            \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "        }\n",
    "\n",
    "        if params[\"grow_policy\"] == \"lossguide\":\n",
    "            params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 512)\n",
    "\n",
    "        oof = np.zeros(len(X), dtype=float)\n",
    "        f1_progress = []\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(split_iter_all, 1):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "            neg = np.sum(y_tr == 0)\n",
    "            pos = np.sum(y_tr == 1)\n",
    "            spw = float(neg / max(1, pos))\n",
    "\n",
    "            model = XGBClassifier(**{**params, \"scale_pos_weight\": spw})\n",
    "            xgb_fit_kaggle(model, X_tr, y_tr, X_va, y_va, verbose=False)\n",
    "\n",
    "            oof[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "            th_fold, f1_fold = best_threshold_f1(y_va, oof[va_idx])\n",
    "            f1_progress.append(f1_fold)\n",
    "\n",
    "            trial.report(float(np.mean(f1_progress)), step=fold)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        th, f1 = best_threshold_f1(y, oof)\n",
    "        return float(f1)\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=seed, multivariate=True, group=True)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=40, n_warmup_steps=3)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name=\"xgb_oof_f1_splitcv_gpu_selected\",\n",
    "        storage=\"sqlite:///optuna_xgb_oof_f1_gpu_selected.db\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=999999, timeout=timeout_sec)\n",
    "\n",
    "    print(\"\\nOptuna best OOF F1:\", study.best_value)\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(k, \"=\", v)\n",
    "\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a5a9ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.689544Z",
     "iopub.status.busy": "2026-01-25T03:03:50.689305Z",
     "iopub.status.idle": "2026-01-25T03:03:50.697853Z",
     "shell.execute_reply": "2026-01-25T03:03:50.697170Z"
    },
    "papermill": {
     "duration": 0.014153,
     "end_time": "2026-01-25T03:03:50.699211",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.685058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_xgb_multiseed(train_feat, test_feat, best_params, feature_cols, n_splits_oof=20, seeds=(6, 67, 6767)):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X_all = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "    X_test_all = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"], add_missing_flags=True)\n",
    "\n",
    "    X = X_all[feature_cols].copy()\n",
    "    X_test = X_test_all[feature_cols].copy()\n",
    "\n",
    "    splitter = make_splitter(n_splits_oof, random_state=6)\n",
    "    split_iter = list(splitter.split(X, y, groups))\n",
    "\n",
    "    oof = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        spw = float(neg / max(1, pos))\n",
    "\n",
    "        probs_va = []\n",
    "        for sd in seeds:\n",
    "            model = XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                eval_metric=\"aucpr\",\n",
    "                random_state=sd,\n",
    "                n_jobs=-1,\n",
    "                tree_method=\"hist\",\n",
    "                device=\"cuda\",\n",
    "                scale_pos_weight=spw,\n",
    "                **best_params\n",
    "            )\n",
    "            xgb_fit_kaggle(model, X_tr, y_tr, X_va, y_va, verbose=False)\n",
    "            probs_va.append(model.predict_proba(X_va)[:, 1])\n",
    "\n",
    "        oof[va_idx] = np.mean(probs_va, axis=0)\n",
    "\n",
    "    best_th, best_f1 = best_threshold_f1(y, oof)\n",
    "    ap = average_precision_score(y, oof)\n",
    "    print(\"\\nOOF multiseed best threshold:\", best_th)\n",
    "    print(\"OOF multiseed best F1:\", best_f1)\n",
    "    print(\"OOF AP (aucpr-ish):\", ap)\n",
    "\n",
    "    probs_test = []\n",
    "    neg_full = np.sum(y == 0)\n",
    "    pos_full = np.sum(y == 1)\n",
    "    spw_full = float(neg_full / max(1, pos_full))\n",
    "\n",
    "    for sd in seeds:\n",
    "        model = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"aucpr\",\n",
    "            random_state=sd,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\",\n",
    "            device=\"cuda\",\n",
    "            scale_pos_weight=spw_full,\n",
    "            **best_params\n",
    "        )\n",
    "        xgb_fit_kaggle(model, X, y, verbose=False)\n",
    "        probs_test.append(model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    p_test = np.mean(probs_test, axis=0)\n",
    "    return p_test, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6020b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:03:50.707255Z",
     "iopub.status.busy": "2026-01-25T03:03:50.707038Z",
     "iopub.status.idle": "2026-01-25T03:51:25.406974Z",
     "shell.execute_reply": "2026-01-25T03:51:25.406153Z"
    },
    "papermill": {
     "duration": 2854.7113,
     "end_time": "2026-01-25T03:51:25.414014",
     "exception": false,
     "start_time": "2026-01-25T03:03:50.702714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (9129, 559)\n",
      "test shape: (7135, 558)\n"
     ]
    }
   ],
   "source": [
    "N_AUG = 2\n",
    "FS_TOPK = 380\n",
    "FS_FOLDS = 10\n",
    "OPTUNA_FOLDS = 10\n",
    "OPTUNA_TIMEOUT_SEC = 28800\n",
    "FINAL_OOF_FOLDS = 20\n",
    "SEEDS = (6, 67, 6767)\n",
    "\n",
    "train_log = pd.read_csv(\"/kaggle/input/mallorn-data/data/train_log.csv\")\n",
    "test_log = pd.read_csv(\"/kaggle/input/mallorn-data/data/test_log.csv\")\n",
    "\n",
    "train_log[\"object_id\"] = train_log[\"object_id\"].astype(str)\n",
    "test_log[\"object_id\"] = test_log[\"object_id\"].astype(str)\n",
    "\n",
    "if \"Z_err\" not in train_log.columns:\n",
    "    train_log[\"Z_err\"] = 0.0\n",
    "train_log[\"Z_err\"] = train_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "if \"Z_err\" not in test_log.columns:\n",
    "    test_log[\"Z_err\"] = 0.0\n",
    "test_log[\"Z_err\"] = test_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "train_splits = sorted(train_log[\"split\"].unique())\n",
    "test_splits = sorted(test_log[\"split\"].unique())\n",
    "\n",
    "train_lc_cache, train_idx_cache = build_lightcurve_cache(train_splits, base_dir=\"/kaggle/input/mallorn-data/data\", kind=\"train\")\n",
    "test_lc_cache, test_idx_cache = build_lightcurve_cache(test_splits, base_dir=\"/kaggle/input/mallorn-data/data\", kind=\"test\")\n",
    "\n",
    "test_zerr_pool = test_log[\"Z_err\"].dropna().values\n",
    "\n",
    "train_feat = build_feature_table(\n",
    "    train_log, train_lc_cache, train_idx_cache,\n",
    "    augment_photoz=True,\n",
    "    test_zerr_pool=test_zerr_pool,\n",
    "    n_aug=N_AUG,\n",
    "    seed=6\n",
    ")\n",
    "\n",
    "test_feat = build_feature_table(\n",
    "    test_log, test_lc_cache, test_idx_cache,\n",
    "    augment_photoz=False\n",
    ")\n",
    "print(f\"train shape: {train_feat.shape}\")\n",
    "print(f\"test shape: {test_feat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54825cf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T03:51:25.423196Z",
     "iopub.status.busy": "2026-01-25T03:51:25.422572Z",
     "iopub.status.idle": "2026-01-25T04:24:50.167257Z",
     "shell.execute_reply": "2026-01-25T04:24:50.166383Z"
    },
    "papermill": {
     "duration": 2004.751376,
     "end_time": "2026-01-25T04:24:50.169148",
     "exception": false,
     "start_time": "2026-01-25T03:51:25.417772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_feat, test_feat = add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10, seed=6)\n",
    "selected_cols = feature_select_gain_topk(train_feat, k=FS_TOPK, n_splits=FS_FOLDS, seed=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b909659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T04:24:50.178064Z",
     "iopub.status.busy": "2026-01-25T04:24:50.177782Z",
     "iopub.status.idle": "2026-01-25T12:26:43.743729Z",
     "shell.execute_reply": "2026-01-25T12:26:43.743025Z"
    },
    "papermill": {
     "duration": 28913.574592,
     "end_time": "2026-01-25T12:26:43.747758",
     "exception": false,
     "start_time": "2026-01-25T04:24:50.173166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``group`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2026-01-25 04:24:52,204] A new study created in RDB with name: xgb_oof_f1_splitcv_gpu_selected\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [04:25:35] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "[I 2026-01-25 04:32:12,491] Trial 0 finished with value: 0.574468085106383 and parameters: {'n_estimators': 12661, 'learning_rate': 0.006805837253045404, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.553828339967984, 'colsample_bytree': 0.7570312385237441, 'colsample_bylevel': 0.7649086810964107, 'colsample_bynode': 0.7094037142783272, 'max_bin': 257, 'gamma': 7.470233186353083, 'reg_alpha': 15.334949913810593, 'reg_lambda': 36.80731121211708, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 508}. Best is trial 0 with value: 0.574468085106383.\n",
      "[I 2026-01-25 04:38:36,982] Trial 1 finished with value: 0.5164003364171573 and parameters: {'n_estimators': 11749, 'learning_rate': 0.009183382670861872, 'max_depth': 9, 'min_child_weight': 66, 'subsample': 0.5272372539111774, 'colsample_bytree': 0.8311823419385166, 'colsample_bylevel': 0.9010852812122376, 'colsample_bynode': 0.8682033219057195, 'max_bin': 401, 'gamma': 6.491242251630338, 'reg_alpha': 4.368846103321342, 'reg_lambda': 47.884482408709935, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 510}. Best is trial 0 with value: 0.574468085106383.\n",
      "[I 2026-01-25 04:41:13,113] Trial 2 finished with value: 0.56657223796034 and parameters: {'n_estimators': 4695, 'learning_rate': 0.023796220163228497, 'max_depth': 7, 'min_child_weight': 58, 'subsample': 0.9686747673224523, 'colsample_bytree': 0.6110858611238212, 'colsample_bylevel': 0.626817048735698, 'colsample_bynode': 0.7012362545183866, 'max_bin': 415, 'gamma': 8.688846808940259, 'reg_alpha': 14.213772862043626, 'reg_lambda': 49.46952342158612, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 56}. Best is trial 0 with value: 0.574468085106383.\n",
      "[I 2026-01-25 04:44:29,384] Trial 3 finished with value: 0.5635593220338984 and parameters: {'n_estimators': 6480, 'learning_rate': 0.03435797424993676, 'max_depth': 8, 'min_child_weight': 23, 'subsample': 0.5947710951851903, 'colsample_bytree': 0.682750578841189, 'colsample_bylevel': 0.6674030222513118, 'colsample_bynode': 0.8673675218568255, 'max_bin': 200, 'gamma': 3.973248889453052, 'reg_alpha': 29.54799472878991, 'reg_lambda': 30.770450514402686, 'max_delta_step': 9, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.574468085106383.\n",
      "[I 2026-01-25 04:47:20,935] Trial 4 finished with value: 0.5623242736644799 and parameters: {'n_estimators': 3760, 'learning_rate': 0.019577657634030098, 'max_depth': 6, 'min_child_weight': 33, 'subsample': 0.8709418437745046, 'colsample_bytree': 0.8955599735015102, 'colsample_bylevel': 0.8416149152438208, 'colsample_bynode': 0.5696358027454862, 'max_bin': 398, 'gamma': 0.7060474819696569, 'reg_alpha': 6.743291616292556, 'reg_lambda': 46.24344855951205, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 97}. Best is trial 0 with value: 0.574468085106383.\n",
      "[I 2026-01-25 04:52:26,873] Trial 5 finished with value: 0.5193482688391039 and parameters: {'n_estimators': 9603, 'learning_rate': 0.005059772003391906, 'max_depth': 2, 'min_child_weight': 77, 'subsample': 0.5331102404067158, 'colsample_bytree': 0.7077641116513831, 'colsample_bylevel': 0.6706482956099604, 'colsample_bynode': 0.8300049093226989, 'max_bin': 453, 'gamma': 7.246398680485267, 'reg_alpha': 20.552479184150734, 'reg_lambda': 5.89580012495536, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 34}. Best is trial 0 with value: 0.574468085106383.\n",
      "[I 2026-01-25 04:55:14,590] Trial 6 finished with value: 0.5637860082304527 and parameters: {'n_estimators': 5442, 'learning_rate': 0.04826768637355595, 'max_depth': 7, 'min_child_weight': 44, 'subsample': 0.6839123294670562, 'colsample_bytree': 0.8968940110593683, 'colsample_bylevel': 0.6381752297163195, 'colsample_bynode': 0.562829170833312, 'max_bin': 344, 'gamma': 3.208068064487326, 'reg_alpha': 27.12622249072254, 'reg_lambda': 9.011346959699976, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.574468085106383.\n",
      "[I 2026-01-25 04:59:18,283] Trial 7 finished with value: 0.5797705943691345 and parameters: {'n_estimators': 8516, 'learning_rate': 0.023519426590585892, 'max_depth': 10, 'min_child_weight': 63, 'subsample': 0.8752152357604187, 'colsample_bytree': 0.698230585500732, 'colsample_bylevel': 0.5213662508264388, 'colsample_bynode': 0.9084337482228717, 'max_bin': 133, 'gamma': 6.60049422858126, 'reg_alpha': 19.837886996376792, 'reg_lambda': 5.690270822617963, 'max_delta_step': 0, 'grow_policy': 'depthwise'}. Best is trial 7 with value: 0.5797705943691345.\n",
      "[I 2026-01-25 05:04:00,951] Trial 8 finished with value: 0.5787401574803149 and parameters: {'n_estimators': 5268, 'learning_rate': 0.002316246985137886, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.5058029222076936, 'colsample_bytree': 0.5659999532071188, 'colsample_bylevel': 0.6507727700470807, 'colsample_bynode': 0.5901588157516722, 'max_bin': 153, 'gamma': 9.350807442312323, 'reg_alpha': 14.026196597991065, 'reg_lambda': 12.328451822772307, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 7 with value: 0.5797705943691345.\n",
      "[I 2026-01-25 05:08:32,672] Trial 9 finished with value: 0.5753424657534246 and parameters: {'n_estimators': 9289, 'learning_rate': 0.07243534357295947, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.9893569028132354, 'colsample_bytree': 0.7522446526004877, 'colsample_bylevel': 0.9908406943687503, 'colsample_bynode': 0.984333040157722, 'max_bin': 399, 'gamma': 0.23262970777197278, 'reg_alpha': 21.449063630823463, 'reg_lambda': 12.51548049353916, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 7 with value: 0.5797705943691345.\n",
      "[I 2026-01-25 05:11:52,598] Trial 10 finished with value: 0.5414680648236415 and parameters: {'n_estimators': 6529, 'learning_rate': 0.025299919207811637, 'max_depth': 7, 'min_child_weight': 77, 'subsample': 0.8387333350849718, 'colsample_bytree': 0.7261635243890506, 'colsample_bylevel': 0.69578565787271, 'colsample_bynode': 0.8924767239815179, 'max_bin': 226, 'gamma': 7.571324689095377, 'reg_alpha': 12.658928370182988, 'reg_lambda': 15.851744893731444, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 345}. Best is trial 7 with value: 0.5797705943691345.\n",
      "[I 2026-01-25 05:16:11,317] Trial 11 finished with value: 0.5816023738872403 and parameters: {'n_estimators': 6983, 'learning_rate': 0.0052523059372493915, 'max_depth': 8, 'min_child_weight': 24, 'subsample': 0.5401986843407856, 'colsample_bytree': 0.48047784965785945, 'colsample_bylevel': 0.7069660367235187, 'colsample_bynode': 0.6466061170364089, 'max_bin': 144, 'gamma': 9.463019544690402, 'reg_alpha': 9.580204697655052, 'reg_lambda': 8.706653157720709, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 252}. Best is trial 11 with value: 0.5816023738872403.\n",
      "[I 2026-01-25 05:19:59,250] Trial 12 finished with value: 0.563691073219659 and parameters: {'n_estimators': 6811, 'learning_rate': 0.01320929156806972, 'max_depth': 10, 'min_child_weight': 60, 'subsample': 0.8102029394818048, 'colsample_bytree': 0.43464906463707875, 'colsample_bylevel': 0.6035055037924221, 'colsample_bynode': 0.8883660649273424, 'max_bin': 241, 'gamma': 4.865703991869612, 'reg_alpha': 21.908642309649977, 'reg_lambda': 17.25032715156007, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 11 with value: 0.5816023738872403.\n",
      "[I 2026-01-25 05:25:33,357] Trial 13 finished with value: 0.5793562708102109 and parameters: {'n_estimators': 11982, 'learning_rate': 0.03193474669831253, 'max_depth': 9, 'min_child_weight': 66, 'subsample': 0.9771119077141612, 'colsample_bytree': 0.6604067207150941, 'colsample_bylevel': 0.5157843609498544, 'colsample_bynode': 0.832490154774574, 'max_bin': 181, 'gamma': 9.28436229974032, 'reg_alpha': 15.555300816745623, 'reg_lambda': 10.280701293842249, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 11 with value: 0.5816023738872403.\n",
      "[I 2026-01-25 05:27:38,866] Trial 14 finished with value: 0.5801376597836775 and parameters: {'n_estimators': 2563, 'learning_rate': 0.0067525598550934865, 'max_depth': 8, 'min_child_weight': 63, 'subsample': 0.6039390505086224, 'colsample_bytree': 0.581607208236793, 'colsample_bylevel': 0.7112864776941944, 'colsample_bynode': 0.6505344124537736, 'max_bin': 224, 'gamma': 9.327055199433271, 'reg_alpha': 5.605168374294706, 'reg_lambda': 1.8084370878064373, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 223}. Best is trial 11 with value: 0.5816023738872403.\n",
      "[I 2026-01-25 05:30:26,392] Trial 15 finished with value: 0.580441640378549 and parameters: {'n_estimators': 4145, 'learning_rate': 0.008732256150220776, 'max_depth': 10, 'min_child_weight': 44, 'subsample': 0.7341415855667606, 'colsample_bytree': 0.5006250459710964, 'colsample_bylevel': 0.5988704159067123, 'colsample_bynode': 0.6529164486515708, 'max_bin': 161, 'gamma': 6.804987424581791, 'reg_alpha': 8.178121099465665, 'reg_lambda': 2.9922603521224342, 'max_delta_step': 9, 'grow_policy': 'lossguide', 'max_leaves': 218}. Best is trial 11 with value: 0.5816023738872403.\n",
      "[I 2026-01-25 05:33:41,681] Trial 16 finished with value: 0.5857572718154463 and parameters: {'n_estimators': 5632, 'learning_rate': 0.01105325224576504, 'max_depth': 9, 'min_child_weight': 28, 'subsample': 0.686627461845249, 'colsample_bytree': 0.5015006073817415, 'colsample_bylevel': 0.6043848918707769, 'colsample_bynode': 0.6506011159802616, 'max_bin': 141, 'gamma': 5.732688364273539, 'reg_alpha': 3.644122184260304, 'reg_lambda': 5.811046547918565, 'max_delta_step': 10, 'grow_policy': 'depthwise'}. Best is trial 16 with value: 0.5857572718154463.\n",
      "[I 2026-01-25 05:39:14,748] Trial 17 finished with value: 0.5536205316223648 and parameters: {'n_estimators': 8448, 'learning_rate': 0.0077696813298503585, 'max_depth': 6, 'min_child_weight': 21, 'subsample': 0.5387214613391904, 'colsample_bytree': 0.6174669125750729, 'colsample_bylevel': 0.6556441089229664, 'colsample_bynode': 0.7397840469347047, 'max_bin': 261, 'gamma': 0.7078478705997968, 'reg_alpha': 0.301767751976191, 'reg_lambda': 3.666346180569356, 'max_delta_step': 9, 'grow_policy': 'depthwise'}. Best is trial 16 with value: 0.5857572718154463.\n",
      "[I 2026-01-25 05:44:30,205] Trial 18 finished with value: 0.5878594249201278 and parameters: {'n_estimators': 9014, 'learning_rate': 0.005299543952315362, 'max_depth': 9, 'min_child_weight': 33, 'subsample': 0.7204232208709277, 'colsample_bytree': 0.4263555980703809, 'colsample_bylevel': 0.7654040953611664, 'colsample_bynode': 0.6631682918840058, 'max_bin': 256, 'gamma': 9.637296246470578, 'reg_alpha': 2.579637671409993, 'reg_lambda': 13.508187240485011, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 18 with value: 0.5878594249201278.\n",
      "[I 2026-01-25 05:49:53,041] Trial 19 finished with value: 0.5826271186440678 and parameters: {'n_estimators': 10093, 'learning_rate': 0.006865342506702628, 'max_depth': 10, 'min_child_weight': 52, 'subsample': 0.7917936788321536, 'colsample_bytree': 0.6213405476709658, 'colsample_bylevel': 0.637054798137409, 'colsample_bynode': 0.5099524870315741, 'max_bin': 203, 'gamma': 11.734786485700617, 'reg_alpha': 3.1505042584293603, 'reg_lambda': 4.283085682112251, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 18 with value: 0.5878594249201278.\n",
      "[I 2026-01-25 05:57:03,150] Trial 20 finished with value: 0.5923566878980892 and parameters: {'n_estimators': 9765, 'learning_rate': 0.0021541388148768317, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.6264567320080306, 'colsample_bytree': 0.5101778487782686, 'colsample_bylevel': 0.853282907838, 'colsample_bynode': 0.7365132704377797, 'max_bin': 315, 'gamma': 11.097070365265727, 'reg_alpha': 1.3682342778712528, 'reg_lambda': 13.706997265134445, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 20 with value: 0.5923566878980892.\n",
      "[I 2026-01-25 06:01:59,696] Trial 21 finished with value: 0.5874587458745875 and parameters: {'n_estimators': 8169, 'learning_rate': 0.005401056889031731, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.6734159013449237, 'colsample_bytree': 0.4849082103153013, 'colsample_bylevel': 0.9180311347500444, 'colsample_bynode': 0.670948510340275, 'max_bin': 334, 'gamma': 11.17872696969094, 'reg_alpha': 12.381072280117424, 'reg_lambda': 13.345278666204583, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 20 with value: 0.5923566878980892.\n",
      "[I 2026-01-25 06:07:38,460] Trial 22 finished with value: 0.5929108485499462 and parameters: {'n_estimators': 8836, 'learning_rate': 0.0035405158835514093, 'max_depth': 6, 'min_child_weight': 15, 'subsample': 0.6430097657985706, 'colsample_bytree': 0.466783494162166, 'colsample_bylevel': 0.9519878808750663, 'colsample_bynode': 0.6477225253541153, 'max_bin': 314, 'gamma': 8.785412530212675, 'reg_alpha': 5.564661744719078, 'reg_lambda': 10.07297060893351, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 22 with value: 0.5929108485499462.\n",
      "[I 2026-01-25 06:13:27,596] Trial 23 finished with value: 0.5915789473684211 and parameters: {'n_estimators': 7521, 'learning_rate': 0.003008463265856992, 'max_depth': 6, 'min_child_weight': 19, 'subsample': 0.6912381006120852, 'colsample_bytree': 0.4397243177937413, 'colsample_bylevel': 0.9288968024239111, 'colsample_bynode': 0.7838189479413127, 'max_bin': 313, 'gamma': 4.567532593397079, 'reg_alpha': 4.5203210127458515, 'reg_lambda': 11.565145059028994, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 22 with value: 0.5929108485499462.\n",
      "[I 2026-01-25 06:21:12,354] Trial 24 finished with value: 0.6012269938650306 and parameters: {'n_estimators': 10005, 'learning_rate': 0.002760348607459435, 'max_depth': 6, 'min_child_weight': 13, 'subsample': 0.6240473456623674, 'colsample_bytree': 0.514457813852729, 'colsample_bylevel': 0.9032234332547688, 'colsample_bynode': 0.8708790114035606, 'max_bin': 348, 'gamma': 2.218514918214409, 'reg_alpha': 0.713813759614796, 'reg_lambda': 9.006881708081815, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 24 with value: 0.6012269938650306.\n",
      "[I 2026-01-25 06:27:50,580] Trial 25 finished with value: 0.5907692307692308 and parameters: {'n_estimators': 10869, 'learning_rate': 0.005353947316552503, 'max_depth': 5, 'min_child_weight': 11, 'subsample': 0.5002136264993318, 'colsample_bytree': 0.6804829505505725, 'colsample_bylevel': 0.8473638993145067, 'colsample_bynode': 0.8198361625410144, 'max_bin': 413, 'gamma': 3.169049163501641, 'reg_alpha': 0.09424764506570049, 'reg_lambda': 14.580590236746147, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 24 with value: 0.6012269938650306.\n",
      "[I 2026-01-25 06:35:07,581] Trial 26 finished with value: 0.5888767720828789 and parameters: {'n_estimators': 11303, 'learning_rate': 0.002029711451540984, 'max_depth': 4, 'min_child_weight': 14, 'subsample': 0.7430493257819781, 'colsample_bytree': 0.41397940352410584, 'colsample_bylevel': 0.7978315382542327, 'colsample_bynode': 0.6905803909493208, 'max_bin': 345, 'gamma': 9.431718934054944, 'reg_alpha': 11.542071110988665, 'reg_lambda': 15.888553402281799, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 24 with value: 0.6012269938650306.\n",
      "[I 2026-01-25 06:40:21,057] Trial 27 finished with value: 0.6031413612565445 and parameters: {'n_estimators': 10639, 'learning_rate': 0.006587967733751101, 'max_depth': 3, 'min_child_weight': 16, 'subsample': 0.565704633281855, 'colsample_bytree': 0.5517852427121177, 'colsample_bylevel': 0.8941361789507868, 'colsample_bynode': 0.5975668917512234, 'max_bin': 217, 'gamma': 8.930967217977855, 'reg_alpha': 2.744685574217954, 'reg_lambda': 0.5064732868638249, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 27 with value: 0.6031413612565445.\n",
      "[I 2026-01-25 06:46:49,780] Trial 28 finished with value: 0.5866666666666667 and parameters: {'n_estimators': 12453, 'learning_rate': 0.0052628818506214176, 'max_depth': 3, 'min_child_weight': 21, 'subsample': 0.5869191653595212, 'colsample_bytree': 0.6246579580206646, 'colsample_bylevel': 0.8920360110156091, 'colsample_bynode': 0.5406488271319614, 'max_bin': 270, 'gamma': 4.75985600938011, 'reg_alpha': 1.869047624067754, 'reg_lambda': 6.612007032115424, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 27 with value: 0.6031413612565445.\n",
      "[I 2026-01-25 06:52:51,898] Trial 29 finished with value: 0.5782608695652174 and parameters: {'n_estimators': 12871, 'learning_rate': 0.019575870914292814, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.5641241338487927, 'colsample_bytree': 0.5790978896684312, 'colsample_bylevel': 0.722475063404327, 'colsample_bynode': 0.7248631591835585, 'max_bin': 244, 'gamma': 11.095242436771104, 'reg_alpha': 11.881647860011412, 'reg_lambda': 17.248994143023808, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 27 with value: 0.6031413612565445.\n",
      "[I 2026-01-25 06:57:12,604] Trial 30 finished with value: 0.6065934065934065 and parameters: {'n_estimators': 7490, 'learning_rate': 0.00633093421129592, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.5981083849178067, 'colsample_bytree': 0.5860250693862176, 'colsample_bylevel': 0.9495702856156044, 'colsample_bynode': 0.5016246850474582, 'max_bin': 446, 'gamma': 11.027863838371092, 'reg_alpha': 2.0307007450453667, 'reg_lambda': 0.7691643646530171, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 06:58:58,550] Trial 31 finished with value: 0.5950920245398773 and parameters: {'n_estimators': 3117, 'learning_rate': 0.01481317682568826, 'max_depth': 3, 'min_child_weight': 14, 'subsample': 0.6051652526215623, 'colsample_bytree': 0.6346455260166679, 'colsample_bylevel': 0.8403162792565585, 'colsample_bynode': 0.6062839086349429, 'max_bin': 321, 'gamma': 8.83275091544373, 'reg_alpha': 4.999879234246977, 'reg_lambda': 9.99708781485528, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:02:05,971] Trial 32 finished with value: 0.5950782997762863 and parameters: {'n_estimators': 5769, 'learning_rate': 0.0073178743041254725, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.6672912615472945, 'colsample_bytree': 0.5785128703780446, 'colsample_bylevel': 0.7353677880673141, 'colsample_bynode': 0.6118305905239775, 'max_bin': 361, 'gamma': 10.439180785765874, 'reg_alpha': 2.309859834714763, 'reg_lambda': 9.379898931542742, 'max_delta_step': 0, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:08:15,616] Trial 33 finished with value: 0.6036519871106337 and parameters: {'n_estimators': 13440, 'learning_rate': 0.01816836319533357, 'max_depth': 4, 'min_child_weight': 21, 'subsample': 0.627033729493067, 'colsample_bytree': 0.5017958782858611, 'colsample_bylevel': 0.8051689502929146, 'colsample_bynode': 0.5318495886300734, 'max_bin': 228, 'gamma': 11.541738252143071, 'reg_alpha': 0.8290106131562669, 'reg_lambda': 5.979737049271599, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:13:27,429] Trial 34 finished with value: 0.5824847250509165 and parameters: {'n_estimators': 11353, 'learning_rate': 0.028381429482205695, 'max_depth': 4, 'min_child_weight': 23, 'subsample': 0.7448461183212866, 'colsample_bytree': 0.41828065980645546, 'colsample_bylevel': 0.8509172366545327, 'colsample_bynode': 0.5912937792087853, 'max_bin': 196, 'gamma': 10.113751139899792, 'reg_alpha': 5.035475428375005, 'reg_lambda': 5.817246742499187, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:17:01,730] Trial 35 finished with value: 0.5945399393326593 and parameters: {'n_estimators': 6335, 'learning_rate': 0.006894741174331821, 'max_depth': 4, 'min_child_weight': 28, 'subsample': 0.5782249229617199, 'colsample_bytree': 0.6115002535010465, 'colsample_bylevel': 0.9649917368518711, 'colsample_bynode': 0.5110567025422963, 'max_bin': 468, 'gamma': 10.775722152922892, 'reg_alpha': 1.8443282467941362, 'reg_lambda': 2.186528708087157, 'max_delta_step': 0, 'grow_policy': 'lossguide', 'max_leaves': 396}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:22:46,404] Trial 36 finished with value: 0.6014418125643667 and parameters: {'n_estimators': 9585, 'learning_rate': 0.006388863214097786, 'max_depth': 7, 'min_child_weight': 18, 'subsample': 0.5409687250725677, 'colsample_bytree': 0.6895881221481964, 'colsample_bylevel': 0.9578772707015252, 'colsample_bynode': 0.5059941897414775, 'max_bin': 483, 'gamma': 7.085548903866361, 'reg_alpha': 2.975433102265686, 'reg_lambda': 6.36955867786052, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:30:02,666] Trial 37 finished with value: 0.6 and parameters: {'n_estimators': 12380, 'learning_rate': 0.006344191757868847, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.5511377983398743, 'colsample_bytree': 0.6804803452116013, 'colsample_bylevel': 0.9004613542340292, 'colsample_bynode': 0.5906499568028287, 'max_bin': 509, 'gamma': 6.992705024617413, 'reg_alpha': 5.210458673729052, 'reg_lambda': 12.75741004253604, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:34:57,771] Trial 38 finished with value: 0.5863192182410424 and parameters: {'n_estimators': 9417, 'learning_rate': 0.007220131624862066, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.5508447027552714, 'colsample_bytree': 0.7018621104543864, 'colsample_bylevel': 0.825431576359018, 'colsample_bynode': 0.5037937979037356, 'max_bin': 189, 'gamma': 9.179444562161162, 'reg_alpha': 0.8643271646410658, 'reg_lambda': 16.621101841323537, 'max_delta_step': 10, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:41:29,966] Trial 39 finished with value: 0.5977249224405378 and parameters: {'n_estimators': 10953, 'learning_rate': 0.0026295373539521015, 'max_depth': 4, 'min_child_weight': 14, 'subsample': 0.5364293975636769, 'colsample_bytree': 0.5795587262524139, 'colsample_bylevel': 0.9018935777549975, 'colsample_bynode': 0.5154270254933769, 'max_bin': 151, 'gamma': 9.71563566619102, 'reg_alpha': 8.477691165119092, 'reg_lambda': 7.941724862042258, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 357}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:45:44,415] Trial 40 finished with value: 0.5990521327014218 and parameters: {'n_estimators': 7869, 'learning_rate': 0.013741668458152815, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.6496894356409018, 'colsample_bytree': 0.6329406520603861, 'colsample_bylevel': 0.9758774759868559, 'colsample_bynode': 0.5215093657179111, 'max_bin': 446, 'gamma': 10.490036772961062, 'reg_alpha': 4.502655017074643, 'reg_lambda': 3.2447641132413834, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:49:31,597] Trial 41 finished with value: 0.5894519131334023 and parameters: {'n_estimators': 5442, 'learning_rate': 0.004656267372617399, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.5377364559954951, 'colsample_bytree': 0.6641177728203447, 'colsample_bylevel': 0.976346048371505, 'colsample_bynode': 0.5608758730821671, 'max_bin': 503, 'gamma': 10.051497090572338, 'reg_alpha': 9.535364180590799, 'reg_lambda': 5.7139829096640655, 'max_delta_step': 0, 'grow_policy': 'depthwise'}. Best is trial 30 with value: 0.6065934065934065.\n",
      "[I 2026-01-25 07:53:37,156] Trial 42 pruned. \n",
      "[I 2026-01-25 07:57:56,908] Trial 43 pruned. \n",
      "[I 2026-01-25 08:03:23,752] Trial 44 finished with value: 0.6077235772357723 and parameters: {'n_estimators': 11568, 'learning_rate': 0.009072576145190681, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.7252096449835865, 'colsample_bytree': 0.5596600451573037, 'colsample_bylevel': 0.7560930840296666, 'colsample_bynode': 0.5660057885203896, 'max_bin': 188, 'gamma': 11.911579336432942, 'reg_alpha': 0.9863369139022113, 'reg_lambda': 0.31829206200839444, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 44 with value: 0.6077235772357723.\n",
      "[I 2026-01-25 08:09:50,746] Trial 45 finished with value: 0.5982532751091703 and parameters: {'n_estimators': 12368, 'learning_rate': 0.003117825986807339, 'max_depth': 3, 'min_child_weight': 17, 'subsample': 0.6485827913293477, 'colsample_bytree': 0.5875311001116307, 'colsample_bylevel': 0.6678630804320669, 'colsample_bynode': 0.5980117456907995, 'max_bin': 230, 'gamma': 11.771213279170652, 'reg_alpha': 4.2102882088514, 'reg_lambda': 7.155418657445128, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 44 with value: 0.6077235772357723.\n",
      "[I 2026-01-25 08:15:21,463] Trial 46 finished with value: 0.5861690450054885 and parameters: {'n_estimators': 11211, 'learning_rate': 0.008062021313646335, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.7655053010928706, 'colsample_bytree': 0.6241945279223814, 'colsample_bylevel': 0.8199288839105704, 'colsample_bynode': 0.6149464718170053, 'max_bin': 238, 'gamma': 11.743202684470173, 'reg_alpha': 1.2289164611395504, 'reg_lambda': 19.9135402119551, 'max_delta_step': 1, 'grow_policy': 'lossguide', 'max_leaves': 431}. Best is trial 44 with value: 0.6077235772357723.\n",
      "[I 2026-01-25 08:19:05,759] Trial 47 finished with value: 0.6075949367088608 and parameters: {'n_estimators': 6155, 'learning_rate': 0.004342342642052506, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.5057729302412918, 'colsample_bytree': 0.6501571350690727, 'colsample_bylevel': 0.9231086862024354, 'colsample_bynode': 0.5455728684095925, 'max_bin': 410, 'gamma': 9.486958797603478, 'reg_alpha': 3.6910171927644235, 'reg_lambda': 3.265510666432279, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 44 with value: 0.6077235772357723.\n",
      "[I 2026-01-25 08:23:14,989] Trial 48 pruned. \n",
      "[I 2026-01-25 08:26:16,597] Trial 49 finished with value: 0.5850202429149798 and parameters: {'n_estimators': 3419, 'learning_rate': 0.0030931284659126068, 'max_depth': 5, 'min_child_weight': 17, 'subsample': 0.5255813155022947, 'colsample_bytree': 0.688979565104038, 'colsample_bylevel': 0.8593957935709661, 'colsample_bynode': 0.7195342673614329, 'max_bin': 435, 'gamma': 9.931417846474075, 'reg_alpha': 1.6946418741970164, 'reg_lambda': 19.24989117749796, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 44 with value: 0.6077235772357723.\n",
      "[I 2026-01-25 08:31:04,299] Trial 50 finished with value: 0.5902702702702702 and parameters: {'n_estimators': 7037, 'learning_rate': 0.0027371886207485756, 'max_depth': 4, 'min_child_weight': 16, 'subsample': 0.5638093341260466, 'colsample_bytree': 0.6825889707124206, 'colsample_bylevel': 0.9077319699620326, 'colsample_bynode': 0.6238402250049606, 'max_bin': 361, 'gamma': 8.131772422152471, 'reg_alpha': 10.814486481469407, 'reg_lambda': 4.465195719696571, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 44 with value: 0.6077235772357723.\n",
      "[I 2026-01-25 08:32:37,206] Trial 51 pruned. \n",
      "[I 2026-01-25 08:35:43,741] Trial 52 finished with value: 0.58397365532382 and parameters: {'n_estimators': 6235, 'learning_rate': 0.012878132057757797, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.7228460252129999, 'colsample_bytree': 0.5074632357373154, 'colsample_bylevel': 0.6522995726807208, 'colsample_bynode': 0.5028898074951469, 'max_bin': 145, 'gamma': 10.4737010008945, 'reg_alpha': 7.590779386036558, 'reg_lambda': 8.524873375256112, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 44 with value: 0.6077235772357723.\n",
      "[I 2026-01-25 08:39:34,348] Trial 53 finished with value: 0.5894519131334023 and parameters: {'n_estimators': 7803, 'learning_rate': 0.03177183857781959, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.5053935588188598, 'colsample_bytree': 0.4918093459093624, 'colsample_bylevel': 0.9585682168863001, 'colsample_bynode': 0.5363278138649364, 'max_bin': 466, 'gamma': 11.528341044041825, 'reg_alpha': 0.5515067873617026, 'reg_lambda': 3.0599588076014843, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 44 with value: 0.6077235772357723.\n",
      "[I 2026-01-25 08:43:46,198] Trial 54 finished with value: 0.6078838174273858 and parameters: {'n_estimators': 8505, 'learning_rate': 0.00997589311821524, 'max_depth': 5, 'min_child_weight': 9, 'subsample': 0.6145606177021152, 'colsample_bytree': 0.7603901057717202, 'colsample_bylevel': 0.9552591655063709, 'colsample_bynode': 0.5890605833583373, 'max_bin': 134, 'gamma': 9.73374206696359, 'reg_alpha': 3.0094539000215317, 'reg_lambda': 2.369022646223393, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 54 with value: 0.6078838174273858.\n",
      "[I 2026-01-25 08:47:16,411] Trial 55 finished with value: 0.6 and parameters: {'n_estimators': 7207, 'learning_rate': 0.023855667595016887, 'max_depth': 6, 'min_child_weight': 16, 'subsample': 0.6329931096144303, 'colsample_bytree': 0.8831060487029824, 'colsample_bylevel': 0.9975906833923256, 'colsample_bynode': 0.5752876528125772, 'max_bin': 186, 'gamma': 9.018697299026867, 'reg_alpha': 5.516664881872414, 'reg_lambda': 0.6172786839034108, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 54 with value: 0.6078838174273858.\n",
      "[I 2026-01-25 08:51:29,950] Trial 56 finished with value: 0.5993820803295572 and parameters: {'n_estimators': 8181, 'learning_rate': 0.008348420050449688, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.5659370069830361, 'colsample_bytree': 0.6343368857028259, 'colsample_bylevel': 0.9504926310769564, 'colsample_bynode': 0.5765801733632916, 'max_bin': 193, 'gamma': 9.438073508546355, 'reg_alpha': 1.1933015741569453, 'reg_lambda': 6.4198288448796355, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 54 with value: 0.6078838174273858.\n",
      "[I 2026-01-25 08:54:56,734] Trial 57 finished with value: 0.5944272445820433 and parameters: {'n_estimators': 5538, 'learning_rate': 0.0059565374771587895, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.6386411009767266, 'colsample_bytree': 0.595291434126726, 'colsample_bylevel': 0.9641739572385042, 'colsample_bynode': 0.5620625410835555, 'max_bin': 438, 'gamma': 11.436309609083363, 'reg_alpha': 5.368569039908334, 'reg_lambda': 6.868089566432668, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 54 with value: 0.6078838174273858.\n",
      "[I 2026-01-25 08:59:57,239] Trial 58 finished with value: 0.602711157455683 and parameters: {'n_estimators': 10766, 'learning_rate': 0.01003852240731175, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.513905667681426, 'colsample_bytree': 0.4168797100014535, 'colsample_bylevel': 0.8371379592340326, 'colsample_bynode': 0.6442102438929393, 'max_bin': 201, 'gamma': 10.654998712693244, 'reg_alpha': 4.577089443915506, 'reg_lambda': 1.5011093204550128, 'max_delta_step': 0, 'grow_policy': 'depthwise'}. Best is trial 54 with value: 0.6078838174273858.\n",
      "[I 2026-01-25 09:02:54,712] Trial 59 pruned. \n",
      "[I 2026-01-25 09:08:09,937] Trial 60 pruned. \n",
      "[I 2026-01-25 09:13:43,392] Trial 61 finished with value: 0.6081370449678801 and parameters: {'n_estimators': 11766, 'learning_rate': 0.024381791324748325, 'max_depth': 4, 'min_child_weight': 23, 'subsample': 0.6004305768627155, 'colsample_bytree': 0.594854201978887, 'colsample_bylevel': 0.8241023733283193, 'colsample_bynode': 0.5372150805015081, 'max_bin': 406, 'gamma': 11.596780399929514, 'reg_alpha': 4.092494873246832, 'reg_lambda': 10.592102344523834, 'max_delta_step': 8, 'grow_policy': 'lossguide', 'max_leaves': 16}. Best is trial 61 with value: 0.6081370449678801.\n",
      "[I 2026-01-25 09:20:02,730] Trial 62 finished with value: 0.5967567567567568 and parameters: {'n_estimators': 12937, 'learning_rate': 0.010388536852027948, 'max_depth': 4, 'min_child_weight': 30, 'subsample': 0.6871206902211074, 'colsample_bytree': 0.4624720223766938, 'colsample_bylevel': 0.8496533804838067, 'colsample_bynode': 0.7524226323558676, 'max_bin': 321, 'gamma': 9.12702176751036, 'reg_alpha': 3.159479330978162, 'reg_lambda': 10.959613175479573, 'max_delta_step': 6, 'grow_policy': 'lossguide', 'max_leaves': 46}. Best is trial 61 with value: 0.6081370449678801.\n",
      "[I 2026-01-25 09:21:47,571] Trial 63 pruned. \n",
      "[I 2026-01-25 09:23:29,389] Trial 64 pruned. \n",
      "[I 2026-01-25 09:28:38,933] Trial 65 finished with value: 0.5976267529665588 and parameters: {'n_estimators': 10861, 'learning_rate': 0.012199747746208613, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.69857398231596, 'colsample_bytree': 0.6134464778512854, 'colsample_bylevel': 0.7774834953897224, 'colsample_bynode': 0.5015139739630625, 'max_bin': 226, 'gamma': 11.678609649309383, 'reg_alpha': 2.997380205398263, 'reg_lambda': 1.780614131971598, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 61 with value: 0.6081370449678801.\n",
      "[I 2026-01-25 09:31:26,463] Trial 66 pruned. \n",
      "[I 2026-01-25 09:38:28,483] Trial 67 finished with value: 0.5972660357518401 and parameters: {'n_estimators': 13961, 'learning_rate': 0.01695639257827918, 'max_depth': 7, 'min_child_weight': 15, 'subsample': 0.5846612881949004, 'colsample_bytree': 0.5900872196981094, 'colsample_bylevel': 0.696189340076496, 'colsample_bynode': 0.5744368916678407, 'max_bin': 360, 'gamma': 9.006605860492794, 'reg_alpha': 3.461296076688826, 'reg_lambda': 10.511338939898174, 'max_delta_step': 10, 'grow_policy': 'lossguide', 'max_leaves': 142}. Best is trial 61 with value: 0.6081370449678801.\n",
      "[I 2026-01-25 09:42:05,303] Trial 68 finished with value: 0.5950226244343891 and parameters: {'n_estimators': 7255, 'learning_rate': 0.01578197361719809, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.7203899097335501, 'colsample_bytree': 0.6632720540205161, 'colsample_bylevel': 0.8458873994921599, 'colsample_bynode': 0.5065226260135559, 'max_bin': 325, 'gamma': 11.329914153925777, 'reg_alpha': 9.929551477178231, 'reg_lambda': 3.0585589658920247, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 157}. Best is trial 61 with value: 0.6081370449678801.\n",
      "[I 2026-01-25 09:43:45,275] Trial 69 pruned. \n",
      "[I 2026-01-25 09:45:13,273] Trial 70 pruned. \n",
      "[I 2026-01-25 09:49:55,278] Trial 71 pruned. \n",
      "[I 2026-01-25 09:52:03,156] Trial 72 pruned. \n",
      "[I 2026-01-25 09:54:02,544] Trial 73 pruned. \n",
      "[I 2026-01-25 09:59:29,102] Trial 74 finished with value: 0.6041666666666666 and parameters: {'n_estimators': 10450, 'learning_rate': 0.005009009977189493, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.5451606512480935, 'colsample_bytree': 0.44709448651917194, 'colsample_bylevel': 0.8291179947578928, 'colsample_bynode': 0.6467406902184093, 'max_bin': 140, 'gamma': 8.246457229079667, 'reg_alpha': 0.7167001761512983, 'reg_lambda': 1.844535647814323, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 61 with value: 0.6081370449678801.\n",
      "[I 2026-01-25 10:03:12,481] Trial 75 pruned. \n",
      "[I 2026-01-25 10:05:17,477] Trial 76 pruned. \n",
      "[I 2026-01-25 10:12:04,259] Trial 77 finished with value: 0.6063157894736843 and parameters: {'n_estimators': 13170, 'learning_rate': 0.009118881600792413, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.5603801526686536, 'colsample_bytree': 0.5095944388154141, 'colsample_bylevel': 0.7978630026672148, 'colsample_bynode': 0.7252751389959353, 'max_bin': 181, 'gamma': 4.273019153385212, 'reg_alpha': 5.8684085119143985, 'reg_lambda': 1.4896185816597594, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 61 with value: 0.6081370449678801.\n",
      "[I 2026-01-25 10:14:39,129] Trial 78 pruned. \n",
      "[I 2026-01-25 10:16:17,982] Trial 79 pruned. \n",
      "[I 2026-01-25 10:22:52,173] Trial 80 finished with value: 0.6070686070686071 and parameters: {'n_estimators': 13081, 'learning_rate': 0.012198830606550302, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.5449449630695067, 'colsample_bytree': 0.5689905094148736, 'colsample_bylevel': 0.866115271573599, 'colsample_bynode': 0.7509547377699317, 'max_bin': 174, 'gamma': 3.4686553210308784, 'reg_alpha': 3.1596241663167426, 'reg_lambda': 2.61679575516934, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 61 with value: 0.6081370449678801.\n",
      "[I 2026-01-25 10:25:20,604] Trial 81 pruned. \n",
      "[I 2026-01-25 10:27:07,222] Trial 82 pruned. \n",
      "[I 2026-01-25 10:29:34,723] Trial 83 pruned. \n",
      "[I 2026-01-25 10:35:44,307] Trial 84 finished with value: 0.6093264248704663 and parameters: {'n_estimators': 9476, 'learning_rate': 0.0024306289953670325, 'max_depth': 7, 'min_child_weight': 6, 'subsample': 0.5344962939912224, 'colsample_bytree': 0.464696420753079, 'colsample_bylevel': 0.8146569410634974, 'colsample_bynode': 0.7285475291884695, 'max_bin': 181, 'gamma': 8.476938947246458, 'reg_alpha': 0.44957196104419117, 'reg_lambda': 5.23806334613521, 'max_delta_step': 0, 'grow_policy': 'depthwise'}. Best is trial 84 with value: 0.6093264248704663.\n",
      "[I 2026-01-25 10:40:30,126] Trial 85 pruned. \n",
      "[I 2026-01-25 10:47:28,911] Trial 86 finished with value: 0.5869565217391305 and parameters: {'n_estimators': 12493, 'learning_rate': 0.00345419880511285, 'max_depth': 7, 'min_child_weight': 17, 'subsample': 0.5962836061988059, 'colsample_bytree': 0.44324718889617715, 'colsample_bylevel': 0.8770918991974396, 'colsample_bynode': 0.604995262273879, 'max_bin': 239, 'gamma': 8.895018422311429, 'reg_alpha': 5.437140445491879, 'reg_lambda': 3.336293448606085, 'max_delta_step': 0, 'grow_policy': 'depthwise'}. Best is trial 84 with value: 0.6093264248704663.\n",
      "[I 2026-01-25 10:55:35,826] Trial 87 finished with value: 0.6079484425349087 and parameters: {'n_estimators': 13951, 'learning_rate': 0.00469381796783923, 'max_depth': 9, 'min_child_weight': 3, 'subsample': 0.5281672574887227, 'colsample_bytree': 0.43434899151046946, 'colsample_bylevel': 0.8985402291104492, 'colsample_bynode': 0.7438505683479536, 'max_bin': 192, 'gamma': 3.9161806631018616, 'reg_alpha': 2.890527748693912, 'reg_lambda': 0.12462991469750317, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 84 with value: 0.6093264248704663.\n",
      "[I 2026-01-25 10:58:31,339] Trial 88 pruned. \n",
      "[I 2026-01-25 11:02:09,788] Trial 89 pruned. \n",
      "[I 2026-01-25 11:03:44,674] Trial 90 pruned. \n",
      "[I 2026-01-25 11:10:04,958] Trial 91 finished with value: 0.602308499475341 and parameters: {'n_estimators': 9686, 'learning_rate': 0.002628485309357022, 'max_depth': 6, 'min_child_weight': 10, 'subsample': 0.5217819734565324, 'colsample_bytree': 0.6339006145498884, 'colsample_bylevel': 0.7697323599793412, 'colsample_bynode': 0.7509193176025312, 'max_bin': 240, 'gamma': 7.386856490640001, 'reg_alpha': 2.377953460431679, 'reg_lambda': 5.005381943191943, 'max_delta_step': 0, 'grow_policy': 'lossguide', 'max_leaves': 131}. Best is trial 84 with value: 0.6093264248704663.\n",
      "[I 2026-01-25 11:14:00,928] Trial 92 pruned. \n",
      "[I 2026-01-25 11:17:25,335] Trial 93 pruned. \n",
      "[I 2026-01-25 11:20:21,465] Trial 94 pruned. \n",
      "[I 2026-01-25 11:23:03,377] Trial 95 pruned. \n",
      "[I 2026-01-25 11:25:28,093] Trial 96 pruned. \n",
      "[I 2026-01-25 11:31:45,239] Trial 97 finished with value: 0.5894039735099338 and parameters: {'n_estimators': 9398, 'learning_rate': 0.002364520389798994, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.6085960434536013, 'colsample_bytree': 0.7218351453208283, 'colsample_bylevel': 0.8827565560848615, 'colsample_bynode': 0.5281251893343798, 'max_bin': 423, 'gamma': 10.713499121605452, 'reg_alpha': 4.728306802993971, 'reg_lambda': 6.039115694143343, 'max_delta_step': 0, 'grow_policy': 'depthwise'}. Best is trial 84 with value: 0.6093264248704663.\n",
      "[I 2026-01-25 11:33:48,186] Trial 98 pruned. \n",
      "[I 2026-01-25 11:38:38,701] Trial 99 pruned. \n",
      "[I 2026-01-25 11:41:49,602] Trial 100 pruned. \n",
      "[I 2026-01-25 11:45:18,671] Trial 101 pruned. \n",
      "[I 2026-01-25 11:50:17,108] Trial 102 finished with value: 0.6024844720496895 and parameters: {'n_estimators': 8261, 'learning_rate': 0.003899224795262818, 'max_depth': 5, 'min_child_weight': 16, 'subsample': 0.5913619691502174, 'colsample_bytree': 0.4714577048555015, 'colsample_bylevel': 0.7201497903246035, 'colsample_bynode': 0.8121486954136443, 'max_bin': 211, 'gamma': 8.455620212931642, 'reg_alpha': 3.0699731011339018, 'reg_lambda': 7.157005911764831, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 84 with value: 0.6093264248704663.\n",
      "[I 2026-01-25 11:52:31,139] Trial 103 pruned. \n",
      "[I 2026-01-25 11:55:56,698] Trial 104 pruned. \n",
      "[I 2026-01-25 12:02:48,771] Trial 105 finished with value: 0.6077003121748179 and parameters: {'n_estimators': 11217, 'learning_rate': 0.0036519827649337477, 'max_depth': 7, 'min_child_weight': 11, 'subsample': 0.5037490962784102, 'colsample_bytree': 0.5105758849153665, 'colsample_bylevel': 0.8979411868174284, 'colsample_bynode': 0.7311215263710873, 'max_bin': 170, 'gamma': 4.750990835682348, 'reg_alpha': 1.7373492078152803, 'reg_lambda': 3.14444352169833, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 84 with value: 0.6093264248704663.\n",
      "[I 2026-01-25 12:04:29,861] Trial 106 pruned. \n",
      "[I 2026-01-25 12:11:01,500] Trial 107 pruned. \n",
      "[I 2026-01-25 12:16:29,409] Trial 108 finished with value: 0.6038135593220338 and parameters: {'n_estimators': 10315, 'learning_rate': 0.011277401284861464, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.5216175243071874, 'colsample_bytree': 0.5373534096699686, 'colsample_bylevel': 0.8971531680186499, 'colsample_bynode': 0.8192257610780416, 'max_bin': 138, 'gamma': 1.8852112775321288, 'reg_alpha': 0.4315359686152851, 'reg_lambda': 1.584315038037909, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 84 with value: 0.6093264248704663.\n",
      "[I 2026-01-25 12:20:47,879] Trial 109 finished with value: 0.5826612903225806 and parameters: {'n_estimators': 8865, 'learning_rate': 0.025742541368641146, 'max_depth': 4, 'min_child_weight': 15, 'subsample': 0.8266167855166779, 'colsample_bytree': 0.5169038533684804, 'colsample_bylevel': 0.8565236332709241, 'colsample_bynode': 0.5368260366228574, 'max_bin': 424, 'gamma': 7.706372762489629, 'reg_alpha': 10.388871728047787, 'reg_lambda': 0.8589836109088951, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 84 with value: 0.6093264248704663.\n",
      "[I 2026-01-25 12:26:43,732] Trial 110 finished with value: 0.5913555992141454 and parameters: {'n_estimators': 11957, 'learning_rate': 0.006208925420393657, 'max_depth': 5, 'min_child_weight': 16, 'subsample': 0.5001115456396876, 'colsample_bytree': 0.45890263892746175, 'colsample_bylevel': 0.8221118162949101, 'colsample_bynode': 0.7980410535721514, 'max_bin': 133, 'gamma': 11.45086765113782, 'reg_alpha': 0.756140697564165, 'reg_lambda': 9.95069721862099, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 84 with value: 0.6093264248704663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna best OOF F1: 0.6093264248704663\n",
      "Best params:\n",
      "n_estimators = 9476\n",
      "learning_rate = 0.0024306289953670325\n",
      "max_depth = 7\n",
      "min_child_weight = 6\n",
      "subsample = 0.5344962939912224\n",
      "colsample_bytree = 0.464696420753079\n",
      "colsample_bylevel = 0.8146569410634974\n",
      "colsample_bynode = 0.7285475291884695\n",
      "max_bin = 181\n",
      "gamma = 8.476938947246458\n",
      "reg_alpha = 0.44957196104419117\n",
      "reg_lambda = 5.23806334613521\n",
      "max_delta_step = 0\n",
      "grow_policy = depthwise\n"
     ]
    }
   ],
   "source": [
    "best_params = run_optuna_xgb_f1(\n",
    "    train_feat,\n",
    "    feature_cols=selected_cols,\n",
    "    n_folds_tune=OPTUNA_FOLDS,\n",
    "    timeout_sec=OPTUNA_TIMEOUT_SEC,\n",
    "    seed=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68bd5a28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T12:26:43.765075Z",
     "iopub.status.busy": "2026-01-25T12:26:43.764568Z",
     "iopub.status.idle": "2026-01-25T13:05:00.329628Z",
     "shell.execute_reply": "2026-01-25T13:05:00.328922Z"
    },
    "papermill": {
     "duration": 2296.575856,
     "end_time": "2026-01-25T13:05:00.331551",
     "exception": false,
     "start_time": "2026-01-25T12:26:43.755695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:29] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:30] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:31] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:32] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:33] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:34] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:35] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:36] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:37] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:38] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:39] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:40] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:41] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:42] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:43] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:44] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:45] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:46] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:47] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:48] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:49] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:50] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:51] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:52] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:53] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:54] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:55] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:56] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:57] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:58] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:41:59] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:00] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:01] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:02] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:03] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:04] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:05] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:06] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:07] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:08] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:09] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:10] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:11] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:12] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:13] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:14] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:15] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:16] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:17] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:18] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:19] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:20] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:21] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:22] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:23] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:24] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:25] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:26] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:27] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:28] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:29] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:30] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:31] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:32] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:33] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:34] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:35] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:36] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:37] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:38] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:39] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:40] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:41] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:42] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:43] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:44] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:45] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:46] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:47] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:48] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:49] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:50] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:51] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:52] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:53] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:54] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:55] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:56] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:57] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:58] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:42:59] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:00] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:01] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:02] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:03] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:04] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:05] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:06] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:07] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:08] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:09] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:10] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:11] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:12] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:13] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:14] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:15] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:16] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:43:17] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:59:57] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:59:58] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [12:59:59] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:00] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:01] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:02] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:03] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:04] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:05] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:06] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:07] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:08] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:09] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:10] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:11] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:12] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:13] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:14] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:15] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:16] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:17] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:18] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:19] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:20] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:21] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:22] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:23] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:24] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:25] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:26] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:27] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:28] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:29] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:30] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:31] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:32] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:33] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:34] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:35] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:36] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:37] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:38] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:39] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:40] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:41] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:42] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:43] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:44] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:45] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:46] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:47] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:48] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:49] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:50] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:51] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:52] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:53] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:54] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:55] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:56] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:57] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:58] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:00:59] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:00] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:01] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:02] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:03] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:04] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:05] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:06] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:07] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:08] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:09] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:10] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:11] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:12] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:13] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:14] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:15] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:16] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:17] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:18] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:19] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:20] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:21] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:22] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:23] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:24] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:25] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:26] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:27] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:28] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:29] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:30] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:31] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:32] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:33] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:34] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:35] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:36] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:37] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:38] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:39] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:40] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:41] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:42] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:43] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:266: UserWarning: [13:01:44] WARNING: /workspace/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OOF multiseed best threshold: 0.4755\n",
      "OOF multiseed best F1: 0.5977482088024565\n",
      "OOF AP (aucpr-ish): 0.5200398486484893\n"
     ]
    }
   ],
   "source": [
    "p_test, best_th = predict_xgb_multiseed(\n",
    "    train_feat,\n",
    "    test_feat,\n",
    "    best_params,\n",
    "    feature_cols=selected_cols,\n",
    "    n_splits_oof=min(FINAL_OOF_FOLDS, len(train_splits)),\n",
    "    seeds=SEEDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e90774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T13:05:00.364566Z",
     "iopub.status.busy": "2026-01-25T13:05:00.364306Z",
     "iopub.status.idle": "2026-01-25T13:05:00.382586Z",
     "shell.execute_reply": "2026-01-25T13:05:00.381876Z"
    },
    "papermill": {
     "duration": 0.036406,
     "end_time": "2026-01-25T13:05:00.383933",
     "exception": false,
     "start_time": "2026-01-25T13:05:00.347527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGB_multiseed_teacher_ADV_selected.csv  threshold: 0.4755\n"
     ]
    }
   ],
   "source": [
    "test_pred = (p_test > best_th).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"object_id\": test_feat[\"object_id\"].values,\n",
    "    \"target\": test_pred\n",
    "})\n",
    "out_name = \"XGB_multiseed_teacher_ADV_selected.csv\"\n",
    "sub.to_csv(out_name, index=False)\n",
    "print(\"Saved\", out_name, \" threshold:\", best_th)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9330987,
     "sourceId": 14608100,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36085.758453,
   "end_time": "2026-01-25T13:05:01.318370",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-25T03:03:35.559917",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
