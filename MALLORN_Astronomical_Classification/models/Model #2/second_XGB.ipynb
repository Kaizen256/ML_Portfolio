{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b633a6",
   "metadata": {},
   "source": [
    "# Model 2 (Physics-aware Features + Split-Safe CV XGBoost Ensemble)\n",
    "\n",
    "This model is a major upgrade from Model 1.\n",
    "\n",
    "## What changed vs Model 1\n",
    "### De-extinction correction (EBV)\n",
    "Model 1 used raw flux values directly.\n",
    "Model 2 applies de-extinction using EBV to correct flux measurements for dust obscuration before feature extraction.\n",
    "\n",
    "This is a common astronomy preprocessing step and a notebook created by the host of the competition showed how to use it.\n",
    "\n",
    "### Stronger feature extraction pipeline\n",
    "Model 1 had simple summary stats.\n",
    "Model 2 builds a much richer set of time-series features.\n",
    "\n",
    "### Split-aware cross validation (GroupKFold)\n",
    "Model 1 used a standard `train_test_split`, which can leak patterns across splits.\n",
    "Model 2 uses GroupKFold grouped by `split`, meaning the model must generalize across different split domains.\n",
    "\n",
    "This makes validation more realistic and reduces leakage.\n",
    "\n",
    "### Fold ensemble + OOF thresholding\n",
    "Instead of training one model, Model 2 trains one model per fold and:\n",
    "- collects out-of-fold predictions (OOF)\n",
    "- selects a global best threshold based on OOF\n",
    "- predicts test using the average of fold probabilities\n",
    "\n",
    "### Optuna tuning is now CV-based (not one holdout split)\n",
    "Model 1 tuned on one validation split.\n",
    "Model 2 tunes hyperparameters using grouped CV, so the best params are more stable.\n",
    "\n",
    "## Performance\n",
    "- Public leaderboard F1: 0.5921\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66417ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "FILTERS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "# Wavelengths for each band, provided by the competition and souved from SVO Filter Profile Service\n",
    "EFF_WL_AA = {\n",
    "    \"u\": 3641.0,\n",
    "    \"g\": 4704.0,\n",
    "    \"r\": 6155.0,\n",
    "    \"i\": 7504.0,\n",
    "    \"z\": 8695.0,\n",
    "    \"y\": 10056.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2214b",
   "metadata": {},
   "source": [
    "## De-extinction using EBV (dust correction)\n",
    "\n",
    "In astronomy, observed flux is often reduced by dust between the source and the observer.\n",
    "\n",
    "- EBV represents how much the source is obscured by dust (reddening)\n",
    "- Different filters are affected differently because dust attenuation depends on wavelength\n",
    "\n",
    "This model applies de-extinction before feature extraction, meaning the model uses an estimate of the intrinsic flux rather than the dust-dimmed observed flux.\n",
    "\n",
    "## De-extinction functions\n",
    "\n",
    "These functions correct flux values using the Fitzpatrick (1999) extinction law.\n",
    "\n",
    "### `deextinct_band()`\n",
    "Applies a wavelength-dependent correction factor to flux and flux error for one filter band:\n",
    "- converts EBV into A_V using R_V (default 3.1)\n",
    "- uses the effective wavelength of the band\n",
    "- returns corrected flux, corrected error, and A_lambda\n",
    "\n",
    "### `deextinct_lightcurve()`\n",
    "Applies `deextinct_band` across all filters in the lightcurve so that feature extraction uses corrected flux values.\n",
    "\n",
    "**I am not an astronomer, I used AI to help me implement this correctly.\n",
    "The competition also provided an example notebook showing de-extinction steps.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cebf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extinction import fitzpatrick99\n",
    "\n",
    "def deextinct_band(flux, flux_err, ebv, band, r_v=3.1):\n",
    "    if ebv is None or (isinstance(ebv, float) and np.isnan(ebv)):\n",
    "        return flux, flux_err, 0.0\n",
    "\n",
    "    A_V = float(ebv) * float(r_v)\n",
    "    wave = np.array([EFF_WL_AA[band]], dtype=float)\n",
    "\n",
    "    A_lambda = float(fitzpatrick99(wave, A_V, r_v=r_v, unit=\"aa\")[0])\n",
    "\n",
    "    fac = 10.0 ** (0.4 * A_lambda)\n",
    "    return flux * fac, flux_err * fac, A_lambda\n",
    "\n",
    "\n",
    "def deextinct_lightcurve(lc, ebv):\n",
    "    flux = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    ferr = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    flux_corr = flux.copy()\n",
    "    ferr_corr = ferr.copy()\n",
    "\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        flux_corr[m], ferr_corr[m], _ = deextinct_band(flux_corr[m], ferr_corr[m], ebv, b)\n",
    "\n",
    "    return flux_corr, ferr_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b69d01e",
   "metadata": {},
   "source": [
    "## Utility functions (safe casting + robust statistics)\n",
    "\n",
    "These functions are helper functions used throughout feature extraction:\n",
    "- safe_float: converts values to float and handles missing values\n",
    "- weighted summary functions: mean and std with weights\n",
    "- robust dispersion stats like MAD and IQR\n",
    "- distribution shape metrics like skewness and kurtosis\n",
    "- variability measures like von Neumann eta\n",
    "- slope-based time-series stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbd279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return default\n",
    "        x = float(x)\n",
    "        if np.isnan(x):\n",
    "            return default\n",
    "        return x\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def weighted_mean(x, w):\n",
    "    s = np.sum(w)\n",
    "    if s <= 0:\n",
    "        return np.nan\n",
    "    return float(np.sum(w * x) / s)\n",
    "\n",
    "\n",
    "def weighted_std(x, w):\n",
    "    mu = weighted_mean(x, w)\n",
    "    if np.isnan(mu):\n",
    "        return np.nan\n",
    "    s = np.sum(w)\n",
    "    if s <= 0:\n",
    "        return np.nan\n",
    "    var = np.sum(w * (x - mu) ** 2) / s\n",
    "    return float(np.sqrt(var))\n",
    "\n",
    "\n",
    "def median_abs_dev(x):\n",
    "    med = np.median(x)\n",
    "    return float(np.median(np.abs(x - med)))\n",
    "\n",
    "\n",
    "def iqr(x):\n",
    "    q75, q25 = np.percentile(x, [75, 25])\n",
    "    return float(q75 - q25)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    x = np.asarray(x)\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m3 = np.mean((x - mu) ** 3)\n",
    "    return float(m3 / (s ** 3))\n",
    "\n",
    "\n",
    "def kurtosis_excess(x):\n",
    "    x = np.asarray(x)\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m4 = np.mean((x - mu) ** 4)\n",
    "    return float(m4 / (s ** 4) - 3.0)\n",
    "\n",
    "\n",
    "def von_neumann_eta(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    v = np.var(x)\n",
    "    if v < 1e-12:\n",
    "        return 0.0\n",
    "    dif = np.diff(x)\n",
    "    return float(np.mean(dif ** 2) / v)\n",
    "\n",
    "\n",
    "def fraction_beyond_n_std(x, n=1.5):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < 3:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    return float(np.mean(np.abs(x - mu) > n * s))\n",
    "\n",
    "\n",
    "def max_slope(t, f):\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.max(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def median_abs_slope(t, f):\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.median(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def first_crossing_time(t, f, level, mode):\n",
    "    if len(t) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    if mode == \"rise\":\n",
    "        idx = np.where(f >= level)[0]\n",
    "        if len(idx) == 0:\n",
    "            return np.nan\n",
    "        return float(t[idx[0]])\n",
    "\n",
    "    if mode == \"decay\":\n",
    "        idx = np.where(f <= level)[0]\n",
    "        if len(idx) == 0:\n",
    "            return np.nan\n",
    "        return float(t[idx[0]])\n",
    "\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b9755",
   "metadata": {},
   "source": [
    "## Bazin parametric lightcurve model (curve fitting)\n",
    "\n",
    "This model fits a Bazin-style parametric function to the lightcurve:\n",
    "\n",
    "- captures rise and decay behavior in a compact set of parameters\n",
    "- gives physically meaningful shape descriptors like:\n",
    "  - rise time\n",
    "  - fall time\n",
    "  - peak time\n",
    "  - fit quality (reduced chi^2-like metric)\n",
    "\n",
    "Instead of only summary stats, the model can also learn from fitted shape parameters that describe the transient profile more directly. Another person in a Kaggle discussion said to take note of the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27550a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bazin_model(t, A, t0, tfall, trise, B):\n",
    "    x1 = -(t - t0) / tfall\n",
    "    x2 = -(t - t0) / trise\n",
    "    x1 = np.clip(x1, -60, 60)\n",
    "    x2 = np.clip(x2, -60, 60)\n",
    "    return A * np.exp(x1) / (1.0 + np.exp(x2)) + B\n",
    "\n",
    "\n",
    "\n",
    "def bazin_fit_features(t, f):\n",
    "    out = {\n",
    "        \"bazin_A\": np.nan,\n",
    "        \"bazin_B\": np.nan,\n",
    "        \"bazin_t0\": np.nan,\n",
    "        \"bazin_trise\": np.nan,\n",
    "        \"bazin_tfall\": np.nan,\n",
    "        \"bazin_redchi2\": np.nan,\n",
    "    }\n",
    "\n",
    "    if len(t) < 6:\n",
    "        return out\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    f = np.asarray(f, dtype=float)\n",
    "\n",
    "    B0 = float(np.median(f))\n",
    "    A0 = float(np.max(f) - B0)\n",
    "    t0 = float(t[np.argmax(f)])\n",
    "    tr0 = max(1.0, 0.1 * (t.max() - t.min() + 1e-6))\n",
    "    tf0 = max(5.0, 0.3 * (t.max() - t.min() + 1e-6))\n",
    "\n",
    "    p0 = [A0, t0, tf0, tr0, B0]\n",
    "\n",
    "    lower = [0.0, t.min(), 0.5, 0.5, np.min(f) - abs(A0)]\n",
    "    upper = [10.0 * abs(A0) + 1e-6, t.max(), 5000.0, 5000.0, np.max(f) + abs(A0)]\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(\n",
    "            bazin_model,\n",
    "            t,\n",
    "            f,\n",
    "            p0=p0,\n",
    "            bounds=(lower, upper),\n",
    "            maxfev=20000\n",
    "        )\n",
    "\n",
    "        A, t0, tfall, trise, B = popt\n",
    "        pred = bazin_model(t, *popt)\n",
    "\n",
    "        resid = f - pred\n",
    "        dof = max(1, len(t) - len(popt))\n",
    "        redchi2 = float(np.sum(resid ** 2) / dof)\n",
    "\n",
    "        out[\"bazin_A\"] = float(A)\n",
    "        out[\"bazin_B\"] = float(B)\n",
    "        out[\"bazin_t0\"] = float(t0)\n",
    "        out[\"bazin_trise\"] = float(trise)\n",
    "        out[\"bazin_tfall\"] = float(tfall)\n",
    "        out[\"bazin_redchi2\"] = float(redchi2)\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4640a53",
   "metadata": {},
   "source": [
    "## Feature extraction for a single object (high-level)\n",
    "\n",
    "extract_features_for_object() converts one raw lightcurve into a single feature row.\n",
    "\n",
    "Key preprocessing steps added in this model:\n",
    "- Sort observations in time order\n",
    "- Apply de-extinction correction using EBV\n",
    "- Convert time to:\n",
    "  - observed time (relative)\n",
    "  - rest-frame time using redshift Z\n",
    "\n",
    "## Global (all-filters combined) features\n",
    "\n",
    "These are computed using all observations across all bands for a given object.  \n",
    "They summarize time coverage, brightness distribution, cadence, variability, and context (redshift + dust).\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_obs` | Total number of observations across all filters | Captures overall sampling density and how well-measured the object is |\n",
    "| `total_time_obs` | Observed-frame time baseline: `max(t_rel) - min(t_rel)` | Separates short transients vs long events and measures overall monitoring duration |\n",
    "| `total_time_rest` | Rest-frame time baseline: `total_time_obs / (1+z)` | Removes time dilation so the model compares intrinsic evolution speed across redshifts |\n",
    "\n",
    "### Flux distribution (dust-corrected `flux_corr`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `flux_mean` | Mean corrected flux | Measures average intrinsic brightness level (sensitive to sustained high flux) |\n",
    "| `flux_median` | Median corrected flux | Robust typical brightness baseline (less sensitive to one-off spikes) |\n",
    "| `flux_std` | Standard deviation of corrected flux | Captures variability strength (high = more change over time) |\n",
    "| `flux_min` | Minimum corrected flux | Captures deep fades / dips / negative excursions from noise-subtraction artifacts |\n",
    "| `flux_max` | Maximum corrected flux | Captures peak brightness or flare intensity (key transient signature) |\n",
    "| `flux_range` | Flux range: `flux_max - flux_min` | Simple amplitude proxy for overall brightness swing |\n",
    "| `flux_mad` | Median absolute deviation of corrected flux | Robust variability estimate that doesn’t get bullied by outliers |\n",
    "| `flux_iqr` | Interquartile range of corrected flux | Another robust variability measure (spread of the middle 50%) |\n",
    "| `flux_skew` | Skewness of corrected flux distribution | Detects asymmetric lightcurves (fast rise / slow decay vs vice versa) |\n",
    "| `flux_kurt_excess` | Excess kurtosis of corrected flux distribution | Detects heavy tails/spiky behavior from rare bursts or sharp transients |\n",
    "| `neg_flux_frac` | Fraction of corrected flux values `< 0` | Flags noise-dominated objects or weak detections where measurements hover around zero |\n",
    "\n",
    "### SNR (using corrected errors `err_corr`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `snr_median` | Median SNR where `snr = \\|flux_corr\\| / (err_corr + 1e-8)` | Typical detection quality (separates clean signals from noisy junk) |\n",
    "| `snr_max` | Maximum SNR | Captures the strongest detection event (some transients “light up” briefly) |\n",
    "\n",
    "### Cadence / gaps\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `median_dt` | Median time gap between consecutive observations in `t_rel` | Describes typical cadence (important since sparse sampling hides shape) |\n",
    "| `max_gap` | Maximum time gap between consecutive observations in `t_rel` | Detects missing windows (large gaps can explain unreliable peak/width estimates) |\n",
    "\n",
    "### Time-series variability / shape\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `eta_von_neumann` | Von Neumann eta statistic on `flux_corr` (smoothness vs jumpiness) | Separates smooth evolving curves from noisy jitter or sudden jumps |\n",
    "| `beyond_1p5std` | Fraction of points beyond `1.5 * std` from the center | Measures outlier / burstiness rate (transients often have extreme points) |\n",
    "| `max_slope_global` | Maximum absolute slope in observed time (`t_rel`) | Captures fastest brightness change (sharp rise/fall events) |\n",
    "| `med_abs_slope_global` | Median absolute slope in observed time (`t_rel`) | Captures typical rate of change (slow drifters vs active transients) |\n",
    "\n",
    "### Context metadata\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `Z` | Redshift `z` | Encodes distance/epoch effects and shifts events into different observed regimes |\n",
    "| `log1pZ` | `log(1+z)` | Stabilizes redshift scaling for models (less extreme leverage at high `z`) |\n",
    "| `EBV` | Dust reddening used for extinction correction | Helps the model learn residual dust systematics and measurement conditions |\n",
    "\n",
    "### Filter coverage\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_filters_present` | Number of filters with ≥ 1 observation | Multi-band coverage gives richer color/shape info; missing bands can correlate with class |\n",
    "| `total_obs` | Total observations summed across all filters (same as `n_obs`) | Redundant but convenient sanity/coverage signal that some models exploit |\n",
    "\n",
    "## Per-filter (band-wise) features\n",
    "\n",
    "For each band `b ∈ {u,g,r,i,z,y}`, the following features are computed independently per filter.  \n",
    "These capture color-dependent brightness behavior and band-specific temporal dynamics.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_{b}` | Number of observations in band `b` | Band missingness and sampling density vary by object/class and affect reliability |\n",
    "| `amp_{b}` | Amplitude above baseline: `max(fb) - median(fb)` | Measures event strength in that band (key for color-specific transient signatures) |\n",
    "| `tpeak_{b}_obs` | Observed-frame time of peak flux in band `b` | Captures when the band reaches maximum brightness (timing is class-dependent) |\n",
    "| `tpeak_{b}_rest` | Rest-frame time of peak flux: `tpeak_obs / (1+z)` | Removes time dilation so peak timing is comparable across redshifts |\n",
    "| `width50_{b}_rest` | Rest-frame width above 50% amplitude (if measurable) | Measures event duration at mid-brightness (distinguishes fast vs slow transients) |\n",
    "| `width80_{b}_rest` | Rest-frame width above 80% amplitude (if measurable) | Focuses on the high-brightness core duration (sharp vs broad peaks) |\n",
    "| `auc_pos_{b}_rest` | Rest-frame AUC of positive signal: `∫ max(fb - baseline, 0) dt` | Measures total emitted “excess flux” over baseline (energy-like summary) |\n",
    "| `snrmax_{b}` | Maximum SNR within band `b` | Strongest detection in that band (some classes peak strongly only in certain filters) |\n",
    "| `eta_{b}` | Von Neumann eta within band `b` | Detects smooth evolution vs noise inside a single wavelength band |\n",
    "| `maxslope_{b}` | Maximum slope within band `b` (rest-frame time) | Captures fastest intrinsic change rate per band (rise/decline sharpness) |\n",
    "\n",
    "### Bazin fit features (only if `n_b >= 6`)\n",
    "\n",
    "These are parametric shape features from fitting a **Bazin transient curve model** in each band.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `bazin_A_{b}` | Bazin amplitude-like parameter | Encodes overall transient strength in a smooth, denoised way |\n",
    "| `bazin_B_{b}` | Bazin baseline-like parameter | Captures persistent baseline flux level (helps separate steady sources vs transient-only) |\n",
    "| `bazin_trise_{b}` | Bazin rise timescale | Learns how quickly brightness increases (very class-discriminative) |\n",
    "| `bazin_tfall_{b}` | Bazin fall timescale | Learns decay speed (slow fades vs rapid drop-offs) |\n",
    "| `bazin_redchi2_{b}` | Reduced chi-square of Bazin fit | Quality-of-fit measure: real transients fit well, noisy/non-transient behavior fits poorly |\n",
    "\n",
    "## Cross-band pair features (adjacent pairs: `ug, gr, ri, iz, zy`)\n",
    "\n",
    "For each adjacent filter pair `(a,b)`, these compare amplitude, timing, peak ratios, and energy-like signal across wavelengths.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `ampdiff_{a}{b}` | Amplitude difference: `amp_a - amp_b` | Captures color gradients and temperature evolution signatures between bands |\n",
    "| `tpeakdiff_{a}{b}_rest` | Rest-frame peak time difference: `tpeak_a_rest - tpeak_b_rest` | Measures chromatic peak lag/lead (some classes peak earlier in blue than red) |\n",
    "| `peakratio_{a}{b}` | Peak flux ratio: `peak_flux_a / (peak_flux_b + 1e-8)` | Strong color/SED proxy without needing explicit magnitudes |\n",
    "| `aucdiff_{a}{b}` | Positive-signal AUC difference: `auc_a - auc_b` | Measures which band dominates total emitted signal (helps separate spectral behaviors) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5236893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_object(lc_raw, z, ebv):\n",
    "    feats = {}\n",
    "\n",
    "    # Sort observations by time so time-based calculations make sense\n",
    "    lc = lc_raw.sort_values(\"Time (MJD)\").reset_index(drop=True)\n",
    "\n",
    "    # Extract time values and filter (band) labels\n",
    "    t = lc[\"Time (MJD)\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    # If there are no observations, return minimal info\n",
    "    if len(t) == 0:\n",
    "        feats[\"n_obs\"] = 0\n",
    "        return feats\n",
    "\n",
    "    # Correct brightness values for dust in the Milky Way\n",
    "    # (dust makes objects look dimmer than they really are)\n",
    "    flux_corr, err_corr = deextinct_lightcurve(lc, ebv)\n",
    "\n",
    "    # Make sure redshift is a valid number\n",
    "    z = safe_float(z, default=0.0)\n",
    "\n",
    "    # Convert time to start at 0 (relative time axis)\n",
    "    t_rel = t - t.min()\n",
    "\n",
    "    # Convert observed time to intrinsic time of the object\n",
    "    # Distant objects appear to evolve slower, so divide by (1 + z)\n",
    "    t_rest = t_rel / (1.0 + z)\n",
    "\n",
    "    # Basic observation statistics\n",
    "    feats[\"n_obs\"] = int(len(t))                                  # total number of measurements\n",
    "    feats[\"total_time_obs\"] = float(t_rel.max() - t_rel.min())    # total observed duration\n",
    "    feats[\"total_time_rest\"] = float(t_rest.max() - t_rest.min()) # duration corrected for distance effects\n",
    "\n",
    "    # Global brightness statistics (after dust correction)\n",
    "    feats[\"flux_mean\"] = float(np.mean(flux_corr))\n",
    "    feats[\"flux_median\"] = float(np.median(flux_corr))\n",
    "    feats[\"flux_std\"] = float(np.std(flux_corr))\n",
    "    feats[\"flux_min\"] = float(np.min(flux_corr))\n",
    "    feats[\"flux_max\"] = float(np.max(flux_corr))\n",
    "    feats[\"flux_range\"] = feats[\"flux_max\"] - feats[\"flux_min\"]\n",
    "\n",
    "    # Robust statistics that are less sensitive to outliers\n",
    "    feats[\"flux_mad\"] = median_abs_dev(flux_corr)   # median absolute deviation\n",
    "    feats[\"flux_iqr\"] = iqr(flux_corr)              # interquartile range\n",
    "\n",
    "    # Distribution shape features\n",
    "    feats[\"flux_skew\"] = skewness(flux_corr)                # asymmetry of values\n",
    "    feats[\"flux_kurt_excess\"] = kurtosis_excess(flux_corr)  # tail heaviness / spikiness\n",
    "\n",
    "    # Fraction of measurements that are below zero\n",
    "    # (often indicates noise-dominated detections)\n",
    "    feats[\"neg_flux_frac\"] = float(np.mean(flux_corr < 0))\n",
    "\n",
    "    # Signal-to-noise ratio summaries\n",
    "    snr = np.abs(flux_corr) / (err_corr + 1e-8)\n",
    "    feats[\"snr_median\"] = float(np.median(snr))    # typical signal quality\n",
    "    feats[\"snr_max\"] = float(np.max(snr))          # strongest detection\n",
    "\n",
    "    # Observation timing properties\n",
    "    if len(t) >= 2:\n",
    "        dt = np.diff(t_rel)\n",
    "        feats[\"median_dt\"] = float(np.median(dt))  # typical time between observations\n",
    "        feats[\"max_gap\"] = float(np.max(dt))       # largest observation gap\n",
    "    else:\n",
    "        feats[\"median_dt\"] = np.nan\n",
    "        feats[\"max_gap\"] = np.nan\n",
    "\n",
    "    # Global time-series shape features\n",
    "    feats[\"eta_von_neumann\"] = von_neumann_eta(flux_corr)              # smoothness vs noise\n",
    "    feats[\"beyond_1p5std\"] = fraction_beyond_n_std(flux_corr, n=1.5)   # outlier fraction\n",
    "    feats[\"max_slope_global\"] = max_slope(t_rel, flux_corr)            # fastest brightness change\n",
    "    feats[\"med_abs_slope_global\"] = median_abs_slope(t_rel, flux_corr) # typical change rate\n",
    "\n",
    "    # Metadata features\n",
    "    feats[\"Z\"] = float(z)                           # distance proxy\n",
    "    feats[\"log1pZ\"] = float(np.log1p(max(0.0, z)))  # scaled redshift\n",
    "    feats[\"EBV\"] = safe_float(ebv, default=np.nan)  # dust amount\n",
    "\n",
    "    # Counters for band coverage\n",
    "    feats[\"n_filters_present\"] = 0\n",
    "    feats[\"total_obs\"] = 0\n",
    "\n",
    "    # Storage for cross-band comparison features\n",
    "    band_amp = {}\n",
    "    band_tpeak = {}\n",
    "    band_peak = {}\n",
    "    band_auc = {}\n",
    "\n",
    "    # Loop over each wavelength band (u, g, r, i, z, y)\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        nb = int(np.sum(m))\n",
    "\n",
    "        # Number of observations in this band\n",
    "        feats[f\"n_{b}\"] = nb\n",
    "        feats[\"total_obs\"] += nb\n",
    "\n",
    "        # Initialize band features as missing by default\n",
    "        feats[f\"amp_{b}\"] = np.nan\n",
    "        feats[f\"tpeak_{b}_obs\"] = np.nan\n",
    "        feats[f\"tpeak_{b}_rest\"] = np.nan\n",
    "        feats[f\"width50_{b}_rest\"] = np.nan\n",
    "        feats[f\"width80_{b}_rest\"] = np.nan\n",
    "        feats[f\"auc_pos_{b}_rest\"] = np.nan\n",
    "        feats[f\"snrmax_{b}\"] = np.nan\n",
    "        feats[f\"eta_{b}\"] = np.nan\n",
    "        feats[f\"maxslope_{b}\"] = np.nan\n",
    "\n",
    "        # Parametric light-curve shape features\n",
    "        feats[f\"bazin_A_{b}\"] = np.nan\n",
    "        feats[f\"bazin_B_{b}\"] = np.nan\n",
    "        feats[f\"bazin_trise_{b}\"] = np.nan\n",
    "        feats[f\"bazin_tfall_{b}\"] = np.nan\n",
    "        feats[f\"bazin_redchi2_{b}\"] = np.nan\n",
    "\n",
    "        # Skip bands with no data\n",
    "        if nb == 0:\n",
    "            continue\n",
    "\n",
    "        feats[\"n_filters_present\"] += 1\n",
    "\n",
    "        # Extract time, brightness, and error for this band\n",
    "        tb = t_rel[m]\n",
    "        fb = flux_corr[m]\n",
    "        eb = err_corr[m]\n",
    "\n",
    "        # Sort observations within the band by time\n",
    "        order = np.argsort(tb)\n",
    "        tb = tb[order]\n",
    "        fb = fb[order]\n",
    "        eb = eb[order]\n",
    "\n",
    "        # Convert to intrinsic time scale\n",
    "        tb_rest = tb / (1.0 + z)\n",
    "\n",
    "        # Define baseline brightness and peak brightness\n",
    "        baseline = float(np.median(fb))     # typical level\n",
    "        pidx = int(np.argmax(fb))           # index of brightest point\n",
    "        peak_flux = float(fb[pidx])         # peak brightness\n",
    "        tpeak_obs = float(tb[pidx])         # time of peak (observed)\n",
    "        tpeak_rest = float(tb_rest[pidx])   # time of peak (intrinsic)\n",
    "\n",
    "        # Amplitude of brightening\n",
    "        amp = peak_flux - baseline\n",
    "\n",
    "        # Core band features\n",
    "        feats[f\"amp_{b}\"] = float(amp)\n",
    "        feats[f\"tpeak_{b}_obs\"] = tpeak_obs\n",
    "        feats[f\"tpeak_{b}_rest\"] = tpeak_rest\n",
    "        feats[f\"snrmax_{b}\"] = float(np.max(np.abs(fb) / (eb + 1e-8)))\n",
    "\n",
    "        feats[f\"eta_{b}\"] = von_neumann_eta(fb)         # smoothness\n",
    "        feats[f\"maxslope_{b}\"] = max_slope(tb_rest, fb) # fastest change rate\n",
    "\n",
    "        # Total positive signal above baseline (area under curve)\n",
    "        if nb >= 2:\n",
    "            feats[f\"auc_pos_{b}_rest\"] = float(\n",
    "                np.trapezoid(np.maximum(fb - baseline, 0.0), tb_rest)\n",
    "            )\n",
    "\n",
    "        # Width of the brightening at 50% and 80% of peak\n",
    "        if (amp > 0) and (nb >= 3):\n",
    "            lvl50 = baseline + 0.50 * amp\n",
    "            lvl80 = baseline + 0.80 * amp\n",
    "\n",
    "            # Rising side\n",
    "            trise_seg_t = tb_rest[:pidx + 1]\n",
    "            trise_seg_f = fb[:pidx + 1]\n",
    "\n",
    "            # Falling side\n",
    "            tdec_seg_t = tb_rest[pidx:]\n",
    "            tdec_seg_f = fb[pidx:]\n",
    "\n",
    "            # Width at 50%\n",
    "            t_rise50 = first_crossing_time(trise_seg_t, trise_seg_f, lvl50, \"rise\")\n",
    "            t_fall50 = first_crossing_time(tdec_seg_t, tdec_seg_f, lvl50, \"decay\")\n",
    "            if (not np.isnan(t_rise50)) and (not np.isnan(t_fall50)):\n",
    "                feats[f\"width50_{b}_rest\"] = float(t_fall50 - t_rise50)\n",
    "\n",
    "            # Width at 80%\n",
    "            t_rise80 = first_crossing_time(trise_seg_t, trise_seg_f, lvl80, \"rise\")\n",
    "            t_fall80 = first_crossing_time(tdec_seg_t, tdec_seg_f, lvl80, \"decay\")\n",
    "            if (not np.isnan(t_rise80)) and (not np.isnan(t_fall80)):\n",
    "                feats[f\"width80_{b}_rest\"] = float(t_fall80 - t_rise80)\n",
    "\n",
    "        # Fit a smooth transient curve model if enough points exist\n",
    "        if nb >= 6:\n",
    "            bf = bazin_fit_features(tb_rest, fb)\n",
    "            feats[f\"bazin_A_{b}\"] = bf[\"bazin_A\"]\n",
    "            feats[f\"bazin_B_{b}\"] = bf[\"bazin_B\"]\n",
    "            feats[f\"bazin_trise_{b}\"] = bf[\"bazin_trise\"]\n",
    "            feats[f\"bazin_tfall_{b}\"] = bf[\"bazin_tfall\"]\n",
    "            feats[f\"bazin_redchi2_{b}\"] = bf[\"bazin_redchi2\"]\n",
    "\n",
    "        # Store values for cross-band comparisons\n",
    "        band_amp[b] = feats[f\"amp_{b}\"]\n",
    "        band_tpeak[b] = feats[f\"tpeak_{b}_rest\"]\n",
    "        band_peak[b] = peak_flux\n",
    "        band_auc[b] = feats[f\"auc_pos_{b}_rest\"]\n",
    "\n",
    "    # Cross-band comparison features between adjacent wavelengths\n",
    "    pairs = [(\"u\", \"g\"), (\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\"), (\"z\", \"y\")]\n",
    "    for a, b in pairs:\n",
    "        va, vb = band_amp.get(a, np.nan), band_amp.get(b, np.nan)\n",
    "        ta, tb_ = band_tpeak.get(a, np.nan), band_tpeak.get(b, np.nan)\n",
    "\n",
    "        # Difference in brightness amplitude\n",
    "        feats[f\"ampdiff_{a}{b}\"] = (va - vb) if (not np.isnan(va) and not np.isnan(vb)) else np.nan\n",
    "\n",
    "        # Difference in peak timing (intrinsic frame)\n",
    "        feats[f\"tpeakdiff_{a}{b}_rest\"] = (ta - tb_) if (not np.isnan(ta) and not np.isnan(tb_)) else np.nan\n",
    "\n",
    "        # Ratio of peak brightness values\n",
    "        pa, pb = band_peak.get(a, np.nan), band_peak.get(b, np.nan)\n",
    "        feats[f\"peakratio_{a}{b}\"] = (pa / (pb + 1e-8)) if (not np.isnan(pa) and not np.isnan(pb)) else np.nan\n",
    "\n",
    "        # Difference in total positive signal\n",
    "        aa, ab = band_auc.get(a, np.nan), band_auc.get(b, np.nan)\n",
    "        feats[f\"aucdiff_{a}{b}\"] = (aa - ab) if (not np.isnan(aa) and not np.isnan(ab)) else np.nan\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c55c3",
   "metadata": {},
   "source": [
    "## Lightcurve Cahce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e44871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lightcurve_cache(splits, base_dir=\"data\", kind=\"train\"):\n",
    "    lc_cache = {}\n",
    "    idx_cache = {}\n",
    "\n",
    "    for s in splits:\n",
    "        path = f\"{base_dir}/{s}/{kind}_full_lightcurves.csv\"\n",
    "        lc = pd.read_csv(path)\n",
    "        groups = lc.groupby(\"object_id\").indices\n",
    "        lc_cache[s] = lc\n",
    "        idx_cache[s] = groups\n",
    "\n",
    "    return lc_cache, idx_cache\n",
    "\n",
    "\n",
    "def get_lightcurve(lc_cache, idx_cache, split, object_id):\n",
    "    idx = idx_cache[split].get(object_id, None)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    return lc_cache[split].iloc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b62a1",
   "metadata": {},
   "source": [
    "## Feature Table\n",
    "Iterates through the log dataframe and fetches each object's raw lightcurve from cache, extracts a feature dictionary using `extract_features_for_object` and assembles all rows into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_table(log_df, lc_cache, idx_cache, drop_cols=None):\n",
    "    rows = []\n",
    "    for i in range(len(log_df)):\n",
    "        r = log_df.iloc[i]\n",
    "        obj = r[\"object_id\"]\n",
    "        split = r[\"split\"]\n",
    "\n",
    "        lc = get_lightcurve(lc_cache, idx_cache, split, obj)\n",
    "        if lc is None:\n",
    "            feats = {\"n_obs\": 0}\n",
    "        else:\n",
    "            feats = extract_features_for_object(\n",
    "                lc_raw=lc,\n",
    "                z=r[\"Z\"],\n",
    "                ebv=r[\"EBV\"]\n",
    "            )\n",
    "\n",
    "        feats[\"object_id\"] = obj\n",
    "        feats[\"split\"] = split\n",
    "        if \"target\" in log_df.columns:\n",
    "            feats[\"target\"] = int(r[\"target\"])\n",
    "\n",
    "        rows.append(feats)\n",
    "\n",
    "    feat_df = pd.DataFrame(rows)\n",
    "\n",
    "    if drop_cols is not None:\n",
    "        for c in drop_cols:\n",
    "            if c in feat_df.columns:\n",
    "                feat_df.drop(columns=[c], inplace=True)\n",
    "\n",
    "    return feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b459a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_f1_threshold(y_true, prob):\n",
    "    ths = np.linspace(0.01, 0.99, 199)\n",
    "    f1s = [f1_score(y_true, prob > t) for t in ths]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return float(ths[j]), float(f1s[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d6c741",
   "metadata": {},
   "source": [
    "## Split-aware CV training + fold ensemble\n",
    "\n",
    "Instead of a random train/val split (Model 1), this model trains using GroupKFold. This forces the model to generalize across split domains.\n",
    "\n",
    "For each fold:\n",
    "- compute fold-specific `scale_pos_weight` instead of globally computing for all folds\n",
    "- train an XGBClassifier with early stopping\n",
    "- store validation probabilities as OOF predictions\n",
    "- select a fold-local best threshold for reporting\n",
    "\n",
    "After all folds finish:\n",
    "- compute a global best threshold using all OOF predictions\n",
    "\n",
    "Outputs:\n",
    "- list of trained models (one per fold)\n",
    "- global best threshold\n",
    "- OOF probability predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv_ensemble(train_feat_df, base_params=None, n_splits=20, seed=6):\n",
    "    y = train_feat_df[\"target\"].to_numpy().astype(int)\n",
    "    groups = train_feat_df[\"split\"].to_numpy()\n",
    "\n",
    "    X = train_feat_df.drop(columns=[\"object_id\", \"split\", \"target\"])\n",
    "\n",
    "    oof = np.zeros(len(train_feat_df), dtype=float)\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    models = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        spw = float(neg / max(1, pos))\n",
    "\n",
    "        params = dict(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            n_estimators=5000,\n",
    "            learning_rate=0.02,\n",
    "            max_depth=5,\n",
    "            min_child_weight=10,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            gamma=0.5,\n",
    "            reg_alpha=5.0,\n",
    "            reg_lambda=2.0,\n",
    "            scale_pos_weight=spw,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        if base_params is not None:\n",
    "            params.update(base_params)\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=200\n",
    "        )\n",
    "\n",
    "        p_va = model.predict_proba(X_va)[:, 1]\n",
    "        oof[va_idx] = p_va\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "        th, f1 = best_f1_threshold(y_va, p_va)\n",
    "        print(f\"Fold {fold:02d} | val best F1={f1:.4f} @ th={th:.3f} | best_iter={model.best_iteration}\")\n",
    "\n",
    "    th_global, f1_global = best_f1_threshold(y, oof)\n",
    "    print(\"\\nOOF best F1:\", f1_global, \" @ th=\", th_global)\n",
    "\n",
    "    return models, th_global, oof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c59b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = pd.read_csv(\"data/train_log.csv\")\n",
    "\n",
    "for c in [\"English Translation\", \"SpecType\"]:\n",
    "    if c in train_log.columns:\n",
    "        train_log.drop(columns=[c], inplace=True)\n",
    "\n",
    "splits = sorted(train_log[\"split\"].unique())\n",
    "\n",
    "train_lc_cache, train_idx_cache = build_lightcurve_cache(splits, base_dir=\"data\", kind=\"train\")\n",
    "train_feat = build_feature_table(train_log, train_lc_cache, train_idx_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9916dfd",
   "metadata": {},
   "source": [
    "## Optuna hyperparameter tuning using split-aware CV\n",
    "\n",
    "This model tunes hyperparameters using GroupKFold grouped by split\n",
    "\n",
    "### What the tuning objective does\n",
    "For each trial:\n",
    "- sample a set of XGBoost hyperparameters\n",
    "- train across multiple GroupKFold folds\n",
    "- predict probabilities for each validation fold\n",
    "- choose the best F1 threshold per fold\n",
    "- return the mean fold F1 as the Optuna score\n",
    "\n",
    "This produces hyperparameters that generalize better across split domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51739334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "groups = train_feat[\"split\"].to_numpy()\n",
    "X = train_feat.drop(columns=[\"object_id\", \"split\", \"target\"])\n",
    "\n",
    "def best_threshold_f1(y_true, probs):\n",
    "    ths = np.linspace(0.03, 0.97, 80)\n",
    "    f1s = [f1_score(y_true, probs > t, zero_division=0) for t in ths]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return float(ths[j]), float(f1s[j])\n",
    "\n",
    "unique_groups = np.unique(groups)\n",
    "N_FOLDS_TUNE = min(10, len(unique_groups))\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 6,\n",
    "        \"n_jobs\": -1,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 5000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.15, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 40),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 20.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.05, 30.0),\n",
    "        \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "    }\n",
    "\n",
    "    gkf = GroupKFold(n_splits=N_FOLDS_TUNE)\n",
    "    fold_f1s = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups), 1):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        params[\"scale_pos_weight\"] = float(neg / max(1, pos))\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "        probs = model.predict_proba(X_va)[:, 1]\n",
    "        _, f1 = best_threshold_f1(y_va, probs)\n",
    "        fold_f1s.append(f1)\n",
    "\n",
    "        trial.report(float(np.mean(fold_f1s)), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return float(np.mean(fold_f1s))\n",
    "\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(\n",
    "    seed=6,\n",
    "    multivariate=True,\n",
    "    group=True\n",
    ")\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(\n",
    "    n_startup_trials=30,\n",
    "    n_warmup_steps=3\n",
    ")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    "    study_name=\"xgb_split_cv_gpu\",\n",
    "    storage=\"sqlite:///optuna_xgb_gpu.db\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=2000, timeout=12*60*60)\n",
    "\n",
    "print(\"\\nBest F1:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd363a",
   "metadata": {},
   "source": [
    "Best F1: **0.5586**\n",
    "\n",
    "Best Parameters:\n",
    "```json\n",
    "{\n",
    "  \"n_estimators\": 3560,\n",
    "  \"learning_rate\": 0.0240,\n",
    "  \"max_depth\": 5,\n",
    "  \"min_child_weight\": 11,\n",
    "  \"subsample\": 0.5332,\n",
    "  \"colsample_bytree\": 0.5563,\n",
    "  \"gamma\": 0.7024,\n",
    "  \"reg_alpha\": 5.8620,\n",
    "  \"reg_lambda\": 9.4988,\n",
    "  \"max_delta_step\": 9\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508521ad",
   "metadata": {},
   "source": [
    "## Final training using the best CV hyperparameters\n",
    "\n",
    "After Optuna tuning:\n",
    "- retrain using GroupKFold across all split groups\n",
    "- store out-of-fold probabilities for every training sample\n",
    "- select a global F1-max threshold using OOF predictions\n",
    "\n",
    "This provides:\n",
    "- a fold-ensemble of models for inference\n",
    "- a threshold optimized on realistic split-aware validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad6b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 | F1=0.6667 @ th=0.220\n",
      "Fold 02 | F1=0.5455 @ th=0.375\n",
      "Fold 03 | F1=0.3529 @ th=0.125\n",
      "Fold 04 | F1=0.8571 @ th=0.589\n",
      "Fold 05 | F1=0.7692 @ th=0.470\n",
      "Fold 06 | F1=0.2727 @ th=0.030\n",
      "Fold 07 | F1=0.7500 @ th=0.232\n",
      "Fold 08 | F1=0.5333 @ th=0.208\n",
      "Fold 09 | F1=0.6000 @ th=0.554\n",
      "Fold 10 | F1=0.6486 @ th=0.220\n",
      "Fold 11 | F1=0.5116 @ th=0.113\n",
      "Fold 12 | F1=0.5714 @ th=0.078\n",
      "Fold 13 | F1=0.0000 @ th=0.030\n",
      "Fold 14 | F1=0.0000 @ th=0.030\n",
      "Fold 15 | F1=0.7143 @ th=0.435\n",
      "Fold 16 | F1=0.7273 @ th=0.530\n",
      "Fold 17 | F1=0.8333 @ th=0.803\n",
      "Fold 18 | F1=0.5385 @ th=0.304\n",
      "Fold 19 | F1=0.5714 @ th=0.161\n",
      "Fold 20 | F1=0.7500 @ th=0.518\n",
      "\n",
      "Best F1: 0.5102040816326531\n",
      "Best threshold: 0.4583544303797469\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "final_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": 6,\n",
    "    \"n_jobs\": -1,\n",
    "    **best_params\n",
    "}\n",
    "\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "\n",
    "models = []\n",
    "oof_probs = np.zeros(len(X), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups), 1):\n",
    "    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "    neg = np.sum(y_tr == 0)\n",
    "    pos = np.sum(y_tr == 1)\n",
    "    final_params[\"scale_pos_weight\"] = float(neg / max(1, pos))\n",
    "\n",
    "    model = XGBClassifier(**final_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    probs = model.predict_proba(X_va)[:, 1]\n",
    "    oof_probs[va_idx] = probs\n",
    "    models.append(model)\n",
    "\n",
    "    th, f1 = best_threshold_f1(y_va, probs)\n",
    "    print(f\"Fold {fold:02d} | F1={f1:.4f} @ th={th:.3f}\")\n",
    "\n",
    "best_t, best_f1 = best_threshold_f1(y, oof_probs)\n",
    "print(\"\\nBest F1:\", best_f1)\n",
    "print(\"Best threshold:\", best_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log = pd.read_csv(\"data/test_log.csv\")\n",
    "\n",
    "for c in [\"English Translation\", \"SpecType\", \"target\"]:\n",
    "    if c in test_log.columns:\n",
    "        test_log.drop(columns=[c], inplace=True)\n",
    "\n",
    "test_splits = sorted(test_log[\"split\"].unique())\n",
    "\n",
    "test_lc_cache, test_idx_cache = build_lightcurve_cache(\n",
    "    test_splits,\n",
    "    base_dir=\"data\",\n",
    "    kind=\"test\"\n",
    ")\n",
    "\n",
    "test_feat = build_feature_table(test_log, test_lc_cache, test_idx_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_feat.drop(columns=[\"object_id\", \"split\"])\n",
    "\n",
    "test_probs = np.mean([m.predict_proba(X_test)[:, 1] for m in models], axis=0)\n",
    "test_pred = (test_probs > best_t).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"object_id\": test_feat[\"object_id\"].values,\n",
    "    \"target\": test_pred\n",
    "})\n",
    "sub.to_csv(\"Submissions/second_XGB.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
