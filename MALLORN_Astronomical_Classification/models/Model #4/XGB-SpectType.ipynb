{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132b323a",
   "metadata": {},
   "source": [
    "# Model 4: XGB SpecType\n",
    "\n",
    "This notebook contains Model 4 for the MALLORN challenge.\n",
    "\n",
    "This model is the first one that performed very well. It was enough to put me in around 130th / 925 participants.\n",
    "The biggest change is using SpecType (train-only metadata) to generate features that can also be computed for the test set.\n",
    "\n",
    "A Kaggle user in a discussion post pointed out focusing on TDE vs SN/AGN which are values in SpecType. Since SpecType is not available at test time, I train a separate model to predict SpecType, and then use its predicted probabilities as additional features in the main TDE classifier.\n",
    "\n",
    "1) Train a multiclass LightGBM model to predict `SpecTypeGroup`:\n",
    "   - TDE\n",
    "   - AGN\n",
    "   - SNIa\n",
    "   - SNother\n",
    "   - Other\n",
    "\n",
    "2) Generate OOF predicted probabilities for the train set:\n",
    "   - Each training object only gets probabilities from a model that did not train on its group-split fold.\n",
    "\n",
    "3) Fit the multiclass model on full train and predict probabilities for test.\n",
    "\n",
    "4) Append these probabilities as features:\n",
    "   - `p_spec_<class>` for each class\n",
    "   - `spec_entropy` as a confidence / ambiguity signal\n",
    "\n",
    "This gives the main classifier extra information about “what kind of transient this looks like” using only lightcurve-derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43252d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "from extinction import fitzpatrick99\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2434cb",
   "metadata": {},
   "source": [
    "## Constants / Configuration\n",
    "\n",
    "- `PRE_BASE_FRAC`: fraction of early-time points used to estimate baseline before peak\n",
    "- `MIN_BAND_POINTS`: minimum points needed for certain per-band features\n",
    "- `PEAK_SIGMA_K`: how strict a “significant peak” must be relative to noise\n",
    "- `REBRIGHT_FRAC`: what fraction of amplitude counts as rebrightening\n",
    "- `EPS`: numerical stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15183be",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "EFF_WL_AA = {\n",
    "    \"u\": 3641.0,\n",
    "    \"g\": 4704.0,\n",
    "    \"r\": 6155.0,\n",
    "    \"i\": 7504.0,\n",
    "    \"z\": 8695.0,\n",
    "    \"y\": 10056.0,\n",
    "}\n",
    "\n",
    "R_V = 3.1\n",
    "\n",
    "PRE_BASE_FRAC = 0.20\n",
    "MIN_BAND_POINTS = 5\n",
    "PEAK_SIGMA_K = 3.0\n",
    "REBRIGHT_FRAC = 0.30\n",
    "EPS = 1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return default\n",
    "        x = float(x)\n",
    "        if np.isnan(x):\n",
    "            return default\n",
    "        return x\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def trapz_safe(y, x):\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return float(np.trapezoid(y, x))\n",
    "    y = np.asarray(y)\n",
    "    x = np.asarray(x)\n",
    "    return float(np.sum((x[1:] - x[:-1]) * (y[1:] + y[:-1]) * 0.5))\n",
    "\n",
    "\n",
    "def median_abs_dev(x):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    med = np.median(x)\n",
    "    return float(np.median(np.abs(x - med)))\n",
    "\n",
    "\n",
    "def iqr(x):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    q75, q25 = np.percentile(x, [75, 25])\n",
    "    return float(q75 - q25)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m3 = np.mean((x - mu) ** 3)\n",
    "    return float(m3 / (s ** 3))\n",
    "\n",
    "\n",
    "def kurtosis_excess(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m4 = np.mean((x - mu) ** 4)\n",
    "    return float(m4 / (s ** 4) - 3.0)\n",
    "\n",
    "\n",
    "def von_neumann_eta(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    v = np.var(x)\n",
    "    if v < 1e-12:\n",
    "        return 0.0\n",
    "    dif = np.diff(x)\n",
    "    return float(np.mean(dif ** 2) / v)\n",
    "\n",
    "\n",
    "def max_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.max(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def median_abs_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.median(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def linear_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    try:\n",
    "        a, b = np.polyfit(t, f, 1)\n",
    "        return float(a)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def chi2_to_constant(f, ferr):\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.median(f)\n",
    "    denom = (ferr + EPS) ** 2\n",
    "    chi2 = np.sum((f - mu) ** 2 / denom)\n",
    "    dof = max(1, n - 1)\n",
    "    return float(chi2 / dof)\n",
    "\n",
    "\n",
    "def interp_flux_at_time(tb, fb, t0):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan\n",
    "    if (t0 < tb.min()) or (t0 > tb.max()):\n",
    "        return np.nan\n",
    "    return float(np.interp(t0, tb, fb))\n",
    "\n",
    "\n",
    "def fractional_variability(f, ferr):\n",
    "    \"\"\"\n",
    "    Noise-corrected intrinsic variability:\n",
    "    F_var = sqrt(max(0, S^2 - mean(err^2))) / |mean(f)|\n",
    "    \"\"\"\n",
    "    f = np.asarray(f, float)\n",
    "    ferr = np.asarray(ferr, float)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "\n",
    "    mu = np.mean(f)\n",
    "    if np.abs(mu) < 1e-8:\n",
    "        return np.nan\n",
    "\n",
    "    s2 = np.var(f, ddof=1)\n",
    "    mean_err2 = np.mean(ferr**2)\n",
    "    excess = max(0.0, s2 - mean_err2)\n",
    "    return float(np.sqrt(excess) / np.abs(mu))\n",
    "\n",
    "def stetson_J_consecutive(t, f, ferr):\n",
    "    \"\"\"\n",
    "    Stetson J using consecutive pairs (always exists if n>=4).\n",
    "    \"\"\"\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(t)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "\n",
    "    mu = np.mean(f)\n",
    "    scale = np.sqrt(n / max(1, n - 1))\n",
    "    delta = scale * (f - mu) / (ferr + EPS)\n",
    "\n",
    "    vals = []\n",
    "    for i in range(n - 1):\n",
    "        P = delta[i] * delta[i + 1]\n",
    "        vals.append(np.sign(P) * np.sqrt(np.abs(P)))\n",
    "\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "def pre_peak_baseline(tb, fb, eb, frac=PRE_BASE_FRAC):\n",
    "    \"\"\"\n",
    "    baseline from earliest fraction of times (robust).\n",
    "    \"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(tb)\n",
    "    if n < 3:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    k = max(2, int(np.ceil(frac * n)))\n",
    "    k = min(k, n)\n",
    "\n",
    "    base = float(np.median(fb[:k]))\n",
    "    mad_pre = median_abs_dev(fb[:k])\n",
    "    mederr_pre = float(np.median(eb[:k])) if k > 0 else np.nan\n",
    "    return base, mad_pre, mederr_pre\n",
    "\n",
    "\n",
    "def count_significant_peaks(tb, fb, eb, baseline_pre, k_sigma=PEAK_SIGMA_K):\n",
    "    \"\"\"\n",
    "    Simple local-maximum peak count above baseline_pre + k_sigma * median_err_pre.\n",
    "    \"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(fb)\n",
    "    if n < 5:\n",
    "        return 0\n",
    "\n",
    "    mederr = float(np.median(eb)) if np.isfinite(np.median(eb)) else 0.0\n",
    "    thresh = baseline_pre + k_sigma * mederr\n",
    "\n",
    "    peaks = 0\n",
    "    for i in range(1, n - 1):\n",
    "        if (fb[i] > fb[i - 1]) and (fb[i] > fb[i + 1]) and (fb[i] > thresh):\n",
    "            peaks += 1\n",
    "    return int(peaks)\n",
    "\n",
    "\n",
    "def postpeak_monotonicity(tb, fb, pidx):\n",
    "    \"\"\"\n",
    "    fraction of negative slopes after peak (monotone decline score).\n",
    "    \"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return np.nan\n",
    "\n",
    "    t2 = tb[pidx:]\n",
    "    f2 = fb[pidx:]\n",
    "    if len(f2) < 3:\n",
    "        return np.nan\n",
    "\n",
    "    dt = np.diff(t2)\n",
    "    df = np.diff(f2)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "\n",
    "    frac_neg = float(np.mean((df[good] / dt[good]) < 0))\n",
    "    return frac_neg\n",
    "\n",
    "\n",
    "def count_rebrighten(tb, fb, baseline_pre, amp, pidx, frac=REBRIGHT_FRAC):\n",
    "    \"\"\"\n",
    "    Count how often post-peak rises above baseline_pre + frac*amp after having dropped below it.\n",
    "    \"\"\"\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return 0\n",
    "\n",
    "    level = baseline_pre + frac * amp\n",
    "    post = fb[pidx:]\n",
    "    if len(post) < 3:\n",
    "        return 0\n",
    "\n",
    "    above = post > level\n",
    "    crossings = np.sum((~above[:-1]) & (above[1:]))\n",
    "    return int(crossings)\n",
    "\n",
    "\n",
    "def fall_time_to_level(tb, fb, baseline_pre, amp, pidx, frac):\n",
    "    \"\"\"\n",
    "    t_fallX: time from peak to first time flux <= baseline_pre + frac*amp\n",
    "    using only decay segment.\n",
    "    \"\"\"\n",
    "    if amp <= 0 or pidx is None:\n",
    "        return np.nan\n",
    "\n",
    "    level = baseline_pre + frac * amp\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "    if len(f_dec) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    idx = np.where(f_dec <= level)[0]\n",
    "    if len(idx) == 0:\n",
    "        return np.nan\n",
    "    return float(t_dec[idx[0]] - t_dec[0])\n",
    "\n",
    "\n",
    "def decay_powerlaw_fit(tb, fb, baseline_pre, pidx, tmax=300.0):\n",
    "    \"\"\"\n",
    "    Fit log(f-baseline) = a + b*log(dt) on post-peak points.\n",
    "    Returns slope b, r2, npts.\n",
    "    \"\"\"\n",
    "    if pidx is None or pidx >= len(fb) - 3:\n",
    "        return np.nan, np.nan, 0\n",
    "\n",
    "    t0 = tb[pidx]\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "\n",
    "    dt = t_dec - t0\n",
    "    m = (dt > 0.0) & (dt <= tmax)\n",
    "    dt = dt[m]\n",
    "    fd = f_dec[m] - baseline_pre\n",
    "\n",
    "    # must be positive\n",
    "    m2 = fd > 0.0\n",
    "    dt = dt[m2]\n",
    "    fd = fd[m2]\n",
    "\n",
    "    if len(dt) < 4:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "\n",
    "    x = np.log(dt + EPS)\n",
    "    y = np.log(fd + EPS)\n",
    "\n",
    "    # linear fit\n",
    "    try:\n",
    "        b, a = np.polyfit(x, y, 1)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "\n",
    "    yhat = a + b * x\n",
    "    ss_res = float(np.sum((y - yhat) ** 2))\n",
    "    ss_tot = float(np.sum((y - np.mean(y)) ** 2)) + EPS\n",
    "    r2 = 1.0 - ss_res / ss_tot\n",
    "\n",
    "    return float(b), float(r2), int(len(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8431f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deextinct_band(flux, flux_err, ebv, band, r_v=R_V):\n",
    "    if ebv is None or (isinstance(ebv, float) and np.isnan(ebv)):\n",
    "        return flux, flux_err, 0.0\n",
    "\n",
    "    A_V = float(ebv) * float(r_v)\n",
    "    wave = np.array([EFF_WL_AA[band]], dtype=float)  # Angstrom\n",
    "    A_lambda = float(fitzpatrick99(wave, A_V, r_v=r_v, unit=\"aa\")[0])  # mag\n",
    "\n",
    "    fac = 10.0 ** (0.4 * A_lambda)\n",
    "    return flux * fac, flux_err * fac, A_lambda\n",
    "\n",
    "\n",
    "def deextinct_lightcurve(lc, ebv):\n",
    "    flux = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    ferr = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    flux_corr = flux.copy()\n",
    "    ferr_corr = ferr.copy()\n",
    "\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        flux_corr[m], ferr_corr[m], _ = deextinct_band(flux_corr[m], ferr_corr[m], ebv, b)\n",
    "\n",
    "    return flux_corr, ferr_corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f8ffa",
   "metadata": {},
   "source": [
    "## Global features\n",
    "\n",
    "These are computed using all observations across all bands for a given object.  \n",
    "They summarize time coverage, brightness distribution, cadence, variability, and context (redshift + dust + redshift uncertainty).\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_obs` | Total number of observations across all filters | Captures overall sampling density and how well-measured the object is |\n",
    "| `total_time_obs` | Observed-frame time baseline: `max(t_rel) - min(t_rel)` | Separates short transients vs long events and measures overall monitoring duration |\n",
    "| `total_time_rest` | Rest-frame time baseline: `total_time_obs / (1+z)` | Removes time dilation so the model compares intrinsic evolution speed across redshifts |\n",
    "\n",
    "### Flux distribution (dust-corrected `flux_corr`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `flux_mean` | Mean corrected flux | Measures average intrinsic brightness level (sensitive to sustained high flux) |\n",
    "| `flux_median` | Median corrected flux | Robust typical brightness baseline (less sensitive to one-off spikes) |\n",
    "| `flux_std` | Standard deviation of corrected flux | Captures variability strength (high = more change over time) |\n",
    "| `flux_min` | Minimum corrected flux | Captures deep fades / dips / negative excursions from noise-subtraction artifacts |\n",
    "| `flux_max` | Maximum corrected flux | Captures peak brightness or flare intensity (key transient signature) |\n",
    "| `flux_mad` | Median absolute deviation of corrected flux | Robust variability estimate that doesn’t get bullied by outliers |\n",
    "| `flux_iqr` | Interquartile range of corrected flux | Another robust variability measure (spread of the middle 50%) |\n",
    "| `flux_skew` | Skewness of corrected flux distribution | Detects asymmetric lightcurves (fast rise / slow decay vs vice versa) |\n",
    "| `flux_kurt_excess` | Excess kurtosis of corrected flux distribution | Detects heavy tails/spiky behavior from rare bursts or sharp transients |\n",
    "| `flux_p5` | 5th percentile of corrected flux | Robust low-end level (less sensitive than min) |\n",
    "| `flux_p25` | 25th percentile of corrected flux | Lower-quartile level |\n",
    "| `flux_p75` | 75th percentile of corrected flux | Upper-quartile level |\n",
    "| `flux_p95` | 95th percentile of corrected flux | Robust high-end level (less sensitive than max) |\n",
    "| `robust_amp_global` | Robust amplitude: `flux_p95 - flux_p5` | Outlier-resistant variability scale, often better than max-min |\n",
    "| `neg_flux_frac` | Fraction of corrected flux values `< 0` | Flags noise-dominated objects or weak detections where measurements hover around zero |\n",
    "\n",
    "### SNR (using corrected errors `err_corr`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `snr_median` | Median SNR where `snr = \\|flux_corr\\| / (err_corr + EPS)` | Typical detection quality (separates clean signals from noisy junk) |\n",
    "| `snr_max` | Maximum SNR | Captures the strongest detection event (some transients “light up” briefly) |\n",
    "\n",
    "### Cadence / gaps\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `median_dt` | Median time gap between consecutive observations in `t_rel` | Describes typical cadence (important since sparse sampling hides shape) |\n",
    "| `max_gap` | Maximum time gap between consecutive observations in `t_rel` | Detects missing windows (large gaps can explain unreliable peak/width estimates) |\n",
    "\n",
    "### Time-series variability / shape\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `eta_von_neumann` | Von Neumann eta statistic on `flux_corr` (smoothness vs jumpiness) | Separates smooth evolving curves from noisy jitter or sudden jumps |\n",
    "| `chi2_const_global` | Chi-square vs constant-flux model using `err_corr` | Quantifies variability relative to measurement noise (true variability vs noise) |\n",
    "| `stetsonJ_global_obs` | Stetson J (consecutive-pairs) using observed-frame times | More cadence-aware correlation metric; reduces sensitivity to irregular sampling |\n",
    "| `stetsonJ_global_rest` | Stetson J (consecutive-pairs) using rest-frame times | Same correlation idea, but corrected for time dilation |\n",
    "\n",
    "### Slopes / rate of change (global)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `max_slope_global_obs` | Maximum absolute slope in observed time (`t_rel`) | Captures fastest brightness change (sharp rise/fall events) |\n",
    "| `max_slope_global_rest` | Maximum absolute slope in rest-frame time (`t_rest`) | Intrinsic fastest change rate (removes redshift stretching) |\n",
    "| `med_abs_slope_global_obs` | Median absolute slope in observed time | Typical observed change rate (slow drifters vs active transients) |\n",
    "| `med_abs_slope_global_rest` | Median absolute slope in rest-frame time | Typical intrinsic change rate |\n",
    "| `slope_global_obs` | Best-fit linear slope over observed time | Captures long-term trend direction (rising vs fading overall) |\n",
    "| `slope_global_rest` | Best-fit linear slope over rest-frame time | Same trend, but comparable across redshifts |\n",
    "\n",
    "### Fractional variability\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `fvar_global` | Fractional variability accounting for measurement errors | Estimates intrinsic variability strength after subtracting noise contribution |\n",
    "\n",
    "### Context metadata\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `Z` | Redshift `z` | Encodes distance/epoch effects and shifts events into different observed regimes |\n",
    "| `log1pZ` | `log(1+z)` | Stabilizes redshift scaling for models (less extreme leverage at high `z`) |\n",
    "| `Z_err` | Redshift uncertainty (clipped to `>= 0`) | Captures confidence in rest-frame correction; noisy redshifts degrade timing features |\n",
    "| `log1pZerr` | `log(1+Z_err)` | Stabilizes uncertainty scaling and helps tree models split more smoothly |\n",
    "| `EBV` | Dust reddening used for extinction correction | Helps the model learn residual dust systematics and measurement conditions |\n",
    "\n",
    "### Filter coverage\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_filters_present` | Number of filters with ≥ 1 observation | Multi-band coverage gives richer color/shape info; missing bands can correlate with class |\n",
    "| `total_obs` | Total observations summed across all filters (same as `n_obs`) | Redundant but convenient sanity/coverage signal that some models exploit |\n",
    "\n",
    "## Per-filter (band-wise) features\n",
    "\n",
    "For each band `b ∈ {u,g,r,i,z,y}`, the following features are computed independently per filter.  \n",
    "This version adds **pre-peak baseline features** and richer **post-peak decay morphology**.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_{b}` | Number of observations in band `b` | Band missingness and sampling density vary by object/class and affect reliability |\n",
    "| `baseline_pre_{b}` | Estimated baseline flux before the main peak (from earliest fraction of points) | Gives a cleaner “true baseline” than median when post-peak tail biases the median |\n",
    "| `amp_{b}` | Peak above median baseline: `peak_flux - median(fb)` | Simple band strength; works even if pre-peak baseline is unreliable |\n",
    "| `amp_pre_{b}` | Peak above pre-peak baseline: `peak_flux - baseline_pre` | Physically better peak amplitude when baseline is stable; improves peak-related shape features |\n",
    "| `robust_amp_{b}` | Robust amplitude: `p95_b - p5_b` | More stable amplitude estimate when peaks/outliers are noisy |\n",
    "| `tpeak_{b}_obs` | Observed-frame time of peak flux in band `b` | Captures when the band reaches maximum brightness (timing is class-dependent) |\n",
    "| `tpeak_{b}_rest` | Rest-frame time of peak flux: `tpeak_obs / (1+z)` | Removes time dilation so peak timing is comparable across redshifts |\n",
    "| `snrmax_{b}` | Maximum SNR within band `b` | Strongest detection in that band (some classes peak strongly only in certain filters) |\n",
    "| `eta_{b}` | Von Neumann eta within band `b` | Detects smooth evolution vs noise inside a single wavelength band |\n",
    "| `chi2_const_{b}` | Chi-square vs constant-flux model within band | Measures variability significance relative to band-specific noise |\n",
    "| `slope_{b}_obs` | Best-fit linear slope in band over observed time | Captures overall rise/fade trend per band |\n",
    "| `slope_{b}_rest` | Best-fit linear slope in band over rest-frame time | Intrinsic trend per band (comparable across redshifts) |\n",
    "| `maxslope_{b}_obs` | Maximum absolute slope in band (observed time) | Captures sharpest observed change per band |\n",
    "| `maxslope_{b}_rest` | Maximum absolute slope in band (rest time) | Captures sharpest intrinsic change rate per band |\n",
    "| `stetsonJ_{b}_obs` | Stetson J (consecutive-pairs) in band using observed time | Cadence-aware correlation metric per band |\n",
    "| `stetsonJ_{b}_rest` | Stetson J (consecutive-pairs) in band using rest time | Same, but corrected for time dilation |\n",
    "| `fvar_{b}` | Fractional variability within band (noise-corrected) | Intrinsic variability strength per band |\n",
    "| `p5_{b}` | 5th percentile of band flux `fb` | Robust low-end level per band |\n",
    "| `p25_{b}` | 25th percentile of `fb` | Lower-quartile level per band |\n",
    "| `p75_{b}` | 75th percentile of `fb` | Upper-quartile level per band |\n",
    "| `p95_{b}` | 95th percentile of `fb` | Robust high-end level per band |\n",
    "| `mad_{b}` | Median absolute deviation of `fb` | Robust band variability (outlier-resistant) |\n",
    "| `iqr_{b}` | Interquartile range of `fb` | Robust spread of the middle 50% per band |\n",
    "| `mad_over_std_{b}` | `mad_b / (std_b + EPS)` | Flags spike-dominated vs Gaussian-like variability (robustness/shape cue) |\n",
    "\n",
    "### Post-peak fall times, widths, and sharpness (only if `amp_pre_{b} > 0`)\n",
    "\n",
    "These use `baseline_pre_{b}` and `amp_pre_{b}` to define levels as fractions of the peak amplitude.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `t_fall50_{b}_obs` | Observed-frame time after peak to reach `baseline_pre + 0.50 * amp_pre` | Encodes decay speed in observed time (fast vs slow fall) |\n",
    "| `t_fall20_{b}_obs` | Observed-frame time after peak to reach `baseline_pre + 0.20 * amp_pre` | Longer-tail decay behavior; distinguishes slow fade vs quick drop |\n",
    "| `t_fall50_{b}_rest` | Rest-frame fall time to 50% level | Intrinsic decay speed comparable across redshifts |\n",
    "| `t_fall20_{b}_rest` | Rest-frame fall time to 20% level | Intrinsic late-time fading timescale |\n",
    "| `width50_{b}_obs` | Observed-frame width above 50% level (time span where `fb >= base + 0.5*amp`) | Measures how long the event stays bright in observed time |\n",
    "| `width80_{b}_obs` | Observed-frame width above 80% level | Captures core peak width (sharp vs broad peak) |\n",
    "| `width50_{b}_rest` | Rest-frame width above 50% level | Intrinsic duration at mid-brightness |\n",
    "| `width80_{b}_rest` | Rest-frame width above 80% level | Intrinsic core-peak duration |\n",
    "| `sharp50_{b}_obs` | Sharpness proxy: `amp_pre / (width50_obs + EPS)` | High = tall + narrow peaks (very class-discriminative) |\n",
    "| `sharp50_{b}_rest` | Sharpness proxy in rest-frame | Same idea, but intrinsic (less redshift-biased) |\n",
    "| `auc_pos_{b}_obs` | Observed-frame AUC above `baseline_pre`: `∫ max(fb - baseline_pre, 0) dt` | Energy-like summary tied to true baseline, not median-biased |\n",
    "| `auc_pos_{b}_rest` | Rest-frame AUC above `baseline_pre` | Comparable across redshifts; strong spectral-energy cue |\n",
    "\n",
    "### Peak structure and post-peak behavior (only if `amp_pre_{b} > 0`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `peak_dominance_{b}` | `amp_pre / (mad_pre + EPS)` where `mad_pre` is pre-peak baseline MAD | Measures how dominant the peak is relative to baseline noise (real transients pop out) |\n",
    "| `std_ratio_prepost_{b}` | `std(pre_seg) / (std(post_seg) + EPS)` | Captures how variability changes after peak (e.g., noisy baseline vs smooth decay) |\n",
    "| `n_peaks_{b}` | Count of significant peaks above baseline (sigma-thresholded) | Distinguishes single-peaked transients from multi-peaked/variable sources |\n",
    "| `postpeak_monotone_frac_{b}` | Fraction of post-peak steps that are monotonic decreasing | Smooth decays (high) vs rebrightening/AGN-like variability (low) |\n",
    "| `n_rebrighten_{b}` | Count of rebrightening events after peak (relative to `amp_pre`) | Strong discriminator: rebrightening often means non-simple transient behavior |\n",
    "\n",
    "### Decay power-law fit (post-peak, only if `amp_pre_{b} > 0`)\n",
    "\n",
    "A power-law fit is attempted on the post-peak decay segment (up to a max time window).\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `decay_pl_slope_{b}_obs` | Fitted power-law decay slope in observed time | Encodes decay physics/shape; different classes have different decay slopes |\n",
    "| `decay_pl_r2_{b}_obs` | R² of the observed-frame power-law fit | Measures how well a clean power-law explains the decay (clean transient vs messy variability) |\n",
    "| `decay_pl_npts_{b}_obs` | Number of points used in the observed-frame decay fit | Reliability indicator: more points = more trustworthy slope |\n",
    "| `decay_pl_slope_{b}_rest` | Fitted power-law decay slope in rest-frame time | Intrinsic decay slope, comparable across redshifts |\n",
    "| `decay_pl_r2_{b}_rest` | R² of the rest-frame power-law fit | Fit quality after time dilation correction |\n",
    "| `decay_pl_npts_{b}_rest` | Number of points used in the rest-frame decay fit | Reliability indicator in rest-frame |\n",
    "\n",
    "## Multi-band peak timing dispersion\n",
    "\n",
    "These summarize how synchronized (or not) the band peaks are across filters.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `tpeak_std_obs` | Standard deviation of `tpeak_b_obs` across bands with peaks | Measures chromatic timing spread in observed time (class-dependent) |\n",
    "| `tpeak_std_rest` | Standard deviation of `tpeak_b_rest` across bands with peaks | Intrinsic chromatic peak spread (less redshift-biased) |\n",
    "\n",
    "## Cross-band pair features (adjacent pairs: `ug, gr, ri, iz, zy`)\n",
    "\n",
    "For each adjacent filter pair `(a,b)`, these compare peak timing and peak flux ratios across wavelengths.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `tpeakdiff_{a}{b}_obs` | Observed-frame peak time difference: `tpeak_a_obs - tpeak_b_obs` | Chromatic peak lag/lead in observed time (includes cadence + dilation effects) |\n",
    "| `tpeakdiff_{a}{b}_rest` | Rest-frame peak time difference: `tpeak_a_rest - tpeak_b_rest` | Intrinsic chromatic lag/lead; strong class signature (blue earlier than red, etc.) |\n",
    "| `peakratio_{a}{b}` | Peak flux ratio: `peak_flux_a / (peak_flux_b + EPS)` | Strong color/SED proxy without needing explicit magnitudes |\n",
    "\n",
    "## Color features at r-peak (observed-frame) + 20/40-day evolution\n",
    "\n",
    "These interpolate `g`, `r`, `i` flux at the observed time when the r-band peaks (`tpeak_r_obs`), then compute log-flux colors.  \n",
    "They also sample the same colors at `+20` and `+40` days to capture cooling/heating trends.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `color_gr_at_rpeak_obs` | `log1p(f_g) - log1p(f_r)` evaluated at `tpeak_r_obs` | Measures g-r color at peak, highly class-dependent |\n",
    "| `color_ri_at_rpeak_obs` | `log1p(f_r) - log1p(f_i)` evaluated at `tpeak_r_obs` | Measures r-i color at peak (temperature / SED proxy) |\n",
    "| `color_gr_rpeak_p20_obs` | g-r color at `tpeak_r_obs + 20` days | Captures medium-term color evolution after peak |\n",
    "| `color_ri_rpeak_p20_obs` | r-i color at `tpeak_r_obs + 20` days | Same, for redder color index |\n",
    "| `color_gr_rpeak_p40_obs` | g-r color at `tpeak_r_obs + 40` days | Captures longer-term cooling/heating behavior |\n",
    "| `color_ri_rpeak_p40_obs` | r-i color at `tpeak_r_obs + 40` days | Longer-term evolution in redder bands |\n",
    "| `color_gr_slope20_obs` | `(color_gr(+20) - color_gr(0)) / 20` | Rate of color change over 20 days (cooling slope) |\n",
    "| `color_ri_slope20_obs` | `(color_ri(+20) - color_ri(0)) / 20` | Rate of red color change over 20 days |\n",
    "| `color_gr_slope40_obs` | `(color_gr(+40) - color_gr(0)) / 40` | Rate of color change over 40 days (more stable, less noisy) |\n",
    "| `color_ri_slope40_obs` | `(color_ri(+40) - color_ri(0)) / 40` | Rate of red color change over 40 days |\n",
    "\n",
    "## SpecType teacher stacking features (high-level)\n",
    "\n",
    "`add_spectype_teacher_features()` adds *legal stacking* features by training a multiclass model on **train only** to predict a grouped version of `SpecType`, then appending the predicted class probabilities as new features.\n",
    "\n",
    "Key steps:\n",
    "- Map `SpecType` → `SpecTypeGroup` (TDE, AGN, SNIa, SNother, Other)\n",
    "- Train a LightGBM multiclass model using CV splits by `split`\n",
    "- Create:\n",
    "  - OOF probabilities for train\n",
    "  - full-fit probabilities for test\n",
    "- Append probabilities + entropy as new features\n",
    "\n",
    "### Per-class probability features\n",
    "\n",
    "For every group label `c` in `classes`:\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `p_spec_{c}` | Predicted probability the object belongs to SpecTypeGroup `c` | Injects a strong “soft label” summary of transient type, which improves the final binary classifier |\n",
    "\n",
    "### Probability-uncertainty feature\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `spec_entropy` | Entropy of the teacher probability vector | High entropy = teacher unsure (ambiguous object); low entropy = confident type signal (more reliable stacking) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa8a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_object(lc_raw, z, z_err, ebv):\n",
    "    feats = {}\n",
    "\n",
    "    lc = lc_raw.sort_values(\"Time (MJD)\").reset_index(drop=True)\n",
    "\n",
    "    t = lc[\"Time (MJD)\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    if len(t) == 0:\n",
    "        feats[\"n_obs\"] = 0\n",
    "        return feats\n",
    "\n",
    "    z = safe_float(z, default=0.0)\n",
    "    z_err = safe_float(z_err, default=0.0)\n",
    "    ebv = safe_float(ebv, default=np.nan)\n",
    "\n",
    "    t_rel = t - t.min()\n",
    "    t_rest = t_rel / (1.0 + z)\n",
    "\n",
    "    flux_corr, err_corr = deextinct_lightcurve(lc, ebv)\n",
    "\n",
    "    feats[\"n_obs\"] = int(len(t))\n",
    "    feats[\"total_time_obs\"] = float(t_rel.max() - t_rel.min())\n",
    "    feats[\"total_time_rest\"] = float(t_rest.max() - t_rest.min())\n",
    "\n",
    "    feats[\"flux_mean\"] = float(np.mean(flux_corr))\n",
    "    feats[\"flux_median\"] = float(np.median(flux_corr))\n",
    "    feats[\"flux_std\"] = float(np.std(flux_corr))\n",
    "    feats[\"flux_min\"] = float(np.min(flux_corr))\n",
    "    feats[\"flux_max\"] = float(np.max(flux_corr))\n",
    "\n",
    "    feats[\"flux_mad\"] = median_abs_dev(flux_corr)\n",
    "    feats[\"flux_iqr\"] = iqr(flux_corr)\n",
    "    feats[\"flux_skew\"] = skewness(flux_corr)\n",
    "    feats[\"flux_kurt_excess\"] = kurtosis_excess(flux_corr)\n",
    "\n",
    "    p5, p25, p75, p95 = np.percentile(flux_corr, [5, 25, 75, 95])\n",
    "    feats[\"flux_p5\"] = float(p5)\n",
    "    feats[\"flux_p25\"] = float(p25)\n",
    "    feats[\"flux_p75\"] = float(p75)\n",
    "    feats[\"flux_p95\"] = float(p95)\n",
    "    feats[\"robust_amp_global\"] = float(p95 - p5)\n",
    "\n",
    "    feats[\"neg_flux_frac\"] = float(np.mean(flux_corr < 0))\n",
    "\n",
    "    snr = np.abs(flux_corr) / (err_corr + EPS)\n",
    "    feats[\"snr_median\"] = float(np.median(snr))\n",
    "    feats[\"snr_max\"] = float(np.max(snr))\n",
    "\n",
    "    if len(t_rel) >= 2:\n",
    "        dt = np.diff(t_rel)\n",
    "        feats[\"median_dt\"] = float(np.median(dt))\n",
    "        feats[\"max_gap\"] = float(np.max(dt))\n",
    "    else:\n",
    "        feats[\"median_dt\"] = np.nan\n",
    "        feats[\"max_gap\"] = np.nan\n",
    "\n",
    "    feats[\"eta_von_neumann\"] = von_neumann_eta(flux_corr)\n",
    "    feats[\"chi2_const_global\"] = chi2_to_constant(flux_corr, err_corr)\n",
    "\n",
    "    feats[\"stetsonJ_global_obs\"] = stetson_J_consecutive(t_rel, flux_corr, err_corr)\n",
    "    feats[\"stetsonJ_global_rest\"] = stetson_J_consecutive(t_rest, flux_corr, err_corr)\n",
    "\n",
    "    feats[\"max_slope_global_obs\"] = max_slope(t_rel, flux_corr)\n",
    "    feats[\"max_slope_global_rest\"] = max_slope(t_rest, flux_corr)\n",
    "\n",
    "    feats[\"med_abs_slope_global_obs\"] = median_abs_slope(t_rel, flux_corr)\n",
    "    feats[\"med_abs_slope_global_rest\"] = median_abs_slope(t_rest, flux_corr)\n",
    "\n",
    "    feats[\"slope_global_obs\"] = linear_slope(t_rel, flux_corr)\n",
    "    feats[\"slope_global_rest\"] = linear_slope(t_rest, flux_corr)\n",
    "\n",
    "    feats[\"fvar_global\"] = fractional_variability(flux_corr, err_corr)\n",
    "\n",
    "    feats[\"Z\"] = float(z)\n",
    "    feats[\"log1pZ\"] = float(np.log1p(max(0.0, z)))\n",
    "    feats[\"Z_err\"] = float(max(0.0, z_err))\n",
    "    feats[\"log1pZerr\"] = float(np.log1p(max(0.0, feats[\"Z_err\"])))\n",
    "    feats[\"EBV\"] = ebv\n",
    "\n",
    "    feats[\"n_filters_present\"] = 0\n",
    "    feats[\"total_obs\"] = 0\n",
    "\n",
    "    band_tpeak_obs = {}\n",
    "    band_tpeak_rest = {}\n",
    "    band_peak_flux = {}\n",
    "    band_tb_obs = {}\n",
    "    band_tb_rest = {}\n",
    "    band_fb = {}\n",
    "\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        nb = int(np.sum(m))\n",
    "        feats[f\"n_{b}\"] = nb\n",
    "        feats[\"total_obs\"] += nb\n",
    "\n",
    "        for k in [\n",
    "            f\"amp_{b}\",\n",
    "            f\"amp_pre_{b}\",\n",
    "            f\"baseline_pre_{b}\",\n",
    "            f\"robust_amp_{b}\",\n",
    "            f\"tpeak_{b}_obs\",\n",
    "            f\"tpeak_{b}_rest\",\n",
    "            f\"width50_{b}_obs\",\n",
    "            f\"width50_{b}_rest\",\n",
    "            f\"width80_{b}_obs\",\n",
    "            f\"width80_{b}_rest\",\n",
    "            f\"auc_pos_{b}_obs\",\n",
    "            f\"auc_pos_{b}_rest\",\n",
    "            f\"snrmax_{b}\",\n",
    "            f\"eta_{b}\",\n",
    "            f\"chi2_const_{b}\",\n",
    "            f\"slope_{b}_obs\",\n",
    "            f\"slope_{b}_rest\",\n",
    "            f\"maxslope_{b}_obs\",\n",
    "            f\"maxslope_{b}_rest\",\n",
    "            f\"stetsonJ_{b}_obs\",\n",
    "            f\"stetsonJ_{b}_rest\",\n",
    "            f\"p5_{b}\",\n",
    "            f\"p25_{b}\",\n",
    "            f\"p75_{b}\",\n",
    "            f\"p95_{b}\",\n",
    "            f\"mad_{b}\",\n",
    "            f\"iqr_{b}\",\n",
    "            f\"mad_over_std_{b}\",\n",
    "            f\"fvar_{b}\",\n",
    "            f\"t_fall50_{b}_obs\",\n",
    "            f\"t_fall20_{b}_obs\",\n",
    "            f\"t_fall50_{b}_rest\",\n",
    "            f\"t_fall20_{b}_rest\",\n",
    "            f\"sharp50_{b}_obs\",\n",
    "            f\"sharp50_{b}_rest\",\n",
    "            f\"peak_dominance_{b}\",\n",
    "            f\"std_ratio_prepost_{b}\",\n",
    "            f\"n_peaks_{b}\",\n",
    "            f\"postpeak_monotone_frac_{b}\",\n",
    "            f\"n_rebrighten_{b}\",\n",
    "            f\"decay_pl_slope_{b}_obs\",\n",
    "            f\"decay_pl_r2_{b}_obs\",\n",
    "            f\"decay_pl_npts_{b}_obs\",\n",
    "            f\"decay_pl_slope_{b}_rest\",\n",
    "            f\"decay_pl_r2_{b}_rest\",\n",
    "            f\"decay_pl_npts_{b}_rest\",\n",
    "        ]:\n",
    "            feats[k] = np.nan\n",
    "\n",
    "        if nb == 0:\n",
    "            continue\n",
    "\n",
    "        feats[\"n_filters_present\"] += 1\n",
    "\n",
    "        tb_obs = t_rel[m]\n",
    "        fb = flux_corr[m]\n",
    "        eb = err_corr[m]\n",
    "\n",
    "        order = np.argsort(tb_obs)\n",
    "        tb_obs = tb_obs[order]\n",
    "        fb = fb[order]\n",
    "        eb = eb[order]\n",
    "\n",
    "        tb_rest = tb_obs / (1.0 + z)\n",
    "\n",
    "        p5b, p25b, p75b, p95b = np.percentile(fb, [5, 25, 75, 95])\n",
    "        feats[f\"p5_{b}\"] = float(p5b)\n",
    "        feats[f\"p25_{b}\"] = float(p25b)\n",
    "        feats[f\"p75_{b}\"] = float(p75b)\n",
    "        feats[f\"p95_{b}\"] = float(p95b)\n",
    "        feats[f\"robust_amp_{b}\"] = float(p95b - p5b)\n",
    "\n",
    "        feats[f\"mad_{b}\"] = median_abs_dev(fb)\n",
    "        feats[f\"iqr_{b}\"] = iqr(fb)\n",
    "        stdb = float(np.std(fb))\n",
    "        feats[f\"mad_over_std_{b}\"] = float(feats[f\"mad_{b}\"] / (stdb + EPS))\n",
    "\n",
    "        base_pre, mad_pre, mederr_pre = pre_peak_baseline(tb_obs, fb, eb, frac=PRE_BASE_FRAC)\n",
    "        feats[f\"baseline_pre_{b}\"] = float(base_pre) if np.isfinite(base_pre) else np.nan\n",
    "\n",
    "        pidx = int(np.argmax(fb))\n",
    "        peak_flux = float(fb[pidx])\n",
    "        tpeak_obs = float(tb_obs[pidx])\n",
    "        tpeak_rest = float(tb_rest[pidx])\n",
    "\n",
    "        amp_median = peak_flux - float(np.median(fb))\n",
    "        amp_pre = peak_flux - base_pre if np.isfinite(base_pre) else np.nan\n",
    "\n",
    "        feats[f\"amp_{b}\"] = float(amp_median)\n",
    "        feats[f\"amp_pre_{b}\"] = float(amp_pre) if np.isfinite(amp_pre) else np.nan\n",
    "\n",
    "        feats[f\"tpeak_{b}_obs\"] = tpeak_obs\n",
    "        feats[f\"tpeak_{b}_rest\"] = tpeak_rest\n",
    "        feats[f\"snrmax_{b}\"] = float(np.max(np.abs(fb) / (eb + EPS)))\n",
    "\n",
    "        feats[f\"eta_{b}\"] = von_neumann_eta(fb)\n",
    "        feats[f\"chi2_const_{b}\"] = chi2_to_constant(fb, eb)\n",
    "\n",
    "        feats[f\"slope_{b}_obs\"] = linear_slope(tb_obs, fb)\n",
    "        feats[f\"slope_{b}_rest\"] = linear_slope(tb_rest, fb)\n",
    "\n",
    "        feats[f\"maxslope_{b}_obs\"] = max_slope(tb_obs, fb)\n",
    "        feats[f\"maxslope_{b}_rest\"] = max_slope(tb_rest, fb)\n",
    "\n",
    "        feats[f\"stetsonJ_{b}_obs\"] = stetson_J_consecutive(tb_obs, fb, eb)\n",
    "        feats[f\"stetsonJ_{b}_rest\"] = stetson_J_consecutive(tb_rest, fb, eb)\n",
    "\n",
    "        feats[f\"fvar_{b}\"] = fractional_variability(fb, eb)\n",
    "\n",
    "        if np.isfinite(amp_pre) and amp_pre > 0:\n",
    "            feats[f\"peak_dominance_{b}\"] = float(amp_pre / (mad_pre + EPS))\n",
    "\n",
    "            pre_seg = fb[:max(2, pidx)]\n",
    "            post_seg = fb[pidx:]\n",
    "            std_pre = float(np.std(pre_seg)) if len(pre_seg) >= 2 else np.nan\n",
    "            std_post = float(np.std(post_seg)) if len(post_seg) >= 2 else np.nan\n",
    "            if np.isfinite(std_pre) and np.isfinite(std_post):\n",
    "                feats[f\"std_ratio_prepost_{b}\"] = float(std_pre / (std_post + EPS))\n",
    "\n",
    "            feats[f\"postpeak_monotone_frac_{b}\"] = float(postpeak_monotonicity(tb_obs, fb, pidx))\n",
    "            feats[f\"n_peaks_{b}\"] = float(count_significant_peaks(tb_obs, fb, eb, base_pre, k_sigma=PEAK_SIGMA_K))\n",
    "            feats[f\"n_rebrighten_{b}\"] = float(count_rebrighten(tb_obs, fb, base_pre, amp_pre, pidx, frac=REBRIGHT_FRAC))\n",
    "            feats[f\"t_fall50_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "            feats[f\"t_fall50_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "            feats[f\"auc_pos_{b}_obs\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_obs))\n",
    "            feats[f\"auc_pos_{b}_rest\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_rest))\n",
    "\n",
    "            def width_at_level(tt, ff, base, amp, frac):\n",
    "                if amp <= 0 or len(ff) < 3:\n",
    "                    return np.nan\n",
    "                level = base + frac * amp\n",
    "                above = ff >= level\n",
    "                if not np.any(above):\n",
    "                    return np.nan\n",
    "                idx = np.where(above)[0]\n",
    "                return float(tt[idx[-1]] - tt[idx[0]])\n",
    "\n",
    "            w50_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.80)\n",
    "            w50_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.80)\n",
    "\n",
    "            feats[f\"width50_{b}_obs\"] = w50_obs\n",
    "            feats[f\"width80_{b}_obs\"] = w80_obs\n",
    "            feats[f\"width50_{b}_rest\"] = w50_rest\n",
    "            feats[f\"width80_{b}_rest\"] = w80_rest\n",
    "\n",
    "            feats[f\"sharp50_{b}_obs\"] = float(amp_pre / (w50_obs + EPS)) if np.isfinite(w50_obs) else np.nan\n",
    "            feats[f\"sharp50_{b}_rest\"] = float(amp_pre / (w50_rest + EPS)) if np.isfinite(w50_rest) else np.nan\n",
    "\n",
    "            b_obs, r2_obs, npts_obs = decay_powerlaw_fit(tb_obs, fb, base_pre, pidx, tmax=300.0)\n",
    "            b_rest, r2_rest, npts_rest = decay_powerlaw_fit(tb_rest, fb, base_pre, pidx, tmax=300.0)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_obs\"] = b_obs\n",
    "            feats[f\"decay_pl_r2_{b}_obs\"] = r2_obs\n",
    "            feats[f\"decay_pl_npts_{b}_obs\"] = float(npts_obs)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_rest\"] = b_rest\n",
    "            feats[f\"decay_pl_r2_{b}_rest\"] = r2_rest\n",
    "            feats[f\"decay_pl_npts_{b}_rest\"] = float(npts_rest)\n",
    "\n",
    "        band_tpeak_obs[b] = tpeak_obs\n",
    "        band_tpeak_rest[b] = tpeak_rest\n",
    "        band_peak_flux[b] = peak_flux\n",
    "        band_tb_obs[b] = tb_obs\n",
    "        band_tb_rest[b] = tb_rest\n",
    "        band_fb[b] = fb\n",
    "\n",
    "    tpeaks_obs = [band_tpeak_obs.get(b, np.nan) for b in FILTERS]\n",
    "    tpeaks_rest = [band_tpeak_rest.get(b, np.nan) for b in FILTERS]\n",
    "\n",
    "    tpeaks_obs = np.array([x for x in tpeaks_obs if np.isfinite(x)], float)\n",
    "    tpeaks_rest = np.array([x for x in tpeaks_rest if np.isfinite(x)], float)\n",
    "\n",
    "    feats[\"tpeak_std_obs\"] = float(np.std(tpeaks_obs)) if len(tpeaks_obs) >= 2 else np.nan\n",
    "    feats[\"tpeak_std_rest\"] = float(np.std(tpeaks_rest)) if len(tpeaks_rest) >= 2 else np.nan\n",
    "\n",
    "    pairs = [(\"u\", \"g\"), (\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\"), (\"z\", \"y\")]\n",
    "    for a, b in pairs:\n",
    "        ta_obs, tb_obs = band_tpeak_obs.get(a, np.nan), band_tpeak_obs.get(b, np.nan)\n",
    "        ta_rest, tb_rest = band_tpeak_rest.get(a, np.nan), band_tpeak_rest.get(b, np.nan)\n",
    "        pa, pb = band_peak_flux.get(a, np.nan), band_peak_flux.get(b, np.nan)\n",
    "\n",
    "        feats[f\"tpeakdiff_{a}{b}_obs\"] = (ta_obs - tb_obs) if (np.isfinite(ta_obs) and np.isfinite(tb_obs)) else np.nan\n",
    "        feats[f\"tpeakdiff_{a}{b}_rest\"] = (ta_rest - tb_rest) if (np.isfinite(ta_rest) and np.isfinite(tb_rest)) else np.nan\n",
    "        feats[f\"peakratio_{a}{b}\"] = (pa / (pb + EPS)) if (np.isfinite(pa) and np.isfinite(pb)) else np.nan\n",
    "\n",
    "    def logp(x):\n",
    "        if np.isnan(x):\n",
    "            return np.nan\n",
    "        return float(np.log1p(max(0.0, x)))\n",
    "\n",
    "    tpr_obs = feats.get(\"tpeak_r_obs\", np.nan)\n",
    "    if np.isfinite(tpr_obs):\n",
    "        def colors_at_time(t0):\n",
    "            fr = interp_flux_at_time(band_tb_obs.get(\"r\", np.array([])), band_fb.get(\"r\", np.array([])), t0)\n",
    "            fg = interp_flux_at_time(band_tb_obs.get(\"g\", np.array([])), band_fb.get(\"g\", np.array([])), t0)\n",
    "            fi = interp_flux_at_time(band_tb_obs.get(\"i\", np.array([])), band_fb.get(\"i\", np.array([])), t0)\n",
    "\n",
    "            cgr = (logp(fg) - logp(fr)) if (np.isfinite(fg) and np.isfinite(fr)) else np.nan\n",
    "            cri = (logp(fr) - logp(fi)) if (np.isfinite(fr) and np.isfinite(fi)) else np.nan\n",
    "            return cgr, cri\n",
    "\n",
    "        cgr0, cri0 = colors_at_time(tpr_obs)\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = cgr0\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = cri0\n",
    "\n",
    "        cgr20, cri20 = colors_at_time(tpr_obs + 20.0)\n",
    "        cgr40, cri40 = colors_at_time(tpr_obs + 40.0)\n",
    "\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = cgr20\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = cri20\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = cgr40\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = cri40\n",
    "\n",
    "        def slope(c1, c2, dt):\n",
    "            if np.isfinite(c1) and np.isfinite(c2):\n",
    "                return float((c2 - c1) / dt)\n",
    "            return np.nan\n",
    "\n",
    "        feats[\"color_gr_slope20_obs\"] = slope(cgr0, cgr20, 20.0)\n",
    "        feats[\"color_ri_slope20_obs\"] = slope(cri0, cri20, 20.0)\n",
    "        feats[\"color_gr_slope40_obs\"] = slope(cgr0, cgr40, 40.0)\n",
    "        feats[\"color_ri_slope40_obs\"] = slope(cri0, cri40, 40.0)\n",
    "\n",
    "    else:\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope20_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope20_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope40_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope40_obs\"] = np.nan\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db43ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lightcurve_cache(splits, base_dir=\"data\", kind=\"train\"):\n",
    "    lc_cache = {}\n",
    "    idx_cache = {}\n",
    "\n",
    "    for s in splits:\n",
    "        path = f\"{base_dir}/{s}/{kind}_full_lightcurves.csv\"\n",
    "        lc = pd.read_csv(path)\n",
    "        groups = lc.groupby(\"object_id\").indices\n",
    "        lc_cache[s] = lc\n",
    "        idx_cache[s] = groups\n",
    "\n",
    "    return lc_cache, idx_cache\n",
    "\n",
    "\n",
    "def get_lightcurve(lc_cache, idx_cache, split, object_id):\n",
    "    idx = idx_cache[split].get(object_id, None)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    return lc_cache[split].iloc[idx]\n",
    "\n",
    "\n",
    "def build_feature_table(\n",
    "    log_df,\n",
    "    lc_cache,\n",
    "    idx_cache,\n",
    "    augment_photoz=False,\n",
    "    test_zerr_pool=None,\n",
    "    n_aug=1,\n",
    "    seed=6\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    if test_zerr_pool is not None:\n",
    "        test_zerr_pool = np.asarray(test_zerr_pool, float)\n",
    "        test_zerr_pool = test_zerr_pool[np.isfinite(test_zerr_pool)]\n",
    "        test_zerr_pool = test_zerr_pool[test_zerr_pool > 0]\n",
    "\n",
    "    for i in range(len(log_df)):\n",
    "        r = log_df.iloc[i]\n",
    "        obj = r[\"object_id\"]\n",
    "        split = r[\"split\"]\n",
    "\n",
    "        lc = get_lightcurve(lc_cache, idx_cache, split, obj)\n",
    "        if lc is None:\n",
    "            feats = {\"n_obs\": 0}\n",
    "            feats[\"object_id\"] = obj\n",
    "            feats[\"split\"] = split\n",
    "            feats[\"photoz_aug\"] = 0\n",
    "            if \"target\" in log_df.columns:\n",
    "                feats[\"target\"] = int(r[\"target\"])\n",
    "            rows.append(feats)\n",
    "            continue\n",
    "\n",
    "        feats = extract_features_for_object(\n",
    "            lc_raw=lc,\n",
    "            z=r[\"Z\"],\n",
    "            z_err=r.get(\"Z_err\", 0.0),\n",
    "            ebv=r[\"EBV\"],\n",
    "        )\n",
    "        feats[\"object_id\"] = obj\n",
    "        feats[\"split\"] = split\n",
    "        feats[\"photoz_aug\"] = 0\n",
    "        if \"target\" in log_df.columns:\n",
    "            feats[\"target\"] = int(r[\"target\"])\n",
    "        rows.append(feats)\n",
    "\n",
    "        if augment_photoz and (\"target\" in log_df.columns) and (test_zerr_pool is not None) and (len(test_zerr_pool) > 0):\n",
    "            z0 = safe_float(r[\"Z\"], default=0.0)\n",
    "            for _ in range(n_aug):\n",
    "                sigma = float(rng.choice(test_zerr_pool))\n",
    "                z_sim = max(0.0, z0 + float(rng.normal(0.0, sigma)))\n",
    "\n",
    "                feats2 = extract_features_for_object(\n",
    "                    lc_raw=lc,\n",
    "                    z=z_sim,\n",
    "                    z_err=sigma,\n",
    "                    ebv=r[\"EBV\"],\n",
    "                )\n",
    "                feats2[\"object_id\"] = obj\n",
    "                feats2[\"split\"] = split\n",
    "                feats2[\"target\"] = int(r[\"target\"])\n",
    "                feats2[\"photoz_aug\"] = 1\n",
    "                rows.append(feats2)\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf6b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(df, drop_cols):\n",
    "    X = df.drop(columns=drop_cols).copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    med = X.median(numeric_only=True)\n",
    "    X = X.fillna(med)\n",
    "    X = X.fillna(0.0)\n",
    "    return X\n",
    "\n",
    "\n",
    "def best_threshold_f1(y_true, probs):\n",
    "    ths = np.linspace(0.01, 0.99, 200)\n",
    "    f1s = [f1_score(y_true, probs > t, zero_division=0) for t in ths]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return float(ths[j]), float(f1s[j])\n",
    "\n",
    "\n",
    "def best_alpha_and_threshold(y_true, p_xgb, p_lgb):\n",
    "    alphas = np.linspace(0.0, 1.0, 101)\n",
    "    best = (0.5, 0.5, -1.0)  # alpha, th, f1\n",
    "\n",
    "    for a in alphas:\n",
    "        p = a * p_xgb + (1.0 - a) * p_lgb\n",
    "        th, f1 = best_threshold_f1(y_true, p)\n",
    "        if f1 > best[2]:\n",
    "            best = (float(a), float(th), float(f1))\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def make_splitter(n_splits, random_state=6):\n",
    "    return StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aaf9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10):\n",
    "    \"\"\"\n",
    "    Legal stacking:\n",
    "    - Train multiclass model to predict \"SpecTypeGroup\" on train\n",
    "    - Generate OOF probs for train, full-fit probs for test\n",
    "    - Append probs + entropy as additional features\n",
    "\n",
    "    This uses only features that exist in both train/test.\n",
    "    \"\"\"\n",
    "\n",
    "    df = train_feat.merge(train_log[[\"object_id\", \"SpecType\"]], on=\"object_id\", how=\"left\")\n",
    "    spec = df[\"SpecType\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "    def map_group(s):\n",
    "        if s == \"TDE\":\n",
    "            return \"TDE\"\n",
    "        if s == \"AGN\":\n",
    "            return \"AGN\"\n",
    "        if s == \"SN Ia\" or s.startswith(\"SN Ia\"):\n",
    "            return \"SNIa\"\n",
    "        if s.startswith(\"SN\"):\n",
    "            return \"SNother\"\n",
    "        return \"Other\"\n",
    "\n",
    "    spec_group = spec.map(map_group).astype(str)\n",
    "\n",
    "    classes = sorted(spec_group.unique())\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_mc = spec_group.map(class_to_idx).to_numpy()\n",
    "\n",
    "    X_tr = clean_features(df, drop_cols=[\"object_id\", \"split\", \"target\", \"SpecType\"])\n",
    "    X_te = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"])\n",
    "\n",
    "    groups = df[\"split\"].to_numpy()\n",
    "\n",
    "    splitter = make_splitter(n_splits, random_state=6)\n",
    "    split_iter = splitter.split(X_tr, y_mc, groups)\n",
    "\n",
    "    oof = np.zeros((len(X_tr), len(classes)), dtype=float)\n",
    "\n",
    "    base = dict(\n",
    "        objective=\"multiclass\",\n",
    "        num_class=len(classes),\n",
    "        metric=\"multi_logloss\",\n",
    "\n",
    "        n_estimators=20000,\n",
    "        learning_rate=0.03,\n",
    "\n",
    "        num_leaves=63,\n",
    "        min_child_samples=5,\n",
    "\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=0.8,\n",
    "\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=-1,\n",
    "        force_col_wise=True\n",
    "    )\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        model = LGBMClassifier(**base)\n",
    "        model.fit(\n",
    "            X_tr.iloc[tr_idx], y_mc[tr_idx],\n",
    "            eval_set=[(X_tr.iloc[va_idx], y_mc[va_idx])],\n",
    "            eval_metric=\"multi_logloss\",\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "        )\n",
    "        oof[va_idx] = model.predict_proba(\n",
    "            X_tr.iloc[va_idx],\n",
    "            num_iteration=model.best_iteration_\n",
    "        )\n",
    "        print(f\"[SpecType teacher] fold {fold:02d} done\")\n",
    "\n",
    "    model_full = LGBMClassifier(**base)\n",
    "    model_full.fit(X_tr, y_mc)\n",
    "    p_test = model_full.predict_proba(X_te)\n",
    "\n",
    "    def entropy(p):\n",
    "        p = np.clip(p, 1e-12, 1.0)\n",
    "        return -np.sum(p * np.log(p), axis=1)\n",
    "\n",
    "    for i, c in enumerate(classes):\n",
    "        train_feat[f\"p_spec_{c}\"] = oof[:, i]\n",
    "        test_feat[f\"p_spec_{c}\"] = p_test[:, i]\n",
    "\n",
    "    train_feat[\"spec_entropy\"] = entropy(oof)\n",
    "    test_feat[\"spec_entropy\"] = entropy(p_test)\n",
    "\n",
    "    return train_feat, test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c36e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_xgb(train_feat, n_folds_tune=10, timeout_sec=28800):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"])\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"random_state\": 6,\n",
    "            \"n_jobs\": -1,\n",
    "\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"device\": \"cuda\",\n",
    "\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 8000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.003, 0.12, log=True),\n",
    "\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 40),\n",
    "\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 20.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.05, 30.0),\n",
    "\n",
    "            \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "\n",
    "            \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "        }\n",
    "\n",
    "        if params[\"grow_policy\"] == \"lossguide\":\n",
    "            params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 256)\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        splitter = make_splitter(n_folds_tune, random_state=6)\n",
    "        split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "            neg = np.sum(y_tr == 0)\n",
    "            pos = np.sum(y_tr == 1)\n",
    "            params[\"scale_pos_weight\"] = float(neg / max(1, pos))\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "            probs = model.predict_proba(X_va)[:, 1]\n",
    "            ap = average_precision_score(y_va, probs)\n",
    "            scores.append(ap)\n",
    "\n",
    "            trial.report(float(np.mean(scores)), step=fold)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=6, multivariate=True, group=True)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=30, n_warmup_steps=3)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name=\"xgb_ap_split_cv_gpu\",\n",
    "        storage=\"sqlite:///optuna_xgb_ap_gpu.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=999999, timeout=timeout_sec)\n",
    "\n",
    "    print(\"\\nOptuna best AP:\", study.best_value)\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(k, \"=\", v)\n",
    "\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9088e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_ensemble(train_feat, xgb_params, n_splits_full=20):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"])\n",
    "\n",
    "    splitter = make_splitter(n_splits_full, random_state=6)\n",
    "    split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "    xgb_base = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 6,\n",
    "        \"n_jobs\": -1,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\",\n",
    "        **xgb_params\n",
    "    }\n",
    "\n",
    "    lgb_base = dict(\n",
    "        objective=\"binary\",\n",
    "        boosting_type=\"gbdt\",\n",
    "        n_estimators=20000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=63,\n",
    "        max_depth=-1,\n",
    "        min_child_samples=5,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        n_jobs=-1,\n",
    "        random_state=6,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "    xgb_models = []\n",
    "    lgb_models = []\n",
    "\n",
    "    oof_xgb = np.zeros(len(X), dtype=float)\n",
    "    oof_lgb = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        spw = float(neg / max(1, pos))\n",
    "\n",
    "        xgb_base[\"scale_pos_weight\"] = spw\n",
    "        xgb_model = XGBClassifier(**xgb_base)\n",
    "        xgb_model.fit(X_tr, y_tr, verbose=False)\n",
    "        p_xgb = xgb_model.predict_proba(X_va)[:, 1]\n",
    "        oof_xgb[va_idx] = p_xgb\n",
    "        xgb_models.append(xgb_model)\n",
    "\n",
    "        lgb_model = LGBMClassifier(**{**lgb_base, \"scale_pos_weight\": spw})\n",
    "        lgb_model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "        )\n",
    "        p_lgb = lgb_model.predict_proba(X_va, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "        oof_lgb[va_idx] = p_lgb\n",
    "        lgb_models.append(lgb_model)\n",
    "\n",
    "        p_tmp = 0.5 * p_xgb + 0.5 * p_lgb\n",
    "        th, f1 = best_threshold_f1(y_va, p_tmp)\n",
    "        print(f\"Fold {fold:02d} | temp blend(0.5) best F1={f1:.4f} @ th={th:.3f}\")\n",
    "\n",
    "    alpha_best, th_best, f1_best = best_alpha_and_threshold(y, oof_xgb, oof_lgb)\n",
    "    print(\"\\nOOF best alpha:\", alpha_best)\n",
    "    print(\"OOF best threshold:\", th_best)\n",
    "    print(\"OOF blended best F1:\", f1_best)\n",
    "\n",
    "    return xgb_models, lgb_models, alpha_best, th_best\n",
    "\n",
    "\n",
    "def predict_ensemble(test_feat, xgb_models, lgb_models, alpha):\n",
    "    X_test = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"])\n",
    "\n",
    "    p_xgb = np.mean([m.predict_proba(X_test)[:, 1] for m in xgb_models], axis=0)\n",
    "    p_lgb = np.mean([m.predict_proba(X_test)[:, 1] for m in lgb_models], axis=0)\n",
    "\n",
    "    p_blend = alpha * p_xgb + (1.0 - alpha) * p_lgb\n",
    "    return p_blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f368d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SPECTYPE_TEACHER = True\n",
    "\n",
    "from pathlib import Path\n",
    "ROOT = Path.cwd().parents[1]\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "train_log = pd.read_csv(DATA_DIR / \"train_log.csv\")\n",
    "test_log  = pd.read_csv(DATA_DIR / \"test_log.csv\")\n",
    "\n",
    "if \"Z_err\" not in train_log.columns:\n",
    "    train_log[\"Z_err\"] = 0.0\n",
    "train_log[\"Z_err\"] = train_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "if \"Z_err\" not in test_log.columns:\n",
    "    test_log[\"Z_err\"] = 0.0\n",
    "test_log[\"Z_err\"] = test_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "train_splits = sorted(train_log[\"split\"].unique())\n",
    "test_splits = sorted(test_log[\"split\"].unique())\n",
    "\n",
    "train_lc_cache, train_idx_cache = build_lightcurve_cache(train_splits, base_dir=\"data\", kind=\"train\")\n",
    "test_lc_cache, test_idx_cache = build_lightcurve_cache(test_splits, base_dir=\"data\", kind=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3411737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feat: (6086, 353)\n",
      "test_feat : (7135, 352)\n"
     ]
    }
   ],
   "source": [
    "test_zerr_pool = test_log[\"Z_err\"].dropna().values\n",
    "\n",
    "train_feat = build_feature_table(\n",
    "    train_log, train_lc_cache, train_idx_cache,\n",
    "    augment_photoz=True,\n",
    "    test_zerr_pool=test_zerr_pool,\n",
    "    n_aug=1,\n",
    "    seed=6\n",
    ")\n",
    "test_feat = build_feature_table(test_log, test_lc_cache, test_idx_cache)\n",
    "print(\"train_feat:\", train_feat.shape)\n",
    "print(\"test_feat :\", test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "273d46a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SpecType teacher] fold 01 done\n",
      "[SpecType teacher] fold 02 done\n",
      "[SpecType teacher] fold 03 done\n",
      "[SpecType teacher] fold 04 done\n",
      "[SpecType teacher] fold 05 done\n",
      "[SpecType teacher] fold 06 done\n",
      "[SpecType teacher] fold 07 done\n",
      "[SpecType teacher] fold 08 done\n",
      "[SpecType teacher] fold 09 done\n",
      "[SpecType teacher] fold 10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rowes\\Documents\\GitHub\\2026-ML-Projects\\.venv\\Lib\\site-packages\\optuna\\_experimental.py:33: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  optuna_warn(\n",
      "c:\\Users\\rowes\\Documents\\GitHub\\2026-ML-Projects\\.venv\\Lib\\site-packages\\optuna\\_experimental.py:33: ExperimentalWarning: Argument ``group`` is an experimental feature. The interface can change in the future.\n",
      "  optuna_warn(\n",
      "\u001b[32m[I 2026-01-23 02:30:22,444]\u001b[0m Using an existing study with name 'xgb_ap_split_cv_gpu' instead of creating a new one.\u001b[0m\n",
      "c:\\Users\\rowes\\Documents\\GitHub\\2026-ML-Projects\\.venv\\Lib\\site-packages\\xgboost\\core.py:774: UserWarning: [02:31:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "\u001b[32m[I 2026-01-23 02:36:52,974]\u001b[0m Trial 54 finished with value: 0.5720202657858958 and parameters: {'n_estimators': 5517, 'learning_rate': 0.007468075888488339, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.535318379384974, 'colsample_bytree': 0.8508864478794678, 'gamma': 1.1792960319994241, 'reg_alpha': 4.043209140453466, 'reg_lambda': 8.834042521353396, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 54 with value: 0.5720202657858958.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 02:43:39,548]\u001b[0m Trial 55 finished with value: 0.57956268187457 and parameters: {'n_estimators': 5839, 'learning_rate': 0.005438201455415597, 'max_depth': 10, 'min_child_weight': 3, 'subsample': 0.5259372045681608, 'colsample_bytree': 0.7234333156870005, 'gamma': 1.4852569303037535, 'reg_alpha': 1.3062101506238921, 'reg_lambda': 7.667749038176675, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 249}. Best is trial 55 with value: 0.57956268187457.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 02:50:04,862]\u001b[0m Trial 56 finished with value: 0.5743588023296468 and parameters: {'n_estimators': 5475, 'learning_rate': 0.005563586798751275, 'max_depth': 9, 'min_child_weight': 5, 'subsample': 0.5882888680804751, 'colsample_bytree': 0.7407931427099644, 'gamma': 0.8761727420695778, 'reg_alpha': 1.861226471333333, 'reg_lambda': 2.272061697016687, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 251}. Best is trial 55 with value: 0.57956268187457.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 02:54:32,938]\u001b[0m Trial 57 finished with value: 0.585589532822849 and parameters: {'n_estimators': 5548, 'learning_rate': 0.00786884717909611, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.6718053517796114, 'colsample_bytree': 0.7613637343559095, 'gamma': 4.865660512915058, 'reg_alpha': 1.6765528595363917, 'reg_lambda': 7.7565653937889465, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 250}. Best is trial 57 with value: 0.585589532822849.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 02:59:40,192]\u001b[0m Trial 58 finished with value: 0.5764089601697764 and parameters: {'n_estimators': 5200, 'learning_rate': 0.006370945945174926, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.6200975274295599, 'colsample_bytree': 0.7583700115703346, 'gamma': 2.6424972472347834, 'reg_alpha': 5.642381071419153, 'reg_lambda': 5.584891138192196, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 251}. Best is trial 57 with value: 0.585589532822849.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:04:22,354]\u001b[0m Trial 59 finished with value: 0.5688904705684487 and parameters: {'n_estimators': 5948, 'learning_rate': 0.00793784350719864, 'max_depth': 9, 'min_child_weight': 8, 'subsample': 0.5171332539073439, 'colsample_bytree': 0.7738288404841664, 'gamma': 1.9376491691093252, 'reg_alpha': 0.28783625178279837, 'reg_lambda': 2.038483923777421, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 251}. Best is trial 57 with value: 0.585589532822849.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:08:18,288]\u001b[0m Trial 60 finished with value: 0.546698443923653 and parameters: {'n_estimators': 5144, 'learning_rate': 0.012827456806099384, 'max_depth': 9, 'min_child_weight': 8, 'subsample': 0.5673625687878349, 'colsample_bytree': 0.7596066979415479, 'gamma': 1.3253978313961392, 'reg_alpha': 1.4753800646398643, 'reg_lambda': 3.0453386275114456, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 251}. Best is trial 57 with value: 0.585589532822849.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:15:17,030]\u001b[0m Trial 61 finished with value: 0.5842922280696381 and parameters: {'n_estimators': 5849, 'learning_rate': 0.005562854938643143, 'max_depth': 10, 'min_child_weight': 2, 'subsample': 0.5347129420091861, 'colsample_bytree': 0.6227783595040715, 'gamma': 1.4118821229144547, 'reg_alpha': 5.187789160362772, 'reg_lambda': 8.996866920099796, 'max_delta_step': 1, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 57 with value: 0.585589532822849.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:18:46,695]\u001b[0m Trial 62 finished with value: 0.5742561416251442 and parameters: {'n_estimators': 5374, 'learning_rate': 0.014019951418507313, 'max_depth': 8, 'min_child_weight': 11, 'subsample': 0.8651578566909922, 'colsample_bytree': 0.7648539339711283, 'gamma': 4.836261395292122, 'reg_alpha': 3.3574547977338245, 'reg_lambda': 11.55398451410822, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 249}. Best is trial 57 with value: 0.585589532822849.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:22:49,973]\u001b[0m Trial 63 finished with value: 0.5840763161044625 and parameters: {'n_estimators': 4940, 'learning_rate': 0.008242898321454294, 'max_depth': 10, 'min_child_weight': 2, 'subsample': 0.5202531232899159, 'colsample_bytree': 0.5251879503850359, 'gamma': 4.494842108390721, 'reg_alpha': 9.793768532136156, 'reg_lambda': 8.929060017595756, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 252}. Best is trial 57 with value: 0.585589532822849.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:25:29,399]\u001b[0m Trial 64 finished with value: 0.5727951462221046 and parameters: {'n_estimators': 4750, 'learning_rate': 0.01660899563818116, 'max_depth': 8, 'min_child_weight': 18, 'subsample': 0.8871481175986891, 'colsample_bytree': 0.8242022001344265, 'gamma': 5.037183602040346, 'reg_alpha': 2.216270296569149, 'reg_lambda': 8.191802237931967, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 57 with value: 0.585589532822849.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:28:55,200]\u001b[0m Trial 65 finished with value: 0.58377788935811 and parameters: {'n_estimators': 4576, 'learning_rate': 0.005697039779611503, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.5017984406674659, 'colsample_bytree': 0.5114875096019209, 'gamma': 6.941408494589012, 'reg_alpha': 9.872451377044253, 'reg_lambda': 3.788392410394617, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 255}. Best is trial 57 with value: 0.585589532822849.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:31:30,672]\u001b[0m Trial 66 finished with value: 0.5908796807933754 and parameters: {'n_estimators': 3795, 'learning_rate': 0.00748623809720597, 'max_depth': 6, 'min_child_weight': 17, 'subsample': 0.6136929880979775, 'colsample_bytree': 0.5266980367374834, 'gamma': 7.945816777869769, 'reg_alpha': 9.009804412461138, 'reg_lambda': 5.667695160186062, 'max_delta_step': 1, 'grow_policy': 'lossguide', 'max_leaves': 255}. Best is trial 66 with value: 0.5908796807933754.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:33:43,930]\u001b[0m Trial 67 finished with value: 0.581185023960953 and parameters: {'n_estimators': 3907, 'learning_rate': 0.013626348135067207, 'max_depth': 7, 'min_child_weight': 16, 'subsample': 0.5283694634379223, 'colsample_bytree': 0.5071788948685332, 'gamma': 7.3638871244503665, 'reg_alpha': 10.892175044445557, 'reg_lambda': 10.058327552998193, 'max_delta_step': 1, 'grow_policy': 'lossguide', 'max_leaves': 255}. Best is trial 66 with value: 0.5908796807933754.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:36:03,030]\u001b[0m Trial 68 finished with value: 0.5936703989114782 and parameters: {'n_estimators': 3730, 'learning_rate': 0.008254774858517189, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.6765918691098602, 'colsample_bytree': 0.5377519143681783, 'gamma': 9.36769770698968, 'reg_alpha': 8.526417407798016, 'reg_lambda': 11.05935619755266, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 68 with value: 0.5936703989114782.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:39:45,782]\u001b[0m Trial 69 finished with value: 0.5955649439205681 and parameters: {'n_estimators': 5603, 'learning_rate': 0.007547709880160317, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.6045075097938886, 'colsample_bytree': 0.5250505349996001, 'gamma': 5.348939695218225, 'reg_alpha': 16.94881804664208, 'reg_lambda': 12.795150176007256, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 255}. Best is trial 69 with value: 0.5955649439205681.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:43:13,853]\u001b[0m Trial 70 finished with value: 0.5956507483234942 and parameters: {'n_estimators': 5884, 'learning_rate': 0.010914113224760944, 'max_depth': 9, 'min_child_weight': 12, 'subsample': 0.6473782068840361, 'colsample_bytree': 0.5981064570073482, 'gamma': 4.710803046020745, 'reg_alpha': 19.837461136255232, 'reg_lambda': 14.631706760932047, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 70 with value: 0.5956507483234942.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:45:14,452]\u001b[0m Trial 71 finished with value: 0.5899129006991973 and parameters: {'n_estimators': 4116, 'learning_rate': 0.010273459707869392, 'max_depth': 4, 'min_child_weight': 23, 'subsample': 0.7652589336937248, 'colsample_bytree': 0.5789253095488835, 'gamma': 9.48277833452102, 'reg_alpha': 7.783619679307849, 'reg_lambda': 8.049769447439457, 'max_delta_step': 1, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 70 with value: 0.5956507483234942.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:48:32,310]\u001b[0m Trial 72 finished with value: 0.585175530589598 and parameters: {'n_estimators': 5983, 'learning_rate': 0.013754273466671825, 'max_depth': 8, 'min_child_weight': 12, 'subsample': 0.5556397474981847, 'colsample_bytree': 0.6071453583935931, 'gamma': 5.227577080467223, 'reg_alpha': 19.558853762671326, 'reg_lambda': 13.12176757490423, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 70 with value: 0.5956507483234942.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:52:18,006]\u001b[0m Trial 73 finished with value: 0.5964124991566666 and parameters: {'n_estimators': 5811, 'learning_rate': 0.007257705897648845, 'max_depth': 10, 'min_child_weight': 19, 'subsample': 0.6596364267100011, 'colsample_bytree': 0.5362527037097052, 'gamma': 5.234091065870451, 'reg_alpha': 17.94421125755283, 'reg_lambda': 12.59624992704851, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 73 with value: 0.5964124991566666.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:55:45,851]\u001b[0m Trial 74 finished with value: 0.5827924043207564 and parameters: {'n_estimators': 5892, 'learning_rate': 0.013258665795690974, 'max_depth': 9, 'min_child_weight': 24, 'subsample': 0.5700853468808281, 'colsample_bytree': 0.5264300189261645, 'gamma': 1.7865649274964546, 'reg_alpha': 18.212983357953412, 'reg_lambda': 11.118931030286944, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 256}. Best is trial 73 with value: 0.5964124991566666.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 03:57:57,619]\u001b[0m Trial 75 finished with value: 0.5848773356729814 and parameters: {'n_estimators': 4581, 'learning_rate': 0.008359067573498656, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7463487737774648, 'colsample_bytree': 0.5530309111204798, 'gamma': 9.392784409355059, 'reg_alpha': 2.863422632172835, 'reg_lambda': 7.929832684694888, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 216}. Best is trial 73 with value: 0.5964124991566666.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:01:03,707]\u001b[0m Trial 76 finished with value: 0.6022484057690456 and parameters: {'n_estimators': 5484, 'learning_rate': 0.010266398488480188, 'max_depth': 10, 'min_child_weight': 17, 'subsample': 0.7684977876746177, 'colsample_bytree': 0.5558004920885947, 'gamma': 7.077903595112536, 'reg_alpha': 18.489033282802282, 'reg_lambda': 14.51211649996187, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 217}. Best is trial 76 with value: 0.6022484057690456.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:04:52,037]\u001b[0m Trial 77 finished with value: 0.6103769844495144 and parameters: {'n_estimators': 5588, 'learning_rate': 0.006843048167920692, 'max_depth': 9, 'min_child_weight': 14, 'subsample': 0.8814636970260613, 'colsample_bytree': 0.5369336182394013, 'gamma': 6.7370005163028, 'reg_alpha': 18.58115668896463, 'reg_lambda': 26.754008408371433, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 203}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:08:10,497]\u001b[0m Trial 78 finished with value: 0.6030820474442354 and parameters: {'n_estimators': 5050, 'learning_rate': 0.007558587260380588, 'max_depth': 7, 'min_child_weight': 14, 'subsample': 0.9233593111559248, 'colsample_bytree': 0.6087944031379975, 'gamma': 7.751909087753279, 'reg_alpha': 17.253716727133646, 'reg_lambda': 25.704120863623174, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 187}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:11:49,771]\u001b[0m Trial 79 finished with value: 0.6099035868394973 and parameters: {'n_estimators': 5196, 'learning_rate': 0.006127579533559572, 'max_depth': 9, 'min_child_weight': 20, 'subsample': 0.9424035422523706, 'colsample_bytree': 0.6070924807546548, 'gamma': 7.251002212987279, 'reg_alpha': 18.513229448335874, 'reg_lambda': 24.145423020468932, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 175}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:15:07,032]\u001b[0m Trial 80 finished with value: 0.6095219338745997 and parameters: {'n_estimators': 5415, 'learning_rate': 0.008396195567280884, 'max_depth': 9, 'min_child_weight': 24, 'subsample': 0.9874373388515835, 'colsample_bytree': 0.5943722885477625, 'gamma': 6.1956545196437425, 'reg_alpha': 18.941771727609122, 'reg_lambda': 21.434835869148245, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 169}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:18:12,999]\u001b[0m Trial 81 finished with value: 0.6095690660486648 and parameters: {'n_estimators': 5991, 'learning_rate': 0.011148680929179495, 'max_depth': 8, 'min_child_weight': 21, 'subsample': 0.9648202110129833, 'colsample_bytree': 0.5559861420874679, 'gamma': 8.53668046005663, 'reg_alpha': 16.52427563289461, 'reg_lambda': 27.80271159256336, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 174}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:22:25,623]\u001b[0m Trial 82 finished with value: 0.5979334050235031 and parameters: {'n_estimators': 5537, 'learning_rate': 0.005372401078166604, 'max_depth': 9, 'min_child_weight': 26, 'subsample': 0.9565297002680596, 'colsample_bytree': 0.6682353086374578, 'gamma': 5.638966327632384, 'reg_alpha': 17.92126493003211, 'reg_lambda': 26.20912640244399, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 174}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:25:27,127]\u001b[0m Trial 83 finished with value: 0.6059536609649898 and parameters: {'n_estimators': 4662, 'learning_rate': 0.009154746012657686, 'max_depth': 8, 'min_child_weight': 26, 'subsample': 0.908655279449879, 'colsample_bytree': 0.5666825132779694, 'gamma': 5.087914664881323, 'reg_alpha': 18.57908856596693, 'reg_lambda': 23.77698824971074, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 169}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:29:08,322]\u001b[0m Trial 84 finished with value: 0.6009926845227238 and parameters: {'n_estimators': 4160, 'learning_rate': 0.005126758851151691, 'max_depth': 10, 'min_child_weight': 20, 'subsample': 0.8778373773971424, 'colsample_bytree': 0.5009262822886411, 'gamma': 5.5142960645952845, 'reg_alpha': 17.49098686715725, 'reg_lambda': 26.747168215917448, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 172}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:33:00,683]\u001b[0m Trial 85 finished with value: 0.6023681270993559 and parameters: {'n_estimators': 5578, 'learning_rate': 0.006687130064624332, 'max_depth': 9, 'min_child_weight': 30, 'subsample': 0.9903215148567217, 'colsample_bytree': 0.7326799430969981, 'gamma': 5.037073320176562, 'reg_alpha': 18.041731037592747, 'reg_lambda': 22.615599547630776, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 166}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:35:49,221]\u001b[0m Trial 86 finished with value: 0.6085828209450743 and parameters: {'n_estimators': 4969, 'learning_rate': 0.010060164888845254, 'max_depth': 6, 'min_child_weight': 20, 'subsample': 0.8673641383068212, 'colsample_bytree': 0.5919048803198476, 'gamma': 9.838760667104339, 'reg_alpha': 15.347064969624492, 'reg_lambda': 21.27586540012671, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 167}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:38:25,586]\u001b[0m Trial 87 finished with value: 0.6033556978047518 and parameters: {'n_estimators': 5136, 'learning_rate': 0.01397424171835711, 'max_depth': 5, 'min_child_weight': 21, 'subsample': 0.7738081781038867, 'colsample_bytree': 0.6000712249726287, 'gamma': 9.664779833261827, 'reg_alpha': 18.154923013640303, 'reg_lambda': 19.801203583910244, 'max_delta_step': 6, 'grow_policy': 'lossguide', 'max_leaves': 169}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:40:21,114]\u001b[0m Trial 88 finished with value: 0.5949720769296177 and parameters: {'n_estimators': 4743, 'learning_rate': 0.02545379142778633, 'max_depth': 4, 'min_child_weight': 16, 'subsample': 0.7316006265427907, 'colsample_bytree': 0.602176151220339, 'gamma': 9.24298751171205, 'reg_alpha': 16.619284375670595, 'reg_lambda': 17.11897904377092, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 164}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:42:54,115]\u001b[0m Trial 89 finished with value: 0.6033043492870709 and parameters: {'n_estimators': 4666, 'learning_rate': 0.009131959206716814, 'max_depth': 5, 'min_child_weight': 30, 'subsample': 0.8920220971501548, 'colsample_bytree': 0.5110193442362571, 'gamma': 8.966572017964356, 'reg_alpha': 17.33269706531976, 'reg_lambda': 13.983856793921456, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 167}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:45:38,395]\u001b[0m Trial 90 finished with value: 0.6049335666467777 and parameters: {'n_estimators': 4195, 'learning_rate': 0.00739554850449455, 'max_depth': 6, 'min_child_weight': 24, 'subsample': 0.9579329218857687, 'colsample_bytree': 0.5482555970796112, 'gamma': 7.522334783270813, 'reg_alpha': 15.612935405265954, 'reg_lambda': 12.474975842265454, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 172}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:47:28,292]\u001b[0m Trial 91 finished with value: 0.5911403306891236 and parameters: {'n_estimators': 4299, 'learning_rate': 0.011210893465621718, 'max_depth': 3, 'min_child_weight': 35, 'subsample': 0.8970885721424248, 'colsample_bytree': 0.5737652643080084, 'gamma': 8.409566635403014, 'reg_alpha': 18.87955221320821, 'reg_lambda': 21.150041756432127, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 172}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:50:52,060]\u001b[0m Trial 92 finished with value: 0.596923249318915 and parameters: {'n_estimators': 5753, 'learning_rate': 0.006180983913309911, 'max_depth': 5, 'min_child_weight': 17, 'subsample': 0.990874810792486, 'colsample_bytree': 0.543074608923238, 'gamma': 8.066961912966814, 'reg_alpha': 18.47799750012948, 'reg_lambda': 21.604037485784307, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 174}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:52:41,932]\u001b[0m Trial 93 finished with value: 0.6022123333450243 and parameters: {'n_estimators': 3757, 'learning_rate': 0.02297138682358142, 'max_depth': 6, 'min_child_weight': 19, 'subsample': 0.960119369719288, 'colsample_bytree': 0.5730377288137882, 'gamma': 7.65699958069021, 'reg_alpha': 13.638730908006847, 'reg_lambda': 8.878384614191257, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 143}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:56:05,605]\u001b[0m Trial 94 finished with value: 0.605487321060832 and parameters: {'n_estimators': 4591, 'learning_rate': 0.005763528115826685, 'max_depth': 8, 'min_child_weight': 14, 'subsample': 0.8912766656923032, 'colsample_bytree': 0.7155885171868842, 'gamma': 9.272560359800885, 'reg_alpha': 17.5890616633286, 'reg_lambda': 26.58104418113941, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 04:58:56,047]\u001b[0m Trial 95 finished with value: 0.601272750002345 and parameters: {'n_estimators': 3177, 'learning_rate': 0.005802874010079581, 'max_depth': 8, 'min_child_weight': 15, 'subsample': 0.8238021431505682, 'colsample_bytree': 0.8170203261562039, 'gamma': 9.987111716645746, 'reg_alpha': 15.736058077377102, 'reg_lambda': 27.456692167556053, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:01:20,664]\u001b[0m Trial 96 finished with value: 0.6050900957560877 and parameters: {'n_estimators': 5432, 'learning_rate': 0.009958409629918466, 'max_depth': 4, 'min_child_weight': 30, 'subsample': 0.9339886453270964, 'colsample_bytree': 0.5353198239474486, 'gamma': 8.726280320094249, 'reg_alpha': 16.883572169162175, 'reg_lambda': 7.704236175276118, 'max_delta_step': 6, 'grow_policy': 'lossguide', 'max_leaves': 141}. Best is trial 77 with value: 0.6103769844495144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:03:42,331]\u001b[0m Trial 97 finished with value: 0.6106008132071306 and parameters: {'n_estimators': 5315, 'learning_rate': 0.022971057270220998, 'max_depth': 6, 'min_child_weight': 27, 'subsample': 0.9748089913287799, 'colsample_bytree': 0.5364977122467027, 'gamma': 9.200938107544728, 'reg_alpha': 17.24468739258794, 'reg_lambda': 26.65130692569438, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 135}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:05:35,290]\u001b[0m Trial 98 finished with value: 0.5777793593192498 and parameters: {'n_estimators': 4614, 'learning_rate': 0.07279469462570025, 'max_depth': 7, 'min_child_weight': 26, 'subsample': 0.9453084535888493, 'colsample_bytree': 0.5665469853882182, 'gamma': 9.45880160937659, 'reg_alpha': 16.766480408088142, 'reg_lambda': 22.71422018962017, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 121}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:07:55,660]\u001b[0m Trial 99 finished with value: 0.5924462126326828 and parameters: {'n_estimators': 4822, 'learning_rate': 0.01984139388468305, 'max_depth': 9, 'min_child_weight': 22, 'subsample': 0.8670283566085906, 'colsample_bytree': 0.5727271114653357, 'gamma': 8.252825048478874, 'reg_alpha': 14.619922973000843, 'reg_lambda': 25.746325257365452, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 139}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:10:42,257]\u001b[0m Trial 100 finished with value: 0.6069684171838203 and parameters: {'n_estimators': 5916, 'learning_rate': 0.021563114377169807, 'max_depth': 7, 'min_child_weight': 21, 'subsample': 0.9427164336579602, 'colsample_bytree': 0.6602989718290723, 'gamma': 8.66380845645536, 'reg_alpha': 18.100382227650076, 'reg_lambda': 26.532430697794407, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:13:46,099]\u001b[0m Trial 101 finished with value: 0.5926329922315292 and parameters: {'n_estimators': 5373, 'learning_rate': 0.01061946009977211, 'max_depth': 7, 'min_child_weight': 27, 'subsample': 0.8260506922397854, 'colsample_bytree': 0.7385177707938191, 'gamma': 7.8739079662985745, 'reg_alpha': 16.85326164399172, 'reg_lambda': 28.087006462169395, 'max_delta_step': 5, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:15:50,013]\u001b[0m Trial 102 finished with value: 0.5925302556355637 and parameters: {'n_estimators': 2854, 'learning_rate': 0.01562786379104628, 'max_depth': 6, 'min_child_weight': 25, 'subsample': 0.7986162136010732, 'colsample_bytree': 0.6142029249638545, 'gamma': 2.3891564327306436, 'reg_alpha': 16.387707692917633, 'reg_lambda': 18.95282066397739, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 131}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:18:55,260]\u001b[0m Trial 103 finished with value: 0.5973626284683243 and parameters: {'n_estimators': 5544, 'learning_rate': 0.01439024744015083, 'max_depth': 9, 'min_child_weight': 3, 'subsample': 0.9193808441672394, 'colsample_bytree': 0.6855856290467782, 'gamma': 7.2803579269211465, 'reg_alpha': 14.759474762659032, 'reg_lambda': 25.608029593198406, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:22:06,700]\u001b[0m Trial 104 finished with value: 0.6004996657921491 and parameters: {'n_estimators': 5648, 'learning_rate': 0.007420909719011309, 'max_depth': 6, 'min_child_weight': 20, 'subsample': 0.9195797457843184, 'colsample_bytree': 0.5601101289045448, 'gamma': 8.835189509529343, 'reg_alpha': 17.680981245326937, 'reg_lambda': 1.1499160023512696, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 189}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:24:31,305]\u001b[0m Trial 105 finished with value: 0.5779679243057075 and parameters: {'n_estimators': 5937, 'learning_rate': 0.030924120504438764, 'max_depth': 5, 'min_child_weight': 17, 'subsample': 0.9405095452107983, 'colsample_bytree': 0.6705022494644249, 'gamma': 6.972516286431596, 'reg_alpha': 10.717751031523736, 'reg_lambda': 23.5187098670324, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:27:32,074]\u001b[0m Trial 106 finished with value: 0.6084858522314633 and parameters: {'n_estimators': 5916, 'learning_rate': 0.010626912481347606, 'max_depth': 8, 'min_child_weight': 31, 'subsample': 0.9915783923852998, 'colsample_bytree': 0.5490027375806458, 'gamma': 8.630465241565544, 'reg_alpha': 11.494851022915391, 'reg_lambda': 29.943701167616357, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 143}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:29:40,303]\u001b[0m Trial 107 finished with value: 0.604261787087629 and parameters: {'n_estimators': 5568, 'learning_rate': 0.02420610075047533, 'max_depth': 4, 'min_child_weight': 22, 'subsample': 0.9835258794910791, 'colsample_bytree': 0.5815581700852009, 'gamma': 9.27589763826064, 'reg_alpha': 17.29542180254385, 'reg_lambda': 27.778879121235214, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 129}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:32:39,577]\u001b[0m Trial 108 finished with value: 0.5966411579574038 and parameters: {'n_estimators': 5968, 'learning_rate': 0.011598070963896364, 'max_depth': 8, 'min_child_weight': 39, 'subsample': 0.9033494242419382, 'colsample_bytree': 0.5268681605881267, 'gamma': 7.308020158977661, 'reg_alpha': 2.9292891136133825, 'reg_lambda': 27.24074847368459, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 138}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:34:49,383]\u001b[0m Trial 109 finished with value: 0.58441571281788 and parameters: {'n_estimators': 5727, 'learning_rate': 0.0063153710233828115, 'max_depth': 2, 'min_child_weight': 38, 'subsample': 0.943103213937674, 'colsample_bytree': 0.5657364760042407, 'gamma': 9.757149946004525, 'reg_alpha': 13.118332336479703, 'reg_lambda': 9.81573239783118, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 139}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:38:18,103]\u001b[0m Trial 110 finished with value: 0.57043519185689 and parameters: {'n_estimators': 5329, 'learning_rate': 0.006779593688359019, 'max_depth': 8, 'min_child_weight': 18, 'subsample': 0.9687250288433618, 'colsample_bytree': 0.6354447892042221, 'gamma': 9.255655207308532, 'reg_alpha': 3.455462964639416, 'reg_lambda': 28.86704354528506, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 191}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:40:33,834]\u001b[0m Trial 111 finished with value: 0.592570657390003 and parameters: {'n_estimators': 4502, 'learning_rate': 0.019919672785174838, 'max_depth': 8, 'min_child_weight': 19, 'subsample': 0.9047574830628127, 'colsample_bytree': 0.6760030851617302, 'gamma': 9.255795073240078, 'reg_alpha': 17.951391448317665, 'reg_lambda': 27.91303068309969, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:43:27,794]\u001b[0m Trial 112 finished with value: 0.6018207815732766 and parameters: {'n_estimators': 4610, 'learning_rate': 0.00926177775676314, 'max_depth': 6, 'min_child_weight': 23, 'subsample': 0.9573983581429711, 'colsample_bytree': 0.6948623135635362, 'gamma': 5.814475585790005, 'reg_alpha': 17.222032961998316, 'reg_lambda': 12.94589767441348, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 162}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:45:57,780]\u001b[0m Trial 113 finished with value: 0.6019257083844748 and parameters: {'n_estimators': 5074, 'learning_rate': 0.016376811647974283, 'max_depth': 6, 'min_child_weight': 35, 'subsample': 0.9605776304421336, 'colsample_bytree': 0.5157572055641447, 'gamma': 4.685290065539404, 'reg_alpha': 18.028999386834926, 'reg_lambda': 9.620944892014023, 'max_delta_step': 9, 'grow_policy': 'lossguide', 'max_leaves': 185}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:48:30,518]\u001b[0m Trial 114 finished with value: 0.6022614447180541 and parameters: {'n_estimators': 5443, 'learning_rate': 0.01618365031337496, 'max_depth': 7, 'min_child_weight': 31, 'subsample': 0.9705907176129474, 'colsample_bytree': 0.5512360101706888, 'gamma': 9.621226477829481, 'reg_alpha': 8.280284546147165, 'reg_lambda': 29.860418370603544, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 150}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:51:21,660]\u001b[0m Trial 115 finished with value: 0.6038699889899843 and parameters: {'n_estimators': 4731, 'learning_rate': 0.007580732321716063, 'max_depth': 10, 'min_child_weight': 30, 'subsample': 0.9453469095770218, 'colsample_bytree': 0.5481617029457154, 'gamma': 9.607579347489391, 'reg_alpha': 19.76859934454825, 'reg_lambda': 17.520087213901785, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 187}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:54:06,016]\u001b[0m Trial 116 finished with value: 0.6047298224686793 and parameters: {'n_estimators': 5042, 'learning_rate': 0.010664043866692362, 'max_depth': 6, 'min_child_weight': 25, 'subsample': 0.8887871056919505, 'colsample_bytree': 0.5578982897160666, 'gamma': 7.357439514937381, 'reg_alpha': 14.45879565536647, 'reg_lambda': 15.32899183548534, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 05:57:39,943]\u001b[0m Trial 117 finished with value: 0.6043341403761606 and parameters: {'n_estimators': 5962, 'learning_rate': 0.007530181378808896, 'max_depth': 10, 'min_child_weight': 25, 'subsample': 0.9664418090099977, 'colsample_bytree': 0.5395170615916428, 'gamma': 5.362797700662188, 'reg_alpha': 19.314505295235556, 'reg_lambda': 28.641743149964956, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:00:41,329]\u001b[0m Trial 118 finished with value: 0.6069896311952091 and parameters: {'n_estimators': 4116, 'learning_rate': 0.005464933426750224, 'max_depth': 9, 'min_child_weight': 15, 'subsample': 0.9271641353464921, 'colsample_bytree': 0.5946110736170688, 'gamma': 9.969737651534027, 'reg_alpha': 16.38551585600742, 'reg_lambda': 21.693660216237735, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:04:06,713]\u001b[0m Trial 119 finished with value: 0.6005122155586698 and parameters: {'n_estimators': 4710, 'learning_rate': 0.005667606414470635, 'max_depth': 10, 'min_child_weight': 16, 'subsample': 0.8188774310584617, 'colsample_bytree': 0.6520023954885035, 'gamma': 8.285016579273709, 'reg_alpha': 15.281127189259575, 'reg_lambda': 14.986813269247161, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:06:24,680]\u001b[0m Trial 120 finished with value: 0.5777112016110308 and parameters: {'n_estimators': 5548, 'learning_rate': 0.03413079739739532, 'max_depth': 6, 'min_child_weight': 36, 'subsample': 0.8527744670159192, 'colsample_bytree': 0.5226352390502873, 'gamma': 8.226696289404131, 'reg_alpha': 16.485764660150025, 'reg_lambda': 26.49286321864336, 'max_delta_step': 9, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:09:18,403]\u001b[0m Trial 121 finished with value: 0.6013037457063002 and parameters: {'n_estimators': 5757, 'learning_rate': 0.01144193637501552, 'max_depth': 10, 'min_child_weight': 30, 'subsample': 0.9130874629289915, 'colsample_bytree': 0.5142465712217165, 'gamma': 8.602703943188457, 'reg_alpha': 9.477982035458123, 'reg_lambda': 26.31451426140136, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:11:47,793]\u001b[0m Trial 122 finished with value: 0.6008554593525814 and parameters: {'n_estimators': 4543, 'learning_rate': 0.012090220006678417, 'max_depth': 7, 'min_child_weight': 22, 'subsample': 0.9970102595262531, 'colsample_bytree': 0.755547448280833, 'gamma': 9.580903157038959, 'reg_alpha': 14.926451414614492, 'reg_lambda': 21.78070457201608, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:14:44,296]\u001b[0m Trial 123 finished with value: 0.6077076452198658 and parameters: {'n_estimators': 5060, 'learning_rate': 0.008232197059644076, 'max_depth': 8, 'min_child_weight': 17, 'subsample': 0.819539012531781, 'colsample_bytree': 0.529604628692244, 'gamma': 9.278802045233592, 'reg_alpha': 18.089279757812108, 'reg_lambda': 17.51761631150997, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:17:18,811]\u001b[0m Trial 124 finished with value: 0.6054674881858484 and parameters: {'n_estimators': 3681, 'learning_rate': 0.008990443762536153, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.7937217428797478, 'colsample_bytree': 0.5902873103057252, 'gamma': 6.7601974350607446, 'reg_alpha': 17.861909768323013, 'reg_lambda': 23.061472294511375, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:18:55,776]\u001b[0m Trial 125 finished with value: 0.6104952369531104 and parameters: {'n_estimators': 2767, 'learning_rate': 0.020878599147036513, 'max_depth': 9, 'min_child_weight': 13, 'subsample': 0.9526674259902932, 'colsample_bytree': 0.5557107498786862, 'gamma': 5.042687248655024, 'reg_alpha': 16.98947605491837, 'reg_lambda': 22.936359447935562, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:21:50,765]\u001b[0m Trial 126 finished with value: 0.5832170525342905 and parameters: {'n_estimators': 5696, 'learning_rate': 0.020693086048556193, 'max_depth': 8, 'min_child_weight': 14, 'subsample': 0.9657080990461056, 'colsample_bytree': 0.5052679591409054, 'gamma': 5.1237350780984805, 'reg_alpha': 13.869000858567208, 'reg_lambda': 23.0050884328982, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 87}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:25:05,842]\u001b[0m Trial 127 finished with value: 0.6015947287693049 and parameters: {'n_estimators': 5632, 'learning_rate': 0.009371241172442344, 'max_depth': 9, 'min_child_weight': 20, 'subsample': 0.9740375554551428, 'colsample_bytree': 0.5133650906528362, 'gamma': 6.54395983786425, 'reg_alpha': 17.19508465763756, 'reg_lambda': 28.567506971326864, 'max_delta_step': 9, 'grow_policy': 'lossguide', 'max_leaves': 196}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:28:39,283]\u001b[0m Trial 128 finished with value: 0.5969592169830684 and parameters: {'n_estimators': 5651, 'learning_rate': 0.00715976696775813, 'max_depth': 7, 'min_child_weight': 25, 'subsample': 0.8179646712343553, 'colsample_bytree': 0.5509107137837712, 'gamma': 5.2378191656809925, 'reg_alpha': 18.45989750053822, 'reg_lambda': 20.399467841840092, 'max_delta_step': 9, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:29:54,294]\u001b[0m Trial 129 finished with value: 0.6091310510906522 and parameters: {'n_estimators': 1835, 'learning_rate': 0.01634880574981715, 'max_depth': 9, 'min_child_weight': 20, 'subsample': 0.9897328721793726, 'colsample_bytree': 0.5219333388113565, 'gamma': 5.731285177510374, 'reg_alpha': 17.749996990397875, 'reg_lambda': 17.401847912701044, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:30:48,285]\u001b[0m Trial 130 finished with value: 0.5948693240440099 and parameters: {'n_estimators': 1056, 'learning_rate': 0.019048210359584216, 'max_depth': 7, 'min_child_weight': 20, 'subsample': 0.8627737122241854, 'colsample_bytree': 0.552213390245659, 'gamma': 6.186903394765284, 'reg_alpha': 16.86385045113814, 'reg_lambda': 23.876183827497364, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:32:48,821]\u001b[0m Trial 131 finished with value: 0.5908752669099725 and parameters: {'n_estimators': 1433, 'learning_rate': 0.005726868331053665, 'max_depth': 10, 'min_child_weight': 13, 'subsample': 0.9973219912933932, 'colsample_bytree': 0.5636853664694468, 'gamma': 3.839717681251619, 'reg_alpha': 18.43885610997047, 'reg_lambda': 18.24064629174042, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:34:20,208]\u001b[0m Trial 132 finished with value: 0.595617501936355 and parameters: {'n_estimators': 2863, 'learning_rate': 0.029126169003300544, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.983912699632539, 'colsample_bytree': 0.6524570966257611, 'gamma': 6.2743644899954365, 'reg_alpha': 13.150314779058181, 'reg_lambda': 24.693538350782525, 'max_delta_step': 10, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:35:37,820]\u001b[0m Trial 133 finished with value: 0.5736150095773244 and parameters: {'n_estimators': 2234, 'learning_rate': 0.039168220400802076, 'max_depth': 9, 'min_child_weight': 8, 'subsample': 0.9813467281311186, 'colsample_bytree': 0.5310655132447469, 'gamma': 2.6322348579555084, 'reg_alpha': 15.134685740820437, 'reg_lambda': 26.696336546746366, 'max_delta_step': 5, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:38:50,162]\u001b[0m Trial 134 finished with value: 0.6024926124713691 and parameters: {'n_estimators': 4957, 'learning_rate': 0.006775759856098513, 'max_depth': 7, 'min_child_weight': 15, 'subsample': 0.8341853803305397, 'colsample_bytree': 0.5990411664130412, 'gamma': 8.200852930892983, 'reg_alpha': 14.853559254573895, 'reg_lambda': 14.431035379798073, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:40:11,453]\u001b[0m Trial 135 finished with value: 0.6014017078110977 and parameters: {'n_estimators': 1847, 'learning_rate': 0.013096207402170185, 'max_depth': 8, 'min_child_weight': 25, 'subsample': 0.9212696027033426, 'colsample_bytree': 0.515913849892812, 'gamma': 6.61158634888432, 'reg_alpha': 13.817425162614382, 'reg_lambda': 17.448020934066097, 'max_delta_step': 9, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:41:10,584]\u001b[0m Trial 136 finished with value: 0.597250100047352 and parameters: {'n_estimators': 1867, 'learning_rate': 0.03610503066453237, 'max_depth': 10, 'min_child_weight': 14, 'subsample': 0.998628378582165, 'colsample_bytree': 0.5762261970102407, 'gamma': 7.689689283260657, 'reg_alpha': 17.188165668357804, 'reg_lambda': 16.329344711722378, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:43:27,846]\u001b[0m Trial 137 finished with value: 0.5998241882632678 and parameters: {'n_estimators': 4530, 'learning_rate': 0.014108643418821922, 'max_depth': 9, 'min_child_weight': 14, 'subsample': 0.9597330005496062, 'colsample_bytree': 0.5057445635731413, 'gamma': 9.953067192023465, 'reg_alpha': 18.072578212984634, 'reg_lambda': 24.275398901087936, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:46:28,337]\u001b[0m Trial 138 finished with value: 0.5955324291480097 and parameters: {'n_estimators': 5306, 'learning_rate': 0.008868624144489492, 'max_depth': 8, 'min_child_weight': 21, 'subsample': 0.6211219479494287, 'colsample_bytree': 0.5913653360294251, 'gamma': 9.330538617778027, 'reg_alpha': 18.617118820212603, 'reg_lambda': 16.318444761046756, 'max_delta_step': 10, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:48:13,976]\u001b[0m Trial 139 finished with value: 0.6093264303706987 and parameters: {'n_estimators': 3136, 'learning_rate': 0.01930310058149258, 'max_depth': 9, 'min_child_weight': 22, 'subsample': 0.9522518021579702, 'colsample_bytree': 0.5673980705429312, 'gamma': 5.3705353520493855, 'reg_alpha': 19.31068722163703, 'reg_lambda': 27.111096472143906, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 97 with value: 0.6106008132071306.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:50:34,863]\u001b[0m Trial 140 finished with value: 0.6127042010241295 and parameters: {'n_estimators': 4403, 'learning_rate': 0.01628873590704205, 'max_depth': 7, 'min_child_weight': 19, 'subsample': 0.9827534570203291, 'colsample_bytree': 0.5859308567993065, 'gamma': 4.941608706939113, 'reg_alpha': 19.321695493198128, 'reg_lambda': 18.71715444214256, 'max_delta_step': 5, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:53:01,914]\u001b[0m Trial 141 finished with value: 0.6026988581536524 and parameters: {'n_estimators': 4038, 'learning_rate': 0.012395918677509964, 'max_depth': 8, 'min_child_weight': 19, 'subsample': 0.9915990360032972, 'colsample_bytree': 0.6122130712372433, 'gamma': 3.458414628226161, 'reg_alpha': 16.976873170427567, 'reg_lambda': 10.930363884010472, 'max_delta_step': 5, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:55:19,543]\u001b[0m Trial 142 finished with value: 0.6003876532906316 and parameters: {'n_estimators': 4243, 'learning_rate': 0.018039600399663826, 'max_depth': 9, 'min_child_weight': 19, 'subsample': 0.9650528860551252, 'colsample_bytree': 0.667551151912612, 'gamma': 5.39022777164972, 'reg_alpha': 17.985301633257045, 'reg_lambda': 26.43586021672825, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:56:35,843]\u001b[0m Trial 143 finished with value: 0.6106006775716916 and parameters: {'n_estimators': 2805, 'learning_rate': 0.034973472545192605, 'max_depth': 5, 'min_child_weight': 24, 'subsample': 0.9190002406467124, 'colsample_bytree': 0.6011916468709334, 'gamma': 4.710099610430985, 'reg_alpha': 19.312591486342722, 'reg_lambda': 27.27901601484806, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:58:13,522]\u001b[0m Trial 144 finished with value: 0.6018272840957237 and parameters: {'n_estimators': 3256, 'learning_rate': 0.027092973752966775, 'max_depth': 7, 'min_child_weight': 30, 'subsample': 0.9060639566194774, 'colsample_bytree': 0.5203635616280582, 'gamma': 5.269743895580878, 'reg_alpha': 17.633608038523228, 'reg_lambda': 18.984856771399194, 'max_delta_step': 5, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 06:59:00,257]\u001b[0m Trial 145 finished with value: 0.5820355204168025 and parameters: {'n_estimators': 1474, 'learning_rate': 0.024397106951066602, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.9874035566662144, 'colsample_bytree': 0.5079357727022455, 'gamma': 4.666311387752149, 'reg_alpha': 12.588438595361907, 'reg_lambda': 24.987474659736435, 'max_delta_step': 10, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:01:08,265]\u001b[0m Trial 146 finished with value: 0.598973008471053 and parameters: {'n_estimators': 1785, 'learning_rate': 0.011291319001093378, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.9151085258253335, 'colsample_bytree': 0.6231251467247547, 'gamma': 4.935492772515499, 'reg_alpha': 13.183557114511368, 'reg_lambda': 25.48502146930747, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 105}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:01:56,353]\u001b[0m Trial 147 finished with value: 0.5829098871264083 and parameters: {'n_estimators': 1806, 'learning_rate': 0.029551141535361983, 'max_depth': 3, 'min_child_weight': 22, 'subsample': 0.7745068797791452, 'colsample_bytree': 0.6094756428842503, 'gamma': 5.933349338723859, 'reg_alpha': 19.067963379241483, 'reg_lambda': 26.526730289276912, 'max_delta_step': 9, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:03:03,625]\u001b[0m Trial 148 finished with value: 0.6090816421116003 and parameters: {'n_estimators': 1659, 'learning_rate': 0.024202382531110074, 'max_depth': 8, 'min_child_weight': 20, 'subsample': 0.9048357438343074, 'colsample_bytree': 0.5419272854348314, 'gamma': 4.050899901661871, 'reg_alpha': 19.098813290223216, 'reg_lambda': 27.49446819989374, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:05:10,408]\u001b[0m Trial 149 finished with value: 0.6118451893306035 and parameters: {'n_estimators': 4313, 'learning_rate': 0.01999982051176752, 'max_depth': 7, 'min_child_weight': 28, 'subsample': 0.9486040336065932, 'colsample_bytree': 0.5816770312089722, 'gamma': 7.986445276809543, 'reg_alpha': 18.35031815543622, 'reg_lambda': 22.198530952321434, 'max_delta_step': 8, 'grow_policy': 'lossguide', 'max_leaves': 206}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:06:04,260]\u001b[0m Trial 150 finished with value: 0.5819654249513018 and parameters: {'n_estimators': 1883, 'learning_rate': 0.028981198632501537, 'max_depth': 3, 'min_child_weight': 30, 'subsample': 0.9365856845394305, 'colsample_bytree': 0.6800953896091078, 'gamma': 1.243946136312752, 'reg_alpha': 19.18060433701424, 'reg_lambda': 20.52284587910939, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:07:30,450]\u001b[0m Trial 151 finished with value: 0.5979635911856469 and parameters: {'n_estimators': 1949, 'learning_rate': 0.019401719270440328, 'max_depth': 8, 'min_child_weight': 23, 'subsample': 0.8209380235412991, 'colsample_bytree': 0.5781260368316155, 'gamma': 2.950649054656604, 'reg_alpha': 18.287353606680472, 'reg_lambda': 29.223650856501152, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:09:34,466]\u001b[0m Trial 152 finished with value: 0.5895055577974235 and parameters: {'n_estimators': 3939, 'learning_rate': 0.019250352209497525, 'max_depth': 8, 'min_child_weight': 31, 'subsample': 0.8432625234693543, 'colsample_bytree': 0.6399495159966394, 'gamma': 6.923024653001779, 'reg_alpha': 17.709096647588503, 'reg_lambda': 24.92401354259663, 'max_delta_step': 10, 'grow_policy': 'lossguide', 'max_leaves': 203}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:12:37,344]\u001b[0m Trial 153 finished with value: 0.6051356885859744 and parameters: {'n_estimators': 4699, 'learning_rate': 0.006571794036746433, 'max_depth': 10, 'min_child_weight': 18, 'subsample': 0.8444174150696195, 'colsample_bytree': 0.5470946305030021, 'gamma': 8.754578307723209, 'reg_alpha': 16.698371360858232, 'reg_lambda': 16.54347031890776, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:13:48,008]\u001b[0m Trial 154 finished with value: 0.589208485925372 and parameters: {'n_estimators': 2675, 'learning_rate': 0.045473303545337955, 'max_depth': 5, 'min_child_weight': 23, 'subsample': 0.8867187803358375, 'colsample_bytree': 0.5201608632255967, 'gamma': 2.83969097507594, 'reg_alpha': 16.452994307698972, 'reg_lambda': 26.62900061736607, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[33m[W 2026-01-23 07:13:48,042]\u001b[0m The parameter `n_estimators` in Trial#155 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:16:48,030]\u001b[0m Trial 155 finished with value: 0.5960021312242307 and parameters: {'n_estimators': 6697, 'learning_rate': 0.037039786845718674, 'max_depth': 8, 'min_child_weight': 18, 'subsample': 0.9859130092572675, 'colsample_bytree': 0.5606541011480066, 'gamma': 1.4658300221815548, 'reg_alpha': 17.40718744641308, 'reg_lambda': 11.577206072028616, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:18:25,243]\u001b[0m Trial 156 finished with value: 0.5761623417860403 and parameters: {'n_estimators': 3504, 'learning_rate': 0.05167224066542522, 'max_depth': 9, 'min_child_weight': 21, 'subsample': 0.7761192227629747, 'colsample_bytree': 0.5907661285271777, 'gamma': 5.9456499153019395, 'reg_alpha': 19.058772932207848, 'reg_lambda': 26.089713193182064, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:21:27,636]\u001b[0m Trial 157 finished with value: 0.5948372823088437 and parameters: {'n_estimators': 5693, 'learning_rate': 0.011433572506167835, 'max_depth': 10, 'min_child_weight': 25, 'subsample': 0.9955161305419952, 'colsample_bytree': 0.554758913262547, 'gamma': 6.0663788571742945, 'reg_alpha': 14.588546575931295, 'reg_lambda': 25.803764144918844, 'max_delta_step': 0, 'grow_policy': 'lossguide', 'max_leaves': 202}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:23:41,100]\u001b[0m Trial 158 finished with value: 0.6039274335460657 and parameters: {'n_estimators': 4488, 'learning_rate': 0.016189151007682444, 'max_depth': 6, 'min_child_weight': 35, 'subsample': 0.9919540808183143, 'colsample_bytree': 0.5679913320284524, 'gamma': 7.747694914333575, 'reg_alpha': 16.070902029310428, 'reg_lambda': 23.997692986896183, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 201}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:27:56,720]\u001b[0m Trial 159 finished with value: 0.5945975775993039 and parameters: {'n_estimators': 5309, 'learning_rate': 0.005136684666675676, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.9849594413898517, 'colsample_bytree': 0.713047275175981, 'gamma': 7.1116937931956725, 'reg_alpha': 19.40736117367647, 'reg_lambda': 14.842644978171313, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 194}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:29:56,615]\u001b[0m Trial 160 finished with value: 0.5958925814856728 and parameters: {'n_estimators': 4268, 'learning_rate': 0.028897582599388048, 'max_depth': 7, 'min_child_weight': 14, 'subsample': 0.8996401707199422, 'colsample_bytree': 0.5130425530955329, 'gamma': 7.2050897934967075, 'reg_alpha': 16.60059430648982, 'reg_lambda': 18.640244952196745, 'max_delta_step': 7, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:31:06,970]\u001b[0m Trial 161 finished with value: 0.6042164992871516 and parameters: {'n_estimators': 2238, 'learning_rate': 0.0432030316138738, 'max_depth': 8, 'min_child_weight': 24, 'subsample': 0.9412846414167401, 'colsample_bytree': 0.6354738649703027, 'gamma': 4.430717615734447, 'reg_alpha': 18.022450317144095, 'reg_lambda': 25.758015195590755, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:33:37,731]\u001b[0m Trial 162 finished with value: 0.5971359755673292 and parameters: {'n_estimators': 5469, 'learning_rate': 0.034009723102081106, 'max_depth': 7, 'min_child_weight': 13, 'subsample': 0.9001762162328892, 'colsample_bytree': 0.7298504958454135, 'gamma': 9.205120790359661, 'reg_alpha': 19.690196672221123, 'reg_lambda': 19.758217475094163, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:35:24,553]\u001b[0m Trial 163 finished with value: 0.5933490417028495 and parameters: {'n_estimators': 3510, 'learning_rate': 0.012304827551211837, 'max_depth': 4, 'min_child_weight': 11, 'subsample': 0.9550104564530191, 'colsample_bytree': 0.6301871967737188, 'gamma': 4.914613511793934, 'reg_alpha': 19.37390683462889, 'reg_lambda': 24.90829554543429, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:37:00,404]\u001b[0m Trial 164 finished with value: 0.6016498451472482 and parameters: {'n_estimators': 2397, 'learning_rate': 0.015622828335295921, 'max_depth': 9, 'min_child_weight': 25, 'subsample': 0.9468540811160521, 'colsample_bytree': 0.5158512128870616, 'gamma': 5.171205659247564, 'reg_alpha': 17.70445717910929, 'reg_lambda': 16.501543705538996, 'max_delta_step': 9, 'grow_policy': 'lossguide', 'max_leaves': 146}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:37:49,621]\u001b[0m Trial 165 finished with value: 0.5909363458693832 and parameters: {'n_estimators': 1496, 'learning_rate': 0.04177565852555064, 'max_depth': 5, 'min_child_weight': 34, 'subsample': 0.9212674044946801, 'colsample_bytree': 0.7498592517910165, 'gamma': 6.485955527854601, 'reg_alpha': 13.481846176554207, 'reg_lambda': 27.28761285125772, 'max_delta_step': 6, 'grow_policy': 'lossguide', 'max_leaves': 206}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:39:12,914]\u001b[0m Trial 166 finished with value: 0.6055619839943527 and parameters: {'n_estimators': 2151, 'learning_rate': 0.01972050510832347, 'max_depth': 8, 'min_child_weight': 19, 'subsample': 0.96844614023329, 'colsample_bytree': 0.5523347916656596, 'gamma': 3.795844499923221, 'reg_alpha': 16.60256135285511, 'reg_lambda': 16.81342480064819, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:43:02,800]\u001b[0m Trial 167 finished with value: 0.6089856653566854 and parameters: {'n_estimators': 5150, 'learning_rate': 0.006381867849937204, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.9090984924530765, 'colsample_bytree': 0.5723845626959907, 'gamma': 8.190789590722101, 'reg_alpha': 19.672117079034344, 'reg_lambda': 25.824105157122464, 'max_delta_step': 1, 'grow_policy': 'lossguide', 'max_leaves': 160}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:45:33,805]\u001b[0m Trial 168 finished with value: 0.5927952759383296 and parameters: {'n_estimators': 4880, 'learning_rate': 0.014126974396982333, 'max_depth': 10, 'min_child_weight': 14, 'subsample': 0.7403712121376416, 'colsample_bytree': 0.5007942183461955, 'gamma': 9.368920566702469, 'reg_alpha': 19.43995959981727, 'reg_lambda': 26.92368152191209, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 149}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:49:06,532]\u001b[0m Trial 169 finished with value: 0.5962630807460723 and parameters: {'n_estimators': 5940, 'learning_rate': 0.007233827303243619, 'max_depth': 9, 'min_child_weight': 18, 'subsample': 0.9691567812231121, 'colsample_bytree': 0.585320705999495, 'gamma': 8.952293627204265, 'reg_alpha': 13.070839112745062, 'reg_lambda': 26.692291431407, 'max_delta_step': 6, 'grow_policy': 'lossguide', 'max_leaves': 159}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:53:18,441]\u001b[0m Trial 170 finished with value: 0.6086319311189168 and parameters: {'n_estimators': 5438, 'learning_rate': 0.00517855284329444, 'max_depth': 8, 'min_child_weight': 11, 'subsample': 0.8482057946432802, 'colsample_bytree': 0.5843771250576364, 'gamma': 7.995934374884284, 'reg_alpha': 17.103231935055227, 'reg_lambda': 28.23198414460866, 'max_delta_step': 1, 'grow_policy': 'lossguide', 'max_leaves': 166}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:54:18,313]\u001b[0m Trial 171 finished with value: 0.6040176549414294 and parameters: {'n_estimators': 2302, 'learning_rate': 0.04359357328690315, 'max_depth': 5, 'min_child_weight': 28, 'subsample': 0.9690791869617402, 'colsample_bytree': 0.6420741858341634, 'gamma': 6.613746264040298, 'reg_alpha': 18.89812643441133, 'reg_lambda': 21.08038868903257, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 07:56:37,998]\u001b[0m Trial 172 finished with value: 0.595333798481888 and parameters: {'n_estimators': 5190, 'learning_rate': 0.026800260006252925, 'max_depth': 7, 'min_child_weight': 12, 'subsample': 0.9658226327207988, 'colsample_bytree': 0.5057633686028089, 'gamma': 9.909665108018297, 'reg_alpha': 13.825709851843836, 'reg_lambda': 21.515128002038075, 'max_delta_step': 10, 'grow_policy': 'lossguide', 'max_leaves': 158}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:00:23,006]\u001b[0m Trial 173 finished with value: 0.6103915717729771 and parameters: {'n_estimators': 4722, 'learning_rate': 0.008254858749865446, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.8266423135079374, 'colsample_bytree': 0.5624957701780068, 'gamma': 6.383897784745788, 'reg_alpha': 16.882757572341355, 'reg_lambda': 25.11442699921325, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 171}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:02:46,369]\u001b[0m Trial 174 finished with value: 0.5921279997333524 and parameters: {'n_estimators': 4606, 'learning_rate': 0.016333171826681415, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.7396944374623037, 'colsample_bytree': 0.6360507289619416, 'gamma': 6.617881838567332, 'reg_alpha': 17.770432389616623, 'reg_lambda': 28.025693266905186, 'max_delta_step': 0, 'grow_policy': 'lossguide', 'max_leaves': 167}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:06:22,321]\u001b[0m Trial 175 finished with value: 0.6024512528914132 and parameters: {'n_estimators': 4101, 'learning_rate': 0.007412799536951686, 'max_depth': 8, 'min_child_weight': 2, 'subsample': 0.8352981609312728, 'colsample_bytree': 0.5310812921602036, 'gamma': 6.354889877046093, 'reg_alpha': 14.589007840335432, 'reg_lambda': 22.702091012774634, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 175}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:10:25,071]\u001b[0m Trial 176 finished with value: 0.604880293293334 and parameters: {'n_estimators': 5495, 'learning_rate': 0.00823245820023483, 'max_depth': 10, 'min_child_weight': 2, 'subsample': 0.853782050567575, 'colsample_bytree': 0.6033073348775654, 'gamma': 7.8345132140207765, 'reg_alpha': 16.971835410969433, 'reg_lambda': 27.942307794432907, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 166}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:14:11,170]\u001b[0m Trial 177 finished with value: 0.5864766986802811 and parameters: {'n_estimators': 5800, 'learning_rate': 0.005655120063611511, 'max_depth': 7, 'min_child_weight': 15, 'subsample': 0.8143236442665501, 'colsample_bytree': 0.5928557034852965, 'gamma': 8.313682871653057, 'reg_alpha': 15.52387326066355, 'reg_lambda': 27.6531399163053, 'max_delta_step': 0, 'grow_policy': 'lossguide', 'max_leaves': 193}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:17:36,785]\u001b[0m Trial 178 finished with value: 0.5940933798240742 and parameters: {'n_estimators': 5461, 'learning_rate': 0.009074093000042019, 'max_depth': 10, 'min_child_weight': 14, 'subsample': 0.9760121363236262, 'colsample_bytree': 0.6663932447150236, 'gamma': 8.51106131662568, 'reg_alpha': 17.17610820180914, 'reg_lambda': 27.83327259800197, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 149}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:20:33,436]\u001b[0m Trial 179 finished with value: 0.5659457734688164 and parameters: {'n_estimators': 4551, 'learning_rate': 0.01358293835110562, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.911090955826334, 'colsample_bytree': 0.5033187384540886, 'gamma': 5.575718866121411, 'reg_alpha': 19.932230535677213, 'reg_lambda': 27.62968216307862, 'max_delta_step': 0, 'grow_policy': 'lossguide', 'max_leaves': 173}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:25:22,807]\u001b[0m Trial 180 finished with value: 0.612629158915323 and parameters: {'n_estimators': 5216, 'learning_rate': 0.005059703137451275, 'max_depth': 9, 'min_child_weight': 11, 'subsample': 0.9155714305093001, 'colsample_bytree': 0.5614350880897079, 'gamma': 5.329063215299537, 'reg_alpha': 19.362981338898045, 'reg_lambda': 25.15102804628077, 'max_delta_step': 6, 'grow_policy': 'lossguide', 'max_leaves': 119}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:27:33,919]\u001b[0m Trial 181 finished with value: 0.6044538568991575 and parameters: {'n_estimators': 4997, 'learning_rate': 0.02966351515969731, 'max_depth': 8, 'min_child_weight': 27, 'subsample': 0.9933222915531864, 'colsample_bytree': 0.5345751764929719, 'gamma': 8.140075207484662, 'reg_alpha': 16.24362208226676, 'reg_lambda': 29.273611518078226, 'max_delta_step': 8, 'grow_policy': 'lossguide', 'max_leaves': 117}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:30:34,085]\u001b[0m Trial 182 finished with value: 0.6112546987916602 and parameters: {'n_estimators': 5409, 'learning_rate': 0.009329913840217238, 'max_depth': 7, 'min_child_weight': 31, 'subsample': 0.9285975057817564, 'colsample_bytree': 0.6801383768961808, 'gamma': 9.838623080936246, 'reg_alpha': 17.812896207502856, 'reg_lambda': 24.367326153810854, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 116}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:33:52,063]\u001b[0m Trial 183 finished with value: 0.5920630245554338 and parameters: {'n_estimators': 4725, 'learning_rate': 0.005069329107911598, 'max_depth': 6, 'min_child_weight': 33, 'subsample': 0.9083365895920102, 'colsample_bytree': 0.7022263077927413, 'gamma': 9.540541818001264, 'reg_alpha': 11.847820515966342, 'reg_lambda': 26.764474226947133, 'max_delta_step': 0, 'grow_policy': 'lossguide', 'max_leaves': 102}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:37:05,072]\u001b[0m Trial 184 finished with value: 0.606830288567543 and parameters: {'n_estimators': 4426, 'learning_rate': 0.006943325511965704, 'max_depth': 9, 'min_child_weight': 19, 'subsample': 0.8777100094582384, 'colsample_bytree': 0.6050517011912686, 'gamma': 6.873308933641927, 'reg_alpha': 19.53889590690687, 'reg_lambda': 19.677738293904106, 'max_delta_step': 8, 'grow_policy': 'lossguide', 'max_leaves': 100}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:40:27,884]\u001b[0m Trial 185 finished with value: 0.5990687511044882 and parameters: {'n_estimators': 5597, 'learning_rate': 0.007207379766487015, 'max_depth': 6, 'min_child_weight': 40, 'subsample': 0.8190261750187998, 'colsample_bytree': 0.6429971320415102, 'gamma': 8.279290529837626, 'reg_alpha': 19.43019749418584, 'reg_lambda': 22.375126094248042, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 126}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:43:42,352]\u001b[0m Trial 186 finished with value: 0.601013731820264 and parameters: {'n_estimators': 5623, 'learning_rate': 0.00848335497141319, 'max_depth': 9, 'min_child_weight': 26, 'subsample': 0.8397392093309941, 'colsample_bytree': 0.5314965036280724, 'gamma': 7.989530712154037, 'reg_alpha': 12.724498019530305, 'reg_lambda': 28.539213225184707, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 54}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:45:03,933]\u001b[0m Trial 187 finished with value: 0.6011365491350009 and parameters: {'n_estimators': 1660, 'learning_rate': 0.013309156507991608, 'max_depth': 10, 'min_child_weight': 28, 'subsample': 0.9598022614788261, 'colsample_bytree': 0.6759553830206378, 'gamma': 5.355879868472539, 'reg_alpha': 17.606935699004477, 'reg_lambda': 9.61509842513476, 'max_delta_step': 5, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:49:05,877]\u001b[0m Trial 188 finished with value: 0.5930949458254555 and parameters: {'n_estimators': 5276, 'learning_rate': 0.0063396758702592496, 'max_depth': 8, 'min_child_weight': 14, 'subsample': 0.6957018292395267, 'colsample_bytree': 0.5995472041472586, 'gamma': 5.3529042440039225, 'reg_alpha': 16.400003559112346, 'reg_lambda': 29.526437627086935, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 127}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:52:06,857]\u001b[0m Trial 189 finished with value: 0.596856652436184 and parameters: {'n_estimators': 5996, 'learning_rate': 0.01480545693278574, 'max_depth': 8, 'min_child_weight': 30, 'subsample': 0.8430452081810449, 'colsample_bytree': 0.80363716658806, 'gamma': 9.76285442491731, 'reg_alpha': 16.788180049072693, 'reg_lambda': 19.621667856773968, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 134}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:53:53,966]\u001b[0m Trial 190 finished with value: 0.6065788701725785 and parameters: {'n_estimators': 3137, 'learning_rate': 0.015758751284068957, 'max_depth': 10, 'min_child_weight': 28, 'subsample': 0.9984652955035153, 'colsample_bytree': 0.576119374681129, 'gamma': 6.065034046486694, 'reg_alpha': 19.095629915644214, 'reg_lambda': 27.04526766066238, 'max_delta_step': 8, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:55:52,597]\u001b[0m Trial 191 finished with value: 0.5945727474272512 and parameters: {'n_estimators': 4327, 'learning_rate': 0.04771043467660784, 'max_depth': 7, 'min_child_weight': 14, 'subsample': 0.9434945573283782, 'colsample_bytree': 0.6441294538224128, 'gamma': 4.897455378915884, 'reg_alpha': 17.453589115587352, 'reg_lambda': 12.440972189711294, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 08:57:24,546]\u001b[0m Trial 192 finished with value: 0.604182206038777 and parameters: {'n_estimators': 3082, 'learning_rate': 0.02459961853675205, 'max_depth': 6, 'min_child_weight': 25, 'subsample': 0.988124465073177, 'colsample_bytree': 0.600456091745014, 'gamma': 9.533062580338523, 'reg_alpha': 19.9732209168472, 'reg_lambda': 28.967457558856033, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 155}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:00:32,823]\u001b[0m Trial 193 finished with value: 0.6081742970940914 and parameters: {'n_estimators': 5855, 'learning_rate': 0.009494052938873211, 'max_depth': 6, 'min_child_weight': 26, 'subsample': 0.9711308804149147, 'colsample_bytree': 0.6415778724775538, 'gamma': 9.247611125455316, 'reg_alpha': 18.391782506178966, 'reg_lambda': 23.94660435735149, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 116}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:03:12,620]\u001b[0m Trial 194 finished with value: 0.6083150456604414 and parameters: {'n_estimators': 5838, 'learning_rate': 0.01924333820768716, 'max_depth': 7, 'min_child_weight': 35, 'subsample': 0.942789480079797, 'colsample_bytree': 0.5884722642867319, 'gamma': 9.244561409260674, 'reg_alpha': 17.968520776532493, 'reg_lambda': 25.01870258229084, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 98}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:07:55,417]\u001b[0m Trial 195 finished with value: 0.5964193380941738 and parameters: {'n_estimators': 5477, 'learning_rate': 0.007924044503098722, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.8898101725534702, 'colsample_bytree': 0.6805168360349916, 'gamma': 2.1875037508219464, 'reg_alpha': 18.603196009288226, 'reg_lambda': 22.69574005509582, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 98}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:10:33,594]\u001b[0m Trial 196 finished with value: 0.6009152887567734 and parameters: {'n_estimators': 5662, 'learning_rate': 0.019521287599413597, 'max_depth': 8, 'min_child_weight': 29, 'subsample': 0.9842153065631497, 'colsample_bytree': 0.5967627422318768, 'gamma': 8.117328479956566, 'reg_alpha': 17.61489032319216, 'reg_lambda': 28.142263638773077, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 82}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:13:46,000]\u001b[0m Trial 197 finished with value: 0.5941303814311176 and parameters: {'n_estimators': 5322, 'learning_rate': 0.007473118854696046, 'max_depth': 9, 'min_child_weight': 30, 'subsample': 0.9949385199810115, 'colsample_bytree': 0.6371442980372898, 'gamma': 8.041228758951236, 'reg_alpha': 9.292475579406315, 'reg_lambda': 27.94975855032814, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 162}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:17:39,676]\u001b[0m Trial 198 finished with value: 0.6068704611605749 and parameters: {'n_estimators': 5916, 'learning_rate': 0.006677741125069276, 'max_depth': 8, 'min_child_weight': 17, 'subsample': 0.9427494441430697, 'colsample_bytree': 0.5614962828546601, 'gamma': 6.2392605148129885, 'reg_alpha': 19.34621107668821, 'reg_lambda': 27.988970611351323, 'max_delta_step': 3, 'grow_policy': 'lossguide', 'max_leaves': 114}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:20:12,017]\u001b[0m Trial 199 finished with value: 0.5878663518707448 and parameters: {'n_estimators': 5857, 'learning_rate': 0.03317364161057045, 'max_depth': 6, 'min_child_weight': 28, 'subsample': 0.887527061375823, 'colsample_bytree': 0.5980680910402871, 'gamma': 7.137088872426725, 'reg_alpha': 14.562727247381169, 'reg_lambda': 16.14418303216317, 'max_delta_step': 8, 'grow_policy': 'lossguide', 'max_leaves': 199}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:24:18,752]\u001b[0m Trial 200 finished with value: 0.598322346986639 and parameters: {'n_estimators': 4282, 'learning_rate': 0.0071303838210116394, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.9181144950119493, 'colsample_bytree': 0.5067500490419219, 'gamma': 2.683101417960119, 'reg_alpha': 19.106738164826297, 'reg_lambda': 20.67265137472674, 'max_delta_step': 5, 'grow_policy': 'lossguide', 'max_leaves': 148}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:26:57,854]\u001b[0m Trial 201 finished with value: 0.5871663239761495 and parameters: {'n_estimators': 5262, 'learning_rate': 0.008508648139522724, 'max_depth': 4, 'min_child_weight': 31, 'subsample': 0.9812009155397298, 'colsample_bytree': 0.8237432865472343, 'gamma': 9.721207046041815, 'reg_alpha': 19.4166440299607, 'reg_lambda': 27.852057606237068, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 109}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:31:17,254]\u001b[0m Trial 202 finished with value: 0.6068771292960109 and parameters: {'n_estimators': 4857, 'learning_rate': 0.007194740984932189, 'max_depth': 10, 'min_child_weight': 3, 'subsample': 0.9269022985169915, 'colsample_bytree': 0.6130380209383866, 'gamma': 4.952371817389043, 'reg_alpha': 17.91600793726041, 'reg_lambda': 26.23708316821035, 'max_delta_step': 7, 'grow_policy': 'lossguide', 'max_leaves': 107}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:34:12,930]\u001b[0m Trial 203 finished with value: 0.6081997284802834 and parameters: {'n_estimators': 5451, 'learning_rate': 0.009638158722605622, 'max_depth': 6, 'min_child_weight': 32, 'subsample': 0.9528532622541698, 'colsample_bytree': 0.6099703981588116, 'gamma': 9.746688989226502, 'reg_alpha': 18.622439641012438, 'reg_lambda': 19.617174832609265, 'max_delta_step': 1, 'grow_policy': 'lossguide', 'max_leaves': 124}. Best is trial 140 with value: 0.6127042010241295.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:36:34,305]\u001b[0m Trial 204 finished with value: 0.6134734232399863 and parameters: {'n_estimators': 4770, 'learning_rate': 0.009408348066891026, 'max_depth': 5, 'min_child_weight': 38, 'subsample': 0.9580408244820326, 'colsample_bytree': 0.5859885271647445, 'gamma': 8.679259249940205, 'reg_alpha': 17.53744145401043, 'reg_lambda': 24.224933334472816, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:38:28,535]\u001b[0m Trial 205 finished with value: 0.5973010963239418 and parameters: {'n_estimators': 3811, 'learning_rate': 0.008350173013484993, 'max_depth': 4, 'min_child_weight': 32, 'subsample': 0.990503737523955, 'colsample_bytree': 0.6058965834018613, 'gamma': 9.705826208026552, 'reg_alpha': 14.708363650921715, 'reg_lambda': 14.72509401064839, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:42:07,394]\u001b[0m Trial 206 finished with value: 0.5996737272929602 and parameters: {'n_estimators': 4542, 'learning_rate': 0.005183218106102345, 'max_depth': 8, 'min_child_weight': 24, 'subsample': 0.7798448794998531, 'colsample_bytree': 0.6655106527103419, 'gamma': 7.721083294830118, 'reg_alpha': 19.722585549800975, 'reg_lambda': 20.86908721175773, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 185}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:43:23,288]\u001b[0m Trial 207 finished with value: 0.5943489007517374 and parameters: {'n_estimators': 2680, 'learning_rate': 0.0526534074944043, 'max_depth': 9, 'min_child_weight': 12, 'subsample': 0.9681741524443845, 'colsample_bytree': 0.5569200466224464, 'gamma': 6.939192476632721, 'reg_alpha': 18.498261735091887, 'reg_lambda': 29.23756728593564, 'max_delta_step': 6, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:46:14,322]\u001b[0m Trial 208 finished with value: 0.5962895123463234 and parameters: {'n_estimators': 4789, 'learning_rate': 0.00610740069405861, 'max_depth': 5, 'min_child_weight': 36, 'subsample': 0.7954518258397092, 'colsample_bytree': 0.6558754153857667, 'gamma': 8.724312040228037, 'reg_alpha': 13.15353401827945, 'reg_lambda': 20.311468145386307, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:47:59,721]\u001b[0m Trial 209 finished with value: 0.5927463736811104 and parameters: {'n_estimators': 5226, 'learning_rate': 0.03556383801517009, 'max_depth': 3, 'min_child_weight': 33, 'subsample': 0.979781139137697, 'colsample_bytree': 0.6857070567368895, 'gamma': 9.577060923105645, 'reg_alpha': 19.825277563051582, 'reg_lambda': 24.984752133922594, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:50:31,302]\u001b[0m Trial 210 finished with value: 0.6037065797669934 and parameters: {'n_estimators': 4375, 'learning_rate': 0.010308828073787872, 'max_depth': 7, 'min_child_weight': 31, 'subsample': 0.9359213150643828, 'colsample_bytree': 0.6731896832965223, 'gamma': 8.67482515286119, 'reg_alpha': 19.596675845204665, 'reg_lambda': 26.482821895991943, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:53:39,716]\u001b[0m Trial 211 finished with value: 0.5899557784251325 and parameters: {'n_estimators': 5195, 'learning_rate': 0.008535286142976965, 'max_depth': 5, 'min_child_weight': 16, 'subsample': 0.8850135504241281, 'colsample_bytree': 0.671501655051745, 'gamma': 3.6461009865375047, 'reg_alpha': 15.434184804310705, 'reg_lambda': 19.933871898330437, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:56:22,370]\u001b[0m Trial 212 finished with value: 0.6090924232014687 and parameters: {'n_estimators': 3679, 'learning_rate': 0.00656080593400991, 'max_depth': 6, 'min_child_weight': 12, 'subsample': 0.7915804710209715, 'colsample_bytree': 0.584657081856703, 'gamma': 9.986183741024108, 'reg_alpha': 12.914700873715883, 'reg_lambda': 19.29085048177241, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 137}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 09:58:43,755]\u001b[0m Trial 213 finished with value: 0.6127961495963463 and parameters: {'n_estimators': 5419, 'learning_rate': 0.0123743646861521, 'max_depth': 5, 'min_child_weight': 31, 'subsample': 0.9924400126324164, 'colsample_bytree': 0.5623880740064892, 'gamma': 8.954175526020956, 'reg_alpha': 16.481316123756244, 'reg_lambda': 26.524446364338893, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:00:48,842]\u001b[0m Trial 214 finished with value: 0.6012946141779923 and parameters: {'n_estimators': 3673, 'learning_rate': 0.01905359510511559, 'max_depth': 10, 'min_child_weight': 8, 'subsample': 0.9623851581101044, 'colsample_bytree': 0.5579537416131968, 'gamma': 4.7658288191230165, 'reg_alpha': 17.43403976769362, 'reg_lambda': 17.611093920675703, 'max_delta_step': 5, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:03:25,616]\u001b[0m Trial 215 finished with value: 0.6021908634985114 and parameters: {'n_estimators': 3363, 'learning_rate': 0.006108087100821562, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.7504984863633698, 'colsample_bytree': 0.5717864479135456, 'gamma': 9.217435834988722, 'reg_alpha': 16.34323619395135, 'reg_lambda': 14.917705220356387, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 159}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:06:03,948]\u001b[0m Trial 216 finished with value: 0.6066325796509423 and parameters: {'n_estimators': 5294, 'learning_rate': 0.006311154628250301, 'max_depth': 4, 'min_child_weight': 31, 'subsample': 0.9643204760840066, 'colsample_bytree': 0.5557118822901462, 'gamma': 8.506002429033657, 'reg_alpha': 19.364866657389438, 'reg_lambda': 18.38906812155472, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:08:05,421]\u001b[0m Trial 217 finished with value: 0.5872470263550185 and parameters: {'n_estimators': 3994, 'learning_rate': 0.012645575137354932, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8755269593234157, 'colsample_bytree': 0.8201656315078422, 'gamma': 9.997404009645713, 'reg_alpha': 12.922394327144781, 'reg_lambda': 19.578740030882436, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 155}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:10:07,889]\u001b[0m Trial 218 finished with value: 0.5985518469466642 and parameters: {'n_estimators': 4272, 'learning_rate': 0.02373809500080088, 'max_depth': 8, 'min_child_weight': 30, 'subsample': 0.969148610833385, 'colsample_bytree': 0.6629744777410743, 'gamma': 7.69990831491452, 'reg_alpha': 18.872184319548907, 'reg_lambda': 13.144544871275654, 'max_delta_step': 9, 'grow_policy': 'lossguide', 'max_leaves': 207}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:12:25,341]\u001b[0m Trial 219 finished with value: 0.5894506628813355 and parameters: {'n_estimators': 3765, 'learning_rate': 0.01913360616266114, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.8226530418601969, 'colsample_bytree': 0.7051566351961484, 'gamma': 7.547463135088824, 'reg_alpha': 18.042198993512216, 'reg_lambda': 18.259294705708587, 'max_delta_step': 6, 'grow_policy': 'lossguide', 'max_leaves': 169}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:15:20,927]\u001b[0m Trial 220 finished with value: 0.6105959048170368 and parameters: {'n_estimators': 5860, 'learning_rate': 0.012888692188744872, 'max_depth': 6, 'min_child_weight': 31, 'subsample': 0.9383066404310637, 'colsample_bytree': 0.5842930751203683, 'gamma': 7.546925829162415, 'reg_alpha': 15.6570326052795, 'reg_lambda': 25.92410954835121, 'max_delta_step': 3, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:16:55,383]\u001b[0m Trial 221 finished with value: 0.5843297480403133 and parameters: {'n_estimators': 2063, 'learning_rate': 0.009963427839697128, 'max_depth': 7, 'min_child_weight': 12, 'subsample': 0.7365420710865259, 'colsample_bytree': 0.5267825643534157, 'gamma': 9.266730561120031, 'reg_alpha': 12.87333913237835, 'reg_lambda': 24.22605565358507, 'max_delta_step': 0, 'grow_policy': 'lossguide', 'max_leaves': 152}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:18:57,182]\u001b[0m Trial 222 finished with value: 0.5979944956330076 and parameters: {'n_estimators': 4679, 'learning_rate': 0.013348261237155884, 'max_depth': 5, 'min_child_weight': 29, 'subsample': 0.9952066854960542, 'colsample_bytree': 0.5042580967830158, 'gamma': 9.100288427700821, 'reg_alpha': 16.68271900043277, 'reg_lambda': 29.489398570032524, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:21:09,072]\u001b[0m Trial 223 finished with value: 0.5984886944917249 and parameters: {'n_estimators': 5041, 'learning_rate': 0.0370462219282627, 'max_depth': 7, 'min_child_weight': 36, 'subsample': 0.9345223370910579, 'colsample_bytree': 0.6734932110100453, 'gamma': 8.81253797449469, 'reg_alpha': 11.62324463899721, 'reg_lambda': 19.376627176679882, 'max_delta_step': 2, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:23:38,895]\u001b[0m Trial 224 finished with value: 0.6049041442914872 and parameters: {'n_estimators': 5554, 'learning_rate': 0.013805972292293724, 'max_depth': 6, 'min_child_weight': 40, 'subsample': 0.9224309126832013, 'colsample_bytree': 0.6208841883022993, 'gamma': 8.996941184397334, 'reg_alpha': 12.783928466046634, 'reg_lambda': 24.642419621051285, 'max_delta_step': 1, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:27:04,334]\u001b[0m Trial 225 finished with value: 0.6092741685064631 and parameters: {'n_estimators': 5059, 'learning_rate': 0.008691087317587628, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.950821705920374, 'colsample_bytree': 0.5317605108526043, 'gamma': 8.343668290405837, 'reg_alpha': 17.870989520353582, 'reg_lambda': 22.13035276705471, 'max_delta_step': 2, 'grow_policy': 'lossguide', 'max_leaves': 193}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:29:49,489]\u001b[0m Trial 226 finished with value: 0.5961063522449603 and parameters: {'n_estimators': 5289, 'learning_rate': 0.007494117775670846, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.7183134875769704, 'colsample_bytree': 0.5062525187931889, 'gamma': 9.352575781113181, 'reg_alpha': 12.521151612512634, 'reg_lambda': 29.488625176605595, 'max_delta_step': 4, 'grow_policy': 'lossguide', 'max_leaves': 210}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n",
      "\u001b[32m[I 2026-01-23 10:32:12,972]\u001b[0m Trial 227 finished with value: 0.6028134813662366 and parameters: {'n_estimators': 5673, 'learning_rate': 0.014774201749756572, 'max_depth': 5, 'min_child_weight': 28, 'subsample': 0.9113289916372955, 'colsample_bytree': 0.5276520757258957, 'gamma': 9.309651136397258, 'reg_alpha': 16.826766132388805, 'reg_lambda': 26.597350657212104, 'max_delta_step': 4, 'grow_policy': 'depthwise'}. Best is trial 204 with value: 0.6134734232399863.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna best AP: 0.6134734232399863\n",
      "Best params:\n",
      "n_estimators = 4770\n",
      "learning_rate = 0.009408348066891026\n",
      "max_depth = 5\n",
      "min_child_weight = 38\n",
      "subsample = 0.9580408244820326\n",
      "colsample_bytree = 0.5859885271647445\n",
      "gamma = 8.679259249940205\n",
      "reg_alpha = 17.53744145401043\n",
      "reg_lambda = 24.224933334472816\n",
      "max_delta_step = 2\n",
      "grow_policy = depthwise\n"
     ]
    }
   ],
   "source": [
    "train_feat, test_feat = add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10)\n",
    "best_xgb_params = run_optuna_xgb(train_feat, n_folds_tune=10, timeout_sec=28800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3a337",
   "metadata": {},
   "source": [
    "LGBM Classifier is trained to predict SpecTypeGroup. A kaggle user pointed out this method. To focus on light curve features that distinguish TDE from SN and AGN. Becaus SpecType is only in the train set and not in the test set, a seperate model is trained to predict SpecType to create additional features.\n",
    "Trial 204 finished with value: 0.6134734232399863 and parameters: {'n_estimators': 4770, 'learning_rate': 0.009408348066891026, 'max_depth': 5, 'min_child_weight': 38, 'subsample': 0.9580408244820326, 'colsample_bytree': 0.5859885271647445, 'gamma': 8.679259249940205, 'reg_alpha': 17.53744145401043, 'reg_lambda': 24.224933334472816, 'max_delta_step': 2, 'grow_policy': 'depthwise'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c4b94da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 | temp blend(0.5) best F1=0.7500 @ th=0.502\n",
      "Fold 02 | temp blend(0.5) best F1=0.5714 @ th=0.143\n",
      "Fold 03 | temp blend(0.5) best F1=0.6061 @ th=0.202\n",
      "Fold 04 | temp blend(0.5) best F1=0.0000 @ th=0.010\n",
      "Fold 05 | temp blend(0.5) best F1=0.5455 @ th=0.305\n",
      "Fold 06 | temp blend(0.5) best F1=0.6250 @ th=0.463\n",
      "Fold 07 | temp blend(0.5) best F1=0.7368 @ th=0.301\n",
      "Fold 08 | temp blend(0.5) best F1=0.6667 @ th=0.143\n",
      "Fold 09 | temp blend(0.5) best F1=0.8333 @ th=0.562\n",
      "Fold 10 | temp blend(0.5) best F1=0.9231 @ th=0.438\n",
      "Fold 11 | temp blend(0.5) best F1=0.3922 @ th=0.246\n",
      "Fold 12 | temp blend(0.5) best F1=0.7500 @ th=0.315\n",
      "Fold 13 | temp blend(0.5) best F1=0.6957 @ th=0.291\n",
      "Fold 14 | temp blend(0.5) best F1=0.5714 @ th=0.148\n",
      "Fold 15 | temp blend(0.5) best F1=0.6875 @ th=0.355\n",
      "Fold 16 | temp blend(0.5) best F1=0.0000 @ th=0.010\n",
      "Fold 17 | temp blend(0.5) best F1=0.6154 @ th=0.399\n",
      "Fold 18 | temp blend(0.5) best F1=0.5000 @ th=0.566\n",
      "Fold 19 | temp blend(0.5) best F1=0.8980 @ th=0.443\n",
      "Fold 20 | temp blend(0.5) best F1=0.5974 @ th=0.079\n",
      "\n",
      "OOF best alpha: 0.2\n",
      "OOF best threshold: 0.18728643216080404\n",
      "OOF blended best F1: 0.5798816568047337\n"
     ]
    }
   ],
   "source": [
    "xgb_models, lgb_models, alpha_best, best_th = train_full_ensemble(\n",
    "    train_feat, best_xgb_params, n_splits_full=len(train_splits)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52416d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGB-LGBM-2.csv | alpha: 0.2 | threshold: 0.18728643216080404\n"
     ]
    }
   ],
   "source": [
    "test_probs = predict_ensemble(test_feat, xgb_models, lgb_models, alpha=alpha_best)\n",
    "test_pred = (test_probs > best_th).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"object_id\": test_feat[\"object_id\"].values,\n",
    "    \"target\": test_pred\n",
    "})\n",
    "sub.to_csv(\"XGB-LGBM-2.csv\", index=False)\n",
    "print(\"Saved XGB-LGBM-2.csv | alpha:\", alpha_best, \"| threshold:\", best_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de6f653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 | XGB best F1=0.8571 @ th=0.571\n",
      "Fold 02 | XGB best F1=0.4000 @ th=0.537\n",
      "Fold 03 | XGB best F1=0.4444 @ th=0.335\n",
      "Fold 04 | XGB best F1=0.0000 @ th=0.010\n",
      "Fold 05 | XGB best F1=0.5000 @ th=0.488\n",
      "Fold 06 | XGB best F1=0.6829 @ th=0.463\n",
      "Fold 07 | XGB best F1=0.6667 @ th=0.468\n",
      "Fold 08 | XGB best F1=0.5333 @ th=0.389\n",
      "Fold 09 | XGB best F1=0.6667 @ th=0.517\n",
      "Fold 10 | XGB best F1=0.6667 @ th=0.586\n",
      "Fold 11 | XGB best F1=0.3846 @ th=0.207\n",
      "Fold 12 | XGB best F1=0.6000 @ th=0.246\n",
      "Fold 13 | XGB best F1=0.7660 @ th=0.502\n",
      "Fold 14 | XGB best F1=0.6753 @ th=0.374\n",
      "Fold 15 | XGB best F1=0.6875 @ th=0.547\n",
      "Fold 16 | XGB best F1=0.0000 @ th=0.010\n",
      "Fold 17 | XGB best F1=0.5000 @ th=0.251\n",
      "Fold 18 | XGB best F1=0.5714 @ th=0.670\n",
      "Fold 19 | XGB best F1=0.8000 @ th=0.424\n",
      "Fold 20 | XGB best F1=0.5405 @ th=0.286\n",
      "\n",
      "OOF XGB best threshold: 0.46798994974874375\n",
      "OOF XGB best F1: 0.5531914893617021\n",
      "Saved XGB-only2.csv | threshold: 0.46798994974874375\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "groups = train_feat[\"split\"].to_numpy()\n",
    "X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"])\n",
    "\n",
    "X_test = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"])\n",
    "\n",
    "xgb_base = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"random_state\": 67,\n",
    "    \"n_jobs\": -1,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"n_estimators\" : 4770,\n",
    "    \"learning_rate\" : 0.009408348066891026,\n",
    "    \"max_depth\" : 5,\n",
    "    \"min_child_weight\" : 38,\n",
    "    \"subsample\" : 0.9580408244820326,\n",
    "    \"colsample_bytree\" : 0.5859885271647445,\n",
    "    \"gamma\" : 8.679259249940205,\n",
    "    \"reg_alpha\" : 17.53744145401043,\n",
    "    \"reg_lambda\" : 24.224933334472816,\n",
    "    \"max_delta_step\" : 2,\n",
    "    \"grow_policy\" : \"depthwise\"\n",
    "}\n",
    "\n",
    "splitter = make_splitter(n_splits=len(train_splits), random_state=6)\n",
    "split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "oof = np.zeros(len(X), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "    neg = np.sum(y_tr == 0)\n",
    "    pos = np.sum(y_tr == 1)\n",
    "    spw = float(neg / max(1, pos))\n",
    "\n",
    "    model = XGBClassifier(**{**xgb_base, \"scale_pos_weight\": spw})\n",
    "    model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "    oof[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    th, f1 = best_threshold_f1(y_va, oof[va_idx])\n",
    "    print(f\"Fold {fold:02d} | XGB best F1={f1:.4f} @ th={th:.3f}\")\n",
    "\n",
    "best_th, best_f1 = best_threshold_f1(y, oof)\n",
    "print(\"\\nOOF XGB best threshold:\", best_th)\n",
    "print(\"OOF XGB best F1:\", best_f1)\n",
    "\n",
    "neg = np.sum(y == 0)\n",
    "pos = np.sum(y == 1)\n",
    "spw_full = float(neg / max(1, pos))\n",
    "\n",
    "final_model = XGBClassifier(**{**xgb_base, \"scale_pos_weight\": spw_full})\n",
    "final_model.fit(X, y, verbose=False)\n",
    "\n",
    "test_probs = final_model.predict_proba(X_test)[:, 1]\n",
    "test_pred = (test_probs > best_th).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"object_id\": test_feat[\"object_id\"].values,\n",
    "    \"target\": test_pred\n",
    "})\n",
    "sub.to_csv(\"XGB-only3.csv\", index=False)\n",
    "print(\"Saved XGB-only2.csv | threshold:\", best_th)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
