{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e8445a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40374204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "from extinction import fitzpatrick99\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "from scipy.optimize import curve_fit\n",
    "import xgboost as xgb\n",
    "\n",
    "FILTERS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "EFF_WL_AA = {\n",
    "    \"u\": 3641.0,\n",
    "    \"g\": 4704.0,\n",
    "    \"r\": 6155.0,\n",
    "    \"i\": 7504.0,\n",
    "    \"z\": 8695.0,\n",
    "    \"y\": 10056.0,\n",
    "}\n",
    "\n",
    "R_V = 3.1\n",
    "PRE_BASE_FRAC = 0.20\n",
    "MIN_BAND_POINTS = 5\n",
    "PEAK_SIGMA_K = 3.0\n",
    "REBRIGHT_FRAC = 0.30\n",
    "EPS = 1e-8\n",
    "\n",
    "SEASON_GAP_DAYS = 90.0\n",
    "\n",
    "SF_LAGS = [5.0, 10.0, 20.0, 50.0, 100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b306342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return default\n",
    "        x = float(x)\n",
    "        if np.isnan(x):\n",
    "            return default\n",
    "        return x\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def trapz_safe(y, x):\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return float(np.trapezoid(y, x))\n",
    "    y = np.asarray(y)\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    return float(np.sum((x[1:] - x[:-1]) * (y[1:] + y[:-1]) * 0.5))\n",
    "\n",
    "\n",
    "def median_abs_dev(x):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    med = np.median(x)\n",
    "    return float(np.median(np.abs(x - med)))\n",
    "\n",
    "\n",
    "def iqr(x):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    q75, q25 = np.percentile(x, [75, 25])\n",
    "    return float(q75 - q25)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m3 = np.mean((x - mu) ** 3)\n",
    "    return float(m3 / (s ** 3))\n",
    "\n",
    "\n",
    "def kurtosis_excess(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m4 = np.mean((x - mu) ** 4)\n",
    "    return float(m4 / (s ** 4) - 3.0)\n",
    "\n",
    "\n",
    "def von_neumann_eta(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    v = np.var(x)\n",
    "    if v < 1e-12:\n",
    "        return 0.0\n",
    "    dif = np.diff(x)\n",
    "    return float(np.mean(dif ** 2) / v)\n",
    "\n",
    "\n",
    "def max_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.max(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def median_abs_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.median(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def linear_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    try:\n",
    "        a, b = np.polyfit(t, f, 1)\n",
    "        return float(a)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def chi2_to_constant(f, ferr):\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.median(f)\n",
    "    denom = (ferr + EPS) ** 2\n",
    "    chi2 = np.sum((f - mu) ** 2 / denom)\n",
    "    dof = max(1, n - 1)\n",
    "    return float(chi2 / dof)\n",
    "\n",
    "\n",
    "def interp_flux_at_time(tb, fb, t0):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan\n",
    "    if (t0 < tb.min()) or (t0 > tb.max()):\n",
    "        return np.nan\n",
    "    return float(np.interp(t0, tb, fb))\n",
    "\n",
    "\n",
    "def interp_err_at_time(tb, eb, t0):\n",
    "    tb = np.asarray(tb)\n",
    "    eb = np.asarray(eb)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan\n",
    "    if (t0 < tb.min()) or (t0 > tb.max()):\n",
    "        return np.nan\n",
    "    return float(np.interp(t0, tb, eb))\n",
    "\n",
    "\n",
    "def fractional_variability(f, ferr):\n",
    "    f = np.asarray(f, float)\n",
    "    ferr = np.asarray(ferr, float)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.mean(f)\n",
    "    if np.abs(mu) < 1e-8:\n",
    "        return np.nan\n",
    "    s2 = np.var(f, ddof=1)\n",
    "    mean_err2 = np.mean(ferr ** 2)\n",
    "    excess = max(0.0, s2 - mean_err2)\n",
    "    return float(np.sqrt(excess) / np.abs(mu))\n",
    "\n",
    "\n",
    "def stetson_J_consecutive(t, f, ferr):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(t)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "    mu = np.mean(f)\n",
    "    scale = np.sqrt(n / max(1, n - 1))\n",
    "    delta = scale * (f - mu) / (ferr + EPS)\n",
    "    vals = []\n",
    "    for i in range(n - 1):\n",
    "        P = delta[i] * delta[i + 1]\n",
    "        vals.append(np.sign(P) * np.sqrt(np.abs(P)))\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "\n",
    "def pre_peak_baseline(tb, fb, eb, frac=PRE_BASE_FRAC):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(tb)\n",
    "    if n < 3:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    k = max(2, int(np.ceil(frac * n)))\n",
    "    k = min(k, n)\n",
    "    base = float(np.median(fb[:k]))\n",
    "    mad_pre = median_abs_dev(fb[:k])\n",
    "    mederr_pre = float(np.median(eb[:k])) if k > 0 else np.nan\n",
    "    return base, mad_pre, mederr_pre\n",
    "\n",
    "\n",
    "def count_significant_peaks(tb, fb, eb, baseline_pre, k_sigma=PEAK_SIGMA_K):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(fb)\n",
    "    if n < 5:\n",
    "        return 0\n",
    "    mederr = float(np.median(eb)) if np.isfinite(np.median(eb)) else 0.0\n",
    "    thresh = baseline_pre + k_sigma * mederr\n",
    "    peaks = 0\n",
    "    for i in range(1, n - 1):\n",
    "        if (fb[i] > fb[i - 1]) and (fb[i] > fb[i + 1]) and (fb[i] > thresh):\n",
    "            peaks += 1\n",
    "    return int(peaks)\n",
    "\n",
    "\n",
    "def postpeak_monotonicity(tb, fb, pidx):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return np.nan\n",
    "    t2 = tb[pidx:]\n",
    "    f2 = fb[pidx:]\n",
    "    if len(f2) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t2)\n",
    "    df = np.diff(f2)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    frac_neg = float(np.mean((df[good] / dt[good]) < 0))\n",
    "    return frac_neg\n",
    "\n",
    "\n",
    "def count_rebrighten(tb, fb, baseline_pre, amp, pidx, frac=REBRIGHT_FRAC):\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return 0\n",
    "    level = baseline_pre + frac * amp\n",
    "    post = fb[pidx:]\n",
    "    if len(post) < 3:\n",
    "        return 0\n",
    "    above = post > level\n",
    "    crossings = np.sum((~above[:-1]) & (above[1:]))\n",
    "    return int(crossings)\n",
    "\n",
    "\n",
    "def fall_time_to_level(tb, fb, baseline_pre, amp, pidx, frac):\n",
    "    if amp <= 0 or pidx is None:\n",
    "        return np.nan\n",
    "    level = baseline_pre + frac * amp\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "    if len(f_dec) < 2:\n",
    "        return np.nan\n",
    "    idx = np.where(f_dec <= level)[0]\n",
    "    if len(idx) == 0:\n",
    "        return np.nan\n",
    "    return float(t_dec[idx[0]] - t_dec[0])\n",
    "\n",
    "\n",
    "def rise_time_to_level(tb, fb, baseline_pre, amp, pidx, frac):\n",
    "    if amp <= 0 or pidx is None or pidx < 2:\n",
    "        return np.nan\n",
    "    level = baseline_pre + frac * amp\n",
    "    t_pre = tb[:pidx + 1]\n",
    "    f_pre = fb[:pidx + 1]\n",
    "    idx = np.where(f_pre >= level)[0]\n",
    "    if len(idx) == 0:\n",
    "        return np.nan\n",
    "    return float(t_pre[-1] - t_pre[idx[0]])\n",
    "\n",
    "\n",
    "def decay_powerlaw_fit(tb, fb, baseline_pre, pidx, tmax=300.0):\n",
    "    if pidx is None or pidx >= len(fb) - 3:\n",
    "        return np.nan, np.nan, 0\n",
    "    t0 = tb[pidx]\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "    dt = t_dec - t0\n",
    "    m = (dt > 0.0) & (dt <= tmax)\n",
    "    dt = dt[m]\n",
    "    fd = f_dec[m] - baseline_pre\n",
    "    m2 = fd > 0.0\n",
    "    dt = dt[m2]\n",
    "    fd = fd[m2]\n",
    "    if len(dt) < 4:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "    x = np.log(dt + EPS)\n",
    "    y = np.log(fd + EPS)\n",
    "    try:\n",
    "        b, a = np.polyfit(x, y, 1)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "    yhat = a + b * x\n",
    "    ss_res = float(np.sum((y - yhat) ** 2))\n",
    "    ss_tot = float(np.sum((y - np.mean(y)) ** 2)) + EPS\n",
    "    r2 = 1.0 - ss_res / ss_tot\n",
    "    return float(b), float(r2), int(len(dt))\n",
    "\n",
    "\n",
    "def signed_log1p(x):\n",
    "    x = float(x)\n",
    "    return float(np.sign(x) * np.log1p(np.abs(x)))\n",
    "\n",
    "\n",
    "def deextinct_band(flux, flux_err, ebv, band, r_v=R_V):\n",
    "    if ebv is None or (isinstance(ebv, float) and np.isnan(ebv)):\n",
    "        return flux, flux_err\n",
    "    A_V = float(ebv) * float(r_v)\n",
    "    wave = np.array([EFF_WL_AA[band]], dtype=float)\n",
    "    A_lambda = float(fitzpatrick99(wave, A_V, r_v=r_v, unit=\"aa\")[0])\n",
    "    fac = 10.0 ** (0.4 * A_lambda)\n",
    "    return flux * fac, flux_err * fac\n",
    "\n",
    "\n",
    "def deextinct_lightcurve(lc, ebv):\n",
    "    flux = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    ferr = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "    flux_corr = flux.copy()\n",
    "    ferr_corr = ferr.copy()\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        flux_corr[m], ferr_corr[m] = deextinct_band(flux_corr[m], ferr_corr[m], ebv, b)\n",
    "    return flux_corr, ferr_corr\n",
    "\n",
    "\n",
    "def band_corr(tt_a, ff_a, tt_b, ff_b, n_grid=30):\n",
    "    tt_a = np.asarray(tt_a, float)\n",
    "    ff_a = np.asarray(ff_a, float)\n",
    "    tt_b = np.asarray(tt_b, float)\n",
    "    ff_b = np.asarray(ff_b, float)\n",
    "\n",
    "    if len(tt_a) < 3 or len(tt_b) < 3:\n",
    "        return np.nan\n",
    "\n",
    "    tmin = max(tt_a.min(), tt_b.min())\n",
    "    tmax = min(tt_a.max(), tt_b.max())\n",
    "    if (tmax - tmin) < 5.0:\n",
    "        return np.nan\n",
    "\n",
    "    grid = np.linspace(tmin, tmax, n_grid)\n",
    "    fa = np.interp(grid, tt_a, ff_a)\n",
    "    fb = np.interp(grid, tt_b, ff_b)\n",
    "\n",
    "    sa = np.std(fa)\n",
    "    sb = np.std(fb)\n",
    "    if sa < 1e-12 or sb < 1e-12:\n",
    "        return 0.0\n",
    "    return float(np.corrcoef(fa, fb)[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347fa589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality_features(tb):\n",
    "    tb = np.asarray(tb, float)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    dt = np.diff(tb)\n",
    "    breaks = np.where(dt > SEASON_GAP_DAYS)[0]\n",
    "    seg_starts = [0] + (breaks + 1).tolist()\n",
    "    seg_ends = breaks.tolist() + [len(tb) - 1]\n",
    "    spans = []\n",
    "    for s, e in zip(seg_starts, seg_ends):\n",
    "        spans.append(tb[e] - tb[s])\n",
    "    spans = np.asarray(spans, float)\n",
    "    n_seasons = float(len(spans))\n",
    "    return n_seasons, float(np.max(spans)), float(np.mean(spans))\n",
    "\n",
    "\n",
    "def structure_function_lags(tb, fb, lags=SF_LAGS):\n",
    "    tb = np.asarray(tb, float)\n",
    "    fb = np.asarray(fb, float)\n",
    "    n = len(tb)\n",
    "    out = {}\n",
    "    if n < 6:\n",
    "        for lag in lags:\n",
    "            out[f\"sf_medabs_{int(lag)}\"] = np.nan\n",
    "            out[f\"sf_n_{int(lag)}\"] = 0.0\n",
    "        return out\n",
    "\n",
    "    for lag in lags:\n",
    "        tol = max(2.0, 0.2 * lag)\n",
    "        vals = []\n",
    "        for i in range(n - 1):\n",
    "            dt = tb[i + 1:] - tb[i]\n",
    "            m = (dt >= (lag - tol)) & (dt <= (lag + tol))\n",
    "            if np.any(m):\n",
    "                dif = np.abs(fb[i + 1:][m] - fb[i])\n",
    "                vals.extend(dif.tolist())\n",
    "        if len(vals) == 0:\n",
    "            out[f\"sf_medabs_{int(lag)}\"] = np.nan\n",
    "            out[f\"sf_n_{int(lag)}\"] = 0.0\n",
    "        else:\n",
    "            out[f\"sf_medabs_{int(lag)}\"] = float(np.median(vals))\n",
    "            out[f\"sf_n_{int(lag)}\"] = float(len(vals))\n",
    "    return out\n",
    "\n",
    "\n",
    "def peak_vs_wavelength_slope(tpeak_by_band, val_by_band, z=0.0):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in FILTERS:\n",
    "        v = val_by_band.get(b, np.nan)\n",
    "        t = tpeak_by_band.get(b, np.nan)\n",
    "        if np.isfinite(v):\n",
    "            lam = float(EFF_WL_AA[b] / (1.0 + float(z)))\n",
    "            xs.append(lam)\n",
    "            ys.append(float(v))\n",
    "    xs = np.asarray(xs, float)\n",
    "    ys = np.asarray(ys, float)\n",
    "    if len(xs) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    try:\n",
    "        slope, intercept = np.polyfit(xs, ys, 1)\n",
    "        yhat = slope * xs + intercept\n",
    "        ss_res = float(np.sum((ys - yhat) ** 2))\n",
    "        ss_tot = float(np.sum((ys - np.mean(ys)) ** 2)) + EPS\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "        return float(slope), float(intercept), float(r2)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "\n",
    "def sed_logflux_vs_loglambda_at_time(band_tb, band_fb, band_eb, t0, z=0.0):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ws = []\n",
    "    for b in FILTERS:\n",
    "        tb = band_tb.get(b, None)\n",
    "        fb = band_fb.get(b, None)\n",
    "        eb = band_eb.get(b, None)\n",
    "        if tb is None or fb is None or eb is None:\n",
    "            continue\n",
    "        f = interp_flux_at_time(tb, fb, t0)\n",
    "        e = interp_err_at_time(tb, eb, t0)\n",
    "        if not np.isfinite(f) or not np.isfinite(e):\n",
    "            continue\n",
    "        if f <= 0:\n",
    "            continue\n",
    "        lam_rest = float(EFF_WL_AA[b] / (1.0 + float(z)))\n",
    "        xs.append(np.log(lam_rest + EPS))\n",
    "        ys.append(np.log(f + EPS))\n",
    "        ws.append(1.0 / ((e / (f + EPS)) ** 2 + EPS))  # weight by relative error\n",
    "    if len(xs) < 2:\n",
    "        return np.nan, np.nan, np.nan, float(len(xs))\n",
    "    xs = np.asarray(xs, float)\n",
    "    ys = np.asarray(ys, float)\n",
    "    ws = np.asarray(ws, float)\n",
    "\n",
    "    try:\n",
    "        W = np.sum(ws)\n",
    "        xbar = np.sum(ws * xs) / (W + EPS)\n",
    "        ybar = np.sum(ws * ys) / (W + EPS)\n",
    "        cov = np.sum(ws * (xs - xbar) * (ys - ybar))\n",
    "        var = np.sum(ws * (xs - xbar) ** 2) + EPS\n",
    "        slope = cov / var\n",
    "        intercept = ybar - slope * xbar\n",
    "        yhat = slope * xs + intercept\n",
    "        ss_res = float(np.sum(ws * (ys - yhat) ** 2))\n",
    "        ss_tot = float(np.sum(ws * (ys - ybar) ** 2)) + EPS\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "        return float(slope), float(intercept), float(r2), float(len(xs))\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, np.nan, float(len(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd5ba7",
   "metadata": {},
   "source": [
    "Had to add bounds to Bazin cause it was overflowing. Pain in the ass. Most of the formulas in here are new to me. Astronomy is not my forte. My entire strategy for this competition was to add quality features, which turned into just creating a plethora of features. Hard to manage. But hey if you are reading this you for some reason decided to look through my projects and are probably doubting whether or not I understand all of this. You are correct. I have a very strong understanding of ML, but when it comes to astronomy I am clueless. I followed a lead in a discussion to focus on what classifies a TDE from the SpecType feature available in the train set, but not in the test. My whole model is to create a seperate model to predict SpecType, and then use that to create even more features with the help of Chat GPT barreling through thousands of research papers and finding hundreds of formulas. I then throw XGB and run that shit through optuna overnight to get a decent model. It's worked. haven't ran this one yet, but I currently sit ~130th out of 800. Should move up to top 50 we will see. 2026/01/24 1:57 AM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f84f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -60.0, 60.0)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def bazin_stable(t, A, t0, trise, tfall, B, eps=EPS):\n",
    "    \"\"\"\n",
    "    Numerically stable Bazin-like function:\n",
    "        f(t) = A * exp(-(t-t0)/tfall) * sigmoid((t-t0)/trise) + B\n",
    "    \"\"\"\n",
    "    trise = np.maximum(trise, eps)\n",
    "    tfall = np.maximum(tfall, eps)\n",
    "\n",
    "    x = (t - t0) / trise\n",
    "    exp_term = np.exp(np.clip(-(t - t0) / tfall, -60.0, 60.0))\n",
    "    return A * exp_term * sigmoid(x) + B\n",
    "\n",
    "\n",
    "def should_fit_bazin(tb, fb, eb, min_points=8, amp_sigma=3.0):\n",
    "    \"\"\"\n",
    "    Gate: only fit when there is enough data and a detectable transient-like signal.\n",
    "    \"\"\"\n",
    "    tb = np.asarray(tb, float)\n",
    "    fb = np.asarray(fb, float)\n",
    "    eb = np.asarray(eb, float)\n",
    "\n",
    "    if len(tb) < min_points:\n",
    "        return False\n",
    "\n",
    "    mederr = float(np.median(eb)) if np.isfinite(np.median(eb)) else np.inf\n",
    "    if not np.isfinite(mederr) or mederr <= 0:\n",
    "        return False\n",
    "\n",
    "    amp = float(np.percentile(fb, 95) - np.percentile(fb, 5))\n",
    "    if not np.isfinite(amp) or amp < amp_sigma * mederr:\n",
    "        return False\n",
    "\n",
    "    if float(np.std(fb)) < 1e-10:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def fit_bazin(tb, fb, eb):\n",
    "    \"\"\"\n",
    "    Returns (A, t0, trise, tfall, B, chi2_red) on success.\n",
    "    Returns (nan...nan) on failure or if scipy missing / gate fails.\n",
    "    \"\"\"\n",
    "    nan_out = (np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "\n",
    "    tb = np.asarray(tb, float)\n",
    "    fb = np.asarray(fb, float)\n",
    "    eb = np.asarray(eb, float)\n",
    "\n",
    "    order = np.argsort(tb)\n",
    "    t = tb[order]\n",
    "    f = fb[order]\n",
    "    e = eb[order]\n",
    "\n",
    "    m = np.isfinite(t) & np.isfinite(f) & np.isfinite(e)\n",
    "    t, f, e = t[m], f[m], e[m]\n",
    "    if len(t) < 3:\n",
    "        return nan_out\n",
    "\n",
    "    e = np.maximum(e, 1e-6)\n",
    "\n",
    "    if not should_fit_bazin(t, f, e, min_points=8, amp_sigma=3.0):\n",
    "        return nan_out\n",
    "\n",
    "    B0 = float(np.median(f))\n",
    "    A0 = float(max(1e-6, np.percentile(f, 95) - B0))\n",
    "    t0_0 = float(t[int(np.argmax(f))])\n",
    "    tr0 = 20.0\n",
    "    tf0 = 60.0\n",
    "    p0 = [A0, t0_0, tr0, tf0, B0]\n",
    "\n",
    "    tmin, tmax = float(t.min()), float(t.max())\n",
    "    iqr = float(np.percentile(f, 75) - np.percentile(f, 25))\n",
    "    amp = float(max(1e-6, np.percentile(f, 95) - np.percentile(f, 5)))\n",
    "\n",
    "    lo = [0.0, tmin - 50.0, 0.5, 1.0, B0 - 5.0 * (iqr + 1e-6)]\n",
    "    hi = [10.0 * amp, tmax + 50.0, 200.0, 600.0, B0 + 5.0 * (iqr + 1e-6)]\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(\n",
    "            bazin_stable, t, f,\n",
    "            p0=p0,\n",
    "            sigma=e,\n",
    "            absolute_sigma=True,\n",
    "            bounds=(lo, hi),\n",
    "            maxfev=5000\n",
    "        )\n",
    "\n",
    "        fhat = bazin_stable(t, *popt)\n",
    "        resid = (f - fhat) / e\n",
    "        chi2 = float(np.sum(resid * resid))\n",
    "        dof = max(1, len(t) - len(popt))\n",
    "        chi2_red = chi2 / dof\n",
    "\n",
    "        A, t0, trise, tfall, B = [float(x) for x in popt]\n",
    "        return (A, t0, trise, tfall, B, float(chi2_red))\n",
    "\n",
    "    except Exception:\n",
    "        return nan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9427600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_object(lc_raw, z, z_err, ebv):\n",
    "    feats = {}\n",
    "    lc = lc_raw.sort_values(\"Time (MJD)\").reset_index(drop=True)\n",
    "\n",
    "    t = lc[\"Time (MJD)\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    if len(t) == 0:\n",
    "        feats[\"n_obs\"] = 0\n",
    "        return feats\n",
    "\n",
    "    z = safe_float(z, default=0.0)\n",
    "    z_err = safe_float(z_err, default=0.0)\n",
    "    ebv = safe_float(ebv, default=np.nan)\n",
    "\n",
    "    t_rel = t - t.min()\n",
    "    t_rest = t_rel / (1.0 + z)\n",
    "\n",
    "    flux_raw = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    err_raw = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "    flux_corr, err_corr = deextinct_lightcurve(lc, ebv)\n",
    "\n",
    "    feats[\"n_obs\"] = int(len(t))\n",
    "    feats[\"total_time_obs\"] = float(t_rel.max() - t_rel.min())\n",
    "    feats[\"total_time_rest\"] = float(t_rest.max() - t_rest.min())\n",
    "\n",
    "    feats[\"flux_mean\"] = float(np.mean(flux_corr))\n",
    "    feats[\"flux_median\"] = float(np.median(flux_corr))\n",
    "    feats[\"flux_std\"] = float(np.std(flux_corr))\n",
    "    feats[\"flux_min\"] = float(np.min(flux_corr))\n",
    "    feats[\"flux_max\"] = float(np.max(flux_corr))\n",
    "\n",
    "    feats[\"flux_mad\"] = median_abs_dev(flux_corr)\n",
    "    feats[\"flux_iqr\"] = iqr(flux_corr)\n",
    "    feats[\"flux_skew\"] = skewness(flux_corr)\n",
    "    feats[\"flux_kurt_excess\"] = kurtosis_excess(flux_corr)\n",
    "\n",
    "    p5, p25, p75, p95 = np.percentile(flux_corr, [5, 25, 75, 95])\n",
    "    feats[\"flux_p5\"] = float(p5)\n",
    "    feats[\"flux_p25\"] = float(p25)\n",
    "    feats[\"flux_p75\"] = float(p75)\n",
    "    feats[\"flux_p95\"] = float(p95)\n",
    "    feats[\"robust_amp_global\"] = float(p95 - p5)\n",
    "\n",
    "    feats[\"neg_flux_frac\"] = float(np.mean(flux_corr < 0))\n",
    "\n",
    "    snr = np.abs(flux_corr) / (err_corr + EPS)\n",
    "    feats[\"snr_median\"] = float(np.median(snr))\n",
    "    feats[\"snr_max\"] = float(np.max(snr))\n",
    "\n",
    "    feats[\"flux_mean_raw\"] = float(np.mean(flux_raw))\n",
    "    feats[\"flux_std_raw\"] = float(np.std(flux_raw))\n",
    "    feats[\"snr_max_raw\"] = float(np.max(np.abs(flux_raw) / (err_raw + EPS)))\n",
    "    feats[\"fvar_raw\"] = fractional_variability(flux_raw, err_raw)\n",
    "\n",
    "    feats[\"flux_mean_deext_minus_raw\"] = float(feats[\"flux_mean\"] - feats[\"flux_mean_raw\"])\n",
    "    feats[\"snrmax_deext_minus_raw\"] = float(feats[\"snr_max\"] - feats[\"snr_max_raw\"])\n",
    "\n",
    "    if len(t_rel) >= 2:\n",
    "        dt = np.diff(t_rel)\n",
    "        feats[\"median_dt\"] = float(np.median(dt))\n",
    "        feats[\"max_gap\"] = float(np.max(dt))\n",
    "        feats[\"n_seasons_global\"] = float(np.sum(dt > SEASON_GAP_DAYS) + 1)\n",
    "        feats[\"gap_frac_gt90\"] = float(np.mean(dt > SEASON_GAP_DAYS))\n",
    "        feats[\"gap_frac_gt30\"] = float(np.mean(dt > 30.0))\n",
    "    else:\n",
    "        feats[\"median_dt\"] = np.nan\n",
    "        feats[\"max_gap\"] = np.nan\n",
    "        feats[\"n_seasons_global\"] = np.nan\n",
    "        feats[\"gap_frac_gt90\"] = np.nan\n",
    "        feats[\"gap_frac_gt30\"] = np.nan\n",
    "\n",
    "    feats[\"eta_von_neumann\"] = von_neumann_eta(flux_corr)\n",
    "    feats[\"chi2_const_global\"] = chi2_to_constant(flux_corr, err_corr)\n",
    "    feats[\"stetsonJ_global_obs\"] = stetson_J_consecutive(t_rel, flux_corr, err_corr)\n",
    "    feats[\"stetsonJ_global_rest\"] = stetson_J_consecutive(t_rest, flux_corr, err_corr)\n",
    "\n",
    "    feats[\"max_slope_global_obs\"] = max_slope(t_rel, flux_corr)\n",
    "    feats[\"max_slope_global_rest\"] = max_slope(t_rest, flux_corr)\n",
    "\n",
    "    feats[\"med_abs_slope_global_obs\"] = median_abs_slope(t_rel, flux_corr)\n",
    "    feats[\"med_abs_slope_global_rest\"] = median_abs_slope(t_rest, flux_corr)\n",
    "\n",
    "    feats[\"slope_global_obs\"] = linear_slope(t_rel, flux_corr)\n",
    "    feats[\"slope_global_rest\"] = linear_slope(t_rest, flux_corr)\n",
    "\n",
    "    feats[\"fvar_global\"] = fractional_variability(flux_corr, err_corr)\n",
    "\n",
    "    feats[\"Z\"] = float(z)\n",
    "    feats[\"log1pZ\"] = float(np.log1p(max(0.0, z)))\n",
    "    feats[\"Z_err\"] = float(max(0.0, z_err))\n",
    "    feats[\"log1pZerr\"] = float(np.log1p(max(0.0, feats[\"Z_err\"])))\n",
    "    feats[\"EBV\"] = ebv\n",
    "\n",
    "    feats[\"n_filters_present\"] = 0\n",
    "    feats[\"total_obs\"] = 0\n",
    "\n",
    "    band_tpeak_obs = {}\n",
    "    band_tpeak_rest = {}\n",
    "    band_peak_flux = {}\n",
    "\n",
    "    band_tb_obs = {}\n",
    "    band_tb_rest = {}\n",
    "    band_fb = {}\n",
    "    band_eb = {}\n",
    "\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        nb = int(np.sum(m))\n",
    "        feats[f\"n_{b}\"] = nb\n",
    "        feats[\"total_obs\"] += nb\n",
    "\n",
    "        keys = [\n",
    "            f\"amp_{b}\",\n",
    "            f\"amp_pre_{b}\",\n",
    "            f\"baseline_pre_{b}\",\n",
    "            f\"robust_amp_{b}\",\n",
    "            f\"tpeak_{b}_obs\",\n",
    "            f\"tpeak_{b}_rest\",\n",
    "            f\"width50_{b}_obs\",\n",
    "            f\"width50_{b}_rest\",\n",
    "            f\"width80_{b}_obs\",\n",
    "            f\"width80_{b}_rest\",\n",
    "            f\"auc_pos_{b}_obs\",\n",
    "            f\"auc_pos_{b}_rest\",\n",
    "            f\"snrmax_{b}\",\n",
    "            f\"eta_{b}\",\n",
    "            f\"chi2_const_{b}\",\n",
    "            f\"slope_{b}_obs\",\n",
    "            f\"slope_{b}_rest\",\n",
    "            f\"maxslope_{b}_obs\",\n",
    "            f\"maxslope_{b}_rest\",\n",
    "            f\"stetsonJ_{b}_obs\",\n",
    "            f\"stetsonJ_{b}_rest\",\n",
    "            f\"p5_{b}\",\n",
    "            f\"p25_{b}\",\n",
    "            f\"p75_{b}\",\n",
    "            f\"p95_{b}\",\n",
    "            f\"mad_{b}\",\n",
    "            f\"iqr_{b}\",\n",
    "            f\"mad_over_std_{b}\",\n",
    "            f\"fvar_{b}\",\n",
    "            f\"t_fall50_{b}_obs\",\n",
    "            f\"t_fall20_{b}_obs\",\n",
    "            f\"t_fall50_{b}_rest\",\n",
    "            f\"t_fall20_{b}_rest\",\n",
    "            f\"t_rise50_{b}_obs\",\n",
    "            f\"t_rise20_{b}_obs\",\n",
    "            f\"t_rise50_{b}_rest\",\n",
    "            f\"t_rise20_{b}_rest\",\n",
    "            f\"asym50_{b}_obs\",\n",
    "            f\"asym50_{b}_rest\",\n",
    "            f\"sharp50_{b}_obs\",\n",
    "            f\"sharp50_{b}_rest\",\n",
    "            f\"peak_dominance_{b}\",\n",
    "            f\"std_ratio_prepost_{b}\",\n",
    "            f\"n_peaks_{b}\",\n",
    "            f\"postpeak_monotone_frac_{b}\",\n",
    "            f\"n_rebrighten_{b}\",\n",
    "            f\"decay_pl_slope_{b}_obs\",\n",
    "            f\"decay_pl_r2_{b}_obs\",\n",
    "            f\"decay_pl_npts_{b}_obs\",\n",
    "            f\"decay_pl_slope_{b}_rest\",\n",
    "            f\"decay_pl_r2_{b}_rest\",\n",
    "            f\"decay_pl_npts_{b}_rest\",\n",
    "\n",
    "            # seasonality and structure function per band\n",
    "            f\"n_seasons_{b}\",\n",
    "            f\"season_maxspan_{b}\",\n",
    "            f\"season_meanspan_{b}\",\n",
    "            f\"sf_medabs_5_{b}\",\n",
    "            f\"sf_n_5_{b}\",\n",
    "            f\"sf_medabs_10_{b}\",\n",
    "            f\"sf_n_10_{b}\",\n",
    "            f\"sf_medabs_20_{b}\",\n",
    "            f\"sf_n_20_{b}\",\n",
    "            f\"sf_medabs_50_{b}\",\n",
    "            f\"sf_n_50_{b}\",\n",
    "            f\"sf_medabs_100_{b}\",\n",
    "            f\"sf_n_100_{b}\",\n",
    "\n",
    "            # Bazin shape fit\n",
    "            f\"bazin_A_{b}\",\n",
    "            f\"bazin_t0_{b}_obs\",\n",
    "            f\"bazin_trise_{b}_obs\",\n",
    "            f\"bazin_tfall_{b}_obs\",\n",
    "            f\"bazin_B_{b}\",\n",
    "            f\"bazin_chi2red_{b}_obs\",\n",
    "            f\"bazin_trise_{b}_rest\",\n",
    "            f\"bazin_tfall_{b}_rest\",\n",
    "        ]\n",
    "        for k in keys:\n",
    "            feats[k] = np.nan\n",
    "\n",
    "        if nb == 0:\n",
    "            continue\n",
    "\n",
    "        feats[\"n_filters_present\"] += 1\n",
    "\n",
    "        tb_obs = t_rel[m]\n",
    "        fb = flux_corr[m]\n",
    "        eb = err_corr[m]\n",
    "\n",
    "        order = np.argsort(tb_obs)\n",
    "        tb_obs = tb_obs[order]\n",
    "        fb = fb[order]\n",
    "        eb = eb[order]\n",
    "        tb_rest = tb_obs / (1.0 + z)\n",
    "\n",
    "        band_tb_obs[b] = tb_obs\n",
    "        band_tb_rest[b] = tb_rest\n",
    "        band_fb[b] = fb\n",
    "        band_eb[b] = eb\n",
    "\n",
    "        ns, maxsp, meansp = seasonality_features(tb_obs)\n",
    "        feats[f\"n_seasons_{b}\"] = ns\n",
    "        feats[f\"season_maxspan_{b}\"] = maxsp\n",
    "        feats[f\"season_meanspan_{b}\"] = meansp\n",
    "\n",
    "        sf = structure_function_lags(tb_obs, fb, lags=SF_LAGS)\n",
    "        for lag in SF_LAGS:\n",
    "            feats[f\"sf_medabs_{int(lag)}_{b}\"] = sf.get(f\"sf_medabs_{int(lag)}\", np.nan)\n",
    "            feats[f\"sf_n_{int(lag)}_{b}\"] = sf.get(f\"sf_n_{int(lag)}\", 0.0)\n",
    "\n",
    "        p5b, p25b, p75b, p95b = np.percentile(fb, [5, 25, 75, 95])\n",
    "        feats[f\"p5_{b}\"] = float(p5b)\n",
    "        feats[f\"p25_{b}\"] = float(p25b)\n",
    "        feats[f\"p75_{b}\"] = float(p75b)\n",
    "        feats[f\"p95_{b}\"] = float(p95b)\n",
    "        feats[f\"robust_amp_{b}\"] = float(p95b - p5b)\n",
    "\n",
    "        feats[f\"mad_{b}\"] = median_abs_dev(fb)\n",
    "        feats[f\"iqr_{b}\"] = iqr(fb)\n",
    "        stdb = float(np.std(fb))\n",
    "        feats[f\"mad_over_std_{b}\"] = float(feats[f\"mad_{b}\"] / (stdb + EPS))\n",
    "\n",
    "        base_pre, mad_pre, mederr_pre = pre_peak_baseline(tb_obs, fb, eb, frac=PRE_BASE_FRAC)\n",
    "        feats[f\"baseline_pre_{b}\"] = float(base_pre) if np.isfinite(base_pre) else np.nan\n",
    "\n",
    "        pidx = int(np.argmax(fb))\n",
    "        peak_flux = float(fb[pidx])\n",
    "        tpeak_obs = float(tb_obs[pidx])\n",
    "        tpeak_rest = float(tb_rest[pidx])\n",
    "\n",
    "        amp_median = peak_flux - float(np.median(fb))\n",
    "        amp_pre = peak_flux - base_pre if np.isfinite(base_pre) else np.nan\n",
    "\n",
    "        feats[f\"amp_{b}\"] = float(amp_median)\n",
    "        feats[f\"amp_pre_{b}\"] = float(amp_pre) if np.isfinite(amp_pre) else np.nan\n",
    "\n",
    "        feats[f\"tpeak_{b}_obs\"] = tpeak_obs\n",
    "        feats[f\"tpeak_{b}_rest\"] = tpeak_rest\n",
    "        feats[f\"snrmax_{b}\"] = float(np.max(np.abs(fb) / (eb + EPS)))\n",
    "\n",
    "        feats[f\"eta_{b}\"] = von_neumann_eta(fb)\n",
    "        feats[f\"chi2_const_{b}\"] = chi2_to_constant(fb, eb)\n",
    "\n",
    "        feats[f\"slope_{b}_obs\"] = linear_slope(tb_obs, fb)\n",
    "        feats[f\"slope_{b}_rest\"] = linear_slope(tb_rest, fb)\n",
    "\n",
    "        feats[f\"maxslope_{b}_obs\"] = max_slope(tb_obs, fb)\n",
    "        feats[f\"maxslope_{b}_rest\"] = max_slope(tb_rest, fb)\n",
    "\n",
    "        feats[f\"stetsonJ_{b}_obs\"] = stetson_J_consecutive(tb_obs, fb, eb)\n",
    "        feats[f\"stetsonJ_{b}_rest\"] = stetson_J_consecutive(tb_rest, fb, eb)\n",
    "\n",
    "        feats[f\"fvar_{b}\"] = fractional_variability(fb, eb)\n",
    "\n",
    "        A, t0, trise, tfall, B, chi2 = fit_bazin(tb_obs, fb, eb)\n",
    "        feats[f\"bazin_A_{b}\"] = A\n",
    "        feats[f\"bazin_t0_{b}_obs\"] = t0\n",
    "        feats[f\"bazin_trise_{b}_obs\"] = trise\n",
    "        feats[f\"bazin_tfall_{b}_obs\"] = tfall\n",
    "        feats[f\"bazin_B_{b}\"] = B\n",
    "        feats[f\"bazin_chi2red_{b}_obs\"] = chi2\n",
    "\n",
    "        feats[f\"bazin_trise_{b}_rest\"] = trise / (1.0 + z) if np.isfinite(trise) else np.nan\n",
    "        feats[f\"bazin_tfall_{b}_rest\"] = tfall / (1.0 + z) if np.isfinite(tfall) else np.nan\n",
    "\n",
    "        if np.isfinite(amp_pre) and amp_pre > 0:\n",
    "            feats[f\"peak_dominance_{b}\"] = float(amp_pre / (mad_pre + EPS))\n",
    "\n",
    "            pre_seg = fb[:max(2, pidx)]\n",
    "            post_seg = fb[pidx:]\n",
    "            std_pre = float(np.std(pre_seg)) if len(pre_seg) >= 2 else np.nan\n",
    "            std_post = float(np.std(post_seg)) if len(post_seg) >= 2 else np.nan\n",
    "            if np.isfinite(std_pre) and np.isfinite(std_post):\n",
    "                feats[f\"std_ratio_prepost_{b}\"] = float(std_pre / (std_post + EPS))\n",
    "\n",
    "            feats[f\"postpeak_monotone_frac_{b}\"] = float(postpeak_monotonicity(tb_obs, fb, pidx))\n",
    "            feats[f\"n_peaks_{b}\"] = float(count_significant_peaks(tb_obs, fb, eb, base_pre, k_sigma=PEAK_SIGMA_K))\n",
    "            feats[f\"n_rebrighten_{b}\"] = float(count_rebrighten(tb_obs, fb, base_pre, amp_pre, pidx, frac=REBRIGHT_FRAC))\n",
    "\n",
    "            feats[f\"t_fall50_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "            feats[f\"t_fall50_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "\n",
    "            feats[f\"t_rise50_{b}_obs\"] = float(rise_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_rise20_{b}_obs\"] = float(rise_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "            feats[f\"t_rise50_{b}_rest\"] = float(rise_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_rise20_{b}_rest\"] = float(rise_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "\n",
    "            tr50o = feats[f\"t_rise50_{b}_obs\"]\n",
    "            tf50o = feats[f\"t_fall50_{b}_obs\"]\n",
    "            tr50r = feats[f\"t_rise50_{b}_rest\"]\n",
    "            tf50r = feats[f\"t_fall50_{b}_rest\"]\n",
    "            feats[f\"asym50_{b}_obs\"] = float(tf50o / (tr50o + EPS)) if np.isfinite(tf50o) and np.isfinite(tr50o) else np.nan\n",
    "            feats[f\"asym50_{b}_rest\"] = float(tf50r / (tr50r + EPS)) if np.isfinite(tf50r) and np.isfinite(tr50r) else np.nan\n",
    "\n",
    "            feats[f\"auc_pos_{b}_obs\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_obs))\n",
    "            feats[f\"auc_pos_{b}_rest\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_rest))\n",
    "\n",
    "            def width_at_level(tt, ff, base, amp, frac):\n",
    "                if amp <= 0 or len(ff) < 3:\n",
    "                    return np.nan\n",
    "                level = base + frac * amp\n",
    "                above = ff >= level\n",
    "                if not np.any(above):\n",
    "                    return np.nan\n",
    "                idx = np.where(above)[0]\n",
    "                return float(tt[idx[-1]] - tt[idx[0]])\n",
    "\n",
    "            w50_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.80)\n",
    "            w50_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.80)\n",
    "\n",
    "            feats[f\"width50_{b}_obs\"] = w50_obs\n",
    "            feats[f\"width80_{b}_obs\"] = w80_obs\n",
    "            feats[f\"width50_{b}_rest\"] = w50_rest\n",
    "            feats[f\"width80_{b}_rest\"] = w80_rest\n",
    "\n",
    "            feats[f\"sharp50_{b}_obs\"] = float(amp_pre / (w50_obs + EPS)) if np.isfinite(w50_obs) else np.nan\n",
    "            feats[f\"sharp50_{b}_rest\"] = float(amp_pre / (w50_rest + EPS)) if np.isfinite(w50_rest) else np.nan\n",
    "\n",
    "            b_obs, r2_obs, npts_obs = decay_powerlaw_fit(tb_obs, fb, base_pre, pidx, tmax=300.0)\n",
    "            b_rest, r2_rest, npts_rest = decay_powerlaw_fit(tb_rest, fb, base_pre, pidx, tmax=300.0)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_obs\"] = b_obs\n",
    "            feats[f\"decay_pl_r2_{b}_obs\"] = r2_obs\n",
    "            feats[f\"decay_pl_npts_{b}_obs\"] = float(npts_obs)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_rest\"] = b_rest\n",
    "            feats[f\"decay_pl_r2_{b}_rest\"] = r2_rest\n",
    "            feats[f\"decay_pl_npts_{b}_rest\"] = float(npts_rest)\n",
    "\n",
    "        band_tpeak_obs[b] = tpeak_obs\n",
    "        band_tpeak_rest[b] = tpeak_rest\n",
    "        band_peak_flux[b] = peak_flux\n",
    "\n",
    "    tpeaks_obs = np.array([band_tpeak_obs.get(b, np.nan) for b in FILTERS], float)\n",
    "    tpeaks_rest = np.array([band_tpeak_rest.get(b, np.nan) for b in FILTERS], float)\n",
    "    tpeaks_obs = np.array([x for x in tpeaks_obs if np.isfinite(x)], float)\n",
    "    tpeaks_rest = np.array([x for x in tpeaks_rest if np.isfinite(x)], float)\n",
    "    feats[\"tpeak_std_obs\"] = float(np.std(tpeaks_obs)) if len(tpeaks_obs) >= 2 else np.nan\n",
    "    feats[\"tpeak_std_rest\"] = float(np.std(tpeaks_rest)) if len(tpeaks_rest) >= 2 else np.nan\n",
    "\n",
    "    pairs = [(\"u\", \"g\"), (\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\"), (\"z\", \"y\")]\n",
    "    for a, b in pairs:\n",
    "        ta_obs = band_tpeak_obs.get(a, np.nan)\n",
    "        tb_obs2 = band_tpeak_obs.get(b, np.nan)\n",
    "        ta_rest = band_tpeak_rest.get(a, np.nan)\n",
    "        tb_rest2 = band_tpeak_rest.get(b, np.nan)\n",
    "        pa = band_peak_flux.get(a, np.nan)\n",
    "        pb = band_peak_flux.get(b, np.nan)\n",
    "\n",
    "        feats[f\"tpeakdiff_{a}{b}_obs\"] = (ta_obs - tb_obs2) if (np.isfinite(ta_obs) and np.isfinite(tb_obs2)) else np.nan\n",
    "        feats[f\"tpeakdiff_{a}{b}_rest\"] = (ta_rest - tb_rest2) if (np.isfinite(ta_rest) and np.isfinite(tb_rest2)) else np.nan\n",
    "        feats[f\"peakratio_{a}{b}\"] = (pa / (pb + EPS)) if (np.isfinite(pa) and np.isfinite(pb)) else np.nan\n",
    "\n",
    "    def ratio_feature(name, num, den):\n",
    "        if np.isfinite(num) and np.isfinite(den):\n",
    "            feats[name] = float(num / (den + EPS))\n",
    "        else:\n",
    "            feats[name] = np.nan\n",
    "\n",
    "    for a, b in pairs:\n",
    "        ratio_feature(f\"amppreratio_{a}{b}\", feats.get(f\"amp_pre_{a}\", np.nan), feats.get(f\"amp_pre_{b}\", np.nan))\n",
    "        ratio_feature(f\"aucratio_{a}{b}_obs\", feats.get(f\"auc_pos_{a}_obs\", np.nan), feats.get(f\"auc_pos_{b}_obs\", np.nan))\n",
    "        ratio_feature(f\"width50ratio_{a}{b}_obs\", feats.get(f\"width50_{a}_obs\", np.nan), feats.get(f\"width50_{b}_obs\", np.nan))\n",
    "        ratio_feature(f\"asym50ratio_{a}{b}_obs\", feats.get(f\"asym50_{a}_obs\", np.nan), feats.get(f\"asym50_{b}_obs\", np.nan))\n",
    "\n",
    "    for a, b in [(\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\")]:\n",
    "        if (a in band_tb_obs) and (b in band_tb_obs):\n",
    "            feats[f\"corr_{a}{b}_obs\"] = band_corr(\n",
    "                band_tb_obs[a], band_fb[a],\n",
    "                band_tb_obs[b], band_fb[b]\n",
    "            )\n",
    "        else:\n",
    "            feats[f\"corr_{a}{b}_obs\"] = np.nan\n",
    "\n",
    "    slope_t, intercept_t, r2_t = peak_vs_wavelength_slope(band_tpeak_obs, band_tpeak_obs, z=z)\n",
    "    feats[\"tpeak_vs_lambda_slope_obs\"] = slope_t\n",
    "    feats[\"tpeak_vs_lambda_intercept_obs\"] = intercept_t\n",
    "    feats[\"tpeak_vs_lambda_r2_obs\"] = r2_t\n",
    "\n",
    "    slope_pf, intercept_pf, r2_pf = peak_vs_wavelength_slope(band_tpeak_obs, band_peak_flux, z=z)\n",
    "    feats[\"peakflux_vs_lambda_slope\"] = slope_pf\n",
    "    feats[\"peakflux_vs_lambda_intercept\"] = intercept_pf\n",
    "    feats[\"peakflux_vs_lambda_r2\"] = r2_pf\n",
    "\n",
    "    tpr_obs = feats.get(\"tpeak_r_obs\", np.nan)\n",
    "    if np.isfinite(tpr_obs):\n",
    "        def colors_at_time(t0):\n",
    "            fr = interp_flux_at_time(band_tb_obs.get(\"r\", np.array([])), band_fb.get(\"r\", np.array([])), t0)\n",
    "            fg = interp_flux_at_time(band_tb_obs.get(\"g\", np.array([])), band_fb.get(\"g\", np.array([])), t0)\n",
    "            fi = interp_flux_at_time(band_tb_obs.get(\"i\", np.array([])), band_fb.get(\"i\", np.array([])), t0)\n",
    "            cgr = (signed_log1p(fg) - signed_log1p(fr)) if (np.isfinite(fg) and np.isfinite(fr)) else np.nan\n",
    "            cri = (signed_log1p(fr) - signed_log1p(fi)) if (np.isfinite(fr) and np.isfinite(fi)) else np.nan\n",
    "            return cgr, cri\n",
    "\n",
    "        cgr0, cri0 = colors_at_time(tpr_obs)\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = cgr0\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = cri0\n",
    "\n",
    "        cgr20, cri20 = colors_at_time(tpr_obs + 20.0)\n",
    "        cgr40, cri40 = colors_at_time(tpr_obs + 40.0)\n",
    "\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = cgr20\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = cri20\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = cgr40\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = cri40\n",
    "\n",
    "        def slope(c1, c2, dt):\n",
    "            if np.isfinite(c1) and np.isfinite(c2):\n",
    "                return float((c2 - c1) / dt)\n",
    "            return np.nan\n",
    "\n",
    "        feats[\"color_gr_slope20_obs\"] = slope(cgr0, cgr20, 20.0)\n",
    "        feats[\"color_ri_slope20_obs\"] = slope(cri0, cri20, 20.0)\n",
    "        feats[\"color_gr_slope40_obs\"] = slope(cgr0, cgr40, 40.0)\n",
    "        feats[\"color_ri_slope40_obs\"] = slope(cri0, cri40, 40.0)\n",
    "    else:\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope20_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope20_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope40_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope40_obs\"] = np.nan\n",
    "\n",
    "    if np.isfinite(tpr_obs):\n",
    "        sed_slope, sed_int, sed_r2, sed_n = sed_logflux_vs_loglambda_at_time(\n",
    "            band_tb_obs, band_fb, band_eb, tpr_obs, z=z\n",
    "        )\n",
    "        feats[\"sed_logflux_loglambda_slope_rpeak\"] = sed_slope\n",
    "        feats[\"sed_logflux_loglambda_r2_rpeak\"] = sed_r2\n",
    "        feats[\"sed_logflux_loglambda_nbands_rpeak\"] = sed_n\n",
    "\n",
    "        sed_slope20, sed_int20, sed_r2_20, sed_n20 = sed_logflux_vs_loglambda_at_time(\n",
    "            band_tb_obs, band_fb, band_eb, tpr_obs + 20.0, z=z\n",
    "        )\n",
    "        feats[\"sed_slope_rpeak_p20\"] = sed_slope20\n",
    "        feats[\"sed_r2_rpeak_p20\"] = sed_r2_20\n",
    "        feats[\"sed_nbands_rpeak_p20\"] = sed_n20\n",
    "    else:\n",
    "        feats[\"sed_logflux_loglambda_slope_rpeak\"] = np.nan\n",
    "        feats[\"sed_logflux_loglambda_r2_rpeak\"] = np.nan\n",
    "        feats[\"sed_logflux_loglambda_nbands_rpeak\"] = np.nan\n",
    "        feats[\"sed_slope_rpeak_p20\"] = np.nan\n",
    "        feats[\"sed_r2_rpeak_p20\"] = np.nan\n",
    "        feats[\"sed_nbands_rpeak_p20\"] = np.nan\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cbd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lightcurve_cache(splits, base_dir=\"data\", kind=\"train\"):\n",
    "    lc_cache = {}\n",
    "    idx_cache = {}\n",
    "    for s in splits:\n",
    "        path = f\"{base_dir}/{s}/{kind}_full_lightcurves.csv\"\n",
    "        lc = pd.read_csv(path)\n",
    "        lc[\"object_id\"] = lc[\"object_id\"].astype(str)\n",
    "        groups = lc.groupby(\"object_id\").indices\n",
    "        lc_cache[s] = lc\n",
    "        idx_cache[s] = groups\n",
    "    return lc_cache, idx_cache\n",
    "\n",
    "\n",
    "def get_lightcurve(lc_cache, idx_cache, split, object_id):\n",
    "    object_id = str(object_id)\n",
    "    idx = idx_cache[split].get(object_id, None)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    return lc_cache[split].iloc[idx]\n",
    "\n",
    "def build_feature_table(\n",
    "    log_df,\n",
    "    lc_cache,\n",
    "    idx_cache,\n",
    "    augment_photoz=False,\n",
    "    test_zerr_pool=None,\n",
    "    n_aug=2,\n",
    "    seed=6\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    if test_zerr_pool is not None:\n",
    "        test_zerr_pool = np.asarray(test_zerr_pool, float)\n",
    "        test_zerr_pool = test_zerr_pool[np.isfinite(test_zerr_pool)]\n",
    "        test_zerr_pool = test_zerr_pool[test_zerr_pool > 0]\n",
    "\n",
    "    for i in range(len(log_df)):\n",
    "        r = log_df.iloc[i]\n",
    "        obj = str(r[\"object_id\"])\n",
    "        split = r[\"split\"]\n",
    "\n",
    "        lc = get_lightcurve(lc_cache, idx_cache, split, obj)\n",
    "        if lc is None:\n",
    "            feats = {\"n_obs\": 0}\n",
    "            feats[\"object_id\"] = obj\n",
    "            feats[\"split\"] = split\n",
    "            feats[\"photoz_aug\"] = 0\n",
    "            if \"target\" in log_df.columns:\n",
    "                feats[\"target\"] = int(r[\"target\"])\n",
    "            rows.append(feats)\n",
    "            continue\n",
    "\n",
    "        feats = extract_features_for_object(\n",
    "            lc_raw=lc,\n",
    "            z=r[\"Z\"],\n",
    "            z_err=r.get(\"Z_err\", 0.0),\n",
    "            ebv=r[\"EBV\"],\n",
    "        )\n",
    "        feats[\"object_id\"] = obj\n",
    "        feats[\"split\"] = split\n",
    "        feats[\"photoz_aug\"] = 0\n",
    "        if \"target\" in log_df.columns:\n",
    "            feats[\"target\"] = int(r[\"target\"])\n",
    "        rows.append(feats)\n",
    "\n",
    "        if augment_photoz and (\"target\" in log_df.columns) and (test_zerr_pool is not None) and (len(test_zerr_pool) > 0):\n",
    "            z0 = safe_float(r[\"Z\"], default=0.0)\n",
    "            for _ in range(n_aug):\n",
    "                sigma = float(rng.choice(test_zerr_pool))\n",
    "                z_sim = max(0.0, z0 + float(rng.normal(0.0, sigma)))\n",
    "                feats2 = extract_features_for_object(\n",
    "                    lc_raw=lc,\n",
    "                    z=z_sim,\n",
    "                    z_err=sigma,\n",
    "                    ebv=r[\"EBV\"],\n",
    "                )\n",
    "                feats2[\"object_id\"] = obj\n",
    "                feats2[\"split\"] = split\n",
    "                feats2[\"target\"] = int(r[\"target\"])\n",
    "                feats2[\"photoz_aug\"] = 1\n",
    "                rows.append(feats2)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def clean_features(df, drop_cols, add_missing_flags=True):\n",
    "    X = df.drop(columns=drop_cols).copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if add_missing_flags:\n",
    "        miss = X.isna().astype(np.uint8)\n",
    "        miss.columns = [c + \"_isnan\" for c in miss.columns]\n",
    "        X = pd.concat([X, miss], axis=1)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27129862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold_f1(y_true, probs):\n",
    "    ths = np.linspace(0.01, 0.99, 401)\n",
    "    f1s = [f1_score(y_true, probs > t, zero_division=0) for t in ths]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return float(ths[j]), float(f1s[j])\n",
    "\n",
    "\n",
    "def make_splitter(n_splits, random_state=6):\n",
    "    return StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24de836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10, seed=6):\n",
    "\n",
    "    df = train_feat.merge(train_log[[\"object_id\", \"SpecType\"]], on=\"object_id\", how=\"left\")\n",
    "    spec = df[\"SpecType\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "    def map_group(s):\n",
    "        s2 = s.strip()\n",
    "        if s2 == \"TDE\":\n",
    "            return \"TDE\"\n",
    "        if s2 == \"AGN\":\n",
    "            return \"AGN\"\n",
    "        if \"SLSN\" in s2:\n",
    "            return \"SLSN\"\n",
    "        if s2 == \"SN Ia\" or s2.startswith(\"SN Ia\"):\n",
    "            return \"SNIa\"\n",
    "        if s2.startswith(\"SN II\") or (\"SN II\" in s2):\n",
    "            return \"SNII\"\n",
    "        if s2.startswith(\"SN\"):\n",
    "            return \"SNother\"\n",
    "        return \"Other\"\n",
    "\n",
    "    spec_group = spec.map(map_group).astype(str)\n",
    "\n",
    "    classes = sorted(spec_group.unique())\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_mc = spec_group.map(class_to_idx).to_numpy()\n",
    "\n",
    "    X_tr = clean_features(df, drop_cols=[\"object_id\", \"split\", \"target\", \"SpecType\"], add_missing_flags=True)\n",
    "    X_te = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"], add_missing_flags=True)\n",
    "\n",
    "    groups = df[\"split\"].to_numpy()\n",
    "\n",
    "    splitter = make_splitter(n_splits, random_state=seed)\n",
    "    split_iter = splitter.split(X_tr, y_mc, groups)\n",
    "\n",
    "    oof = np.zeros((len(X_tr), len(classes)), dtype=float)\n",
    "\n",
    "    base = dict(\n",
    "        objective=\"multiclass\",\n",
    "        num_class=len(classes),\n",
    "        metric=\"multi_logloss\",\n",
    "        n_estimators=20000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=63,\n",
    "        min_child_samples=5,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        verbosity=-1,\n",
    "        force_col_wise=True\n",
    "    )\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        model = LGBMClassifier(**base)\n",
    "        model.fit(\n",
    "            X_tr.iloc[tr_idx], y_mc[tr_idx],\n",
    "            eval_set=[(X_tr.iloc[va_idx], y_mc[va_idx])],\n",
    "            eval_metric=\"multi_logloss\",\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "        )\n",
    "        oof[va_idx] = model.predict_proba(X_tr.iloc[va_idx], num_iteration=model.best_iteration_)\n",
    "\n",
    "    model_full = LGBMClassifier(**base)\n",
    "    model_full.fit(X_tr, y_mc)\n",
    "    p_test = model_full.predict_proba(X_te)\n",
    "\n",
    "    def entropy(p):\n",
    "        p = np.clip(p, 1e-12, 1.0)\n",
    "        return -np.sum(p * np.log(p), axis=1)\n",
    "\n",
    "    for i, c in enumerate(classes):\n",
    "        train_feat[f\"p_spec_{c}\"] = oof[:, i]\n",
    "        test_feat[f\"p_spec_{c}\"] = p_test[:, i]\n",
    "\n",
    "    train_feat[\"spec_entropy\"] = entropy(oof)\n",
    "    test_feat[\"spec_entropy\"] = entropy(p_test)\n",
    "\n",
    "    train_feat[\"spec_topprob\"] = np.max(oof, axis=1)\n",
    "    test_feat[\"spec_topprob\"] = np.max(p_test, axis=1)\n",
    "\n",
    "    return train_feat, test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5755cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_gain_topk(train_feat, k=350, n_splits=10, seed=6):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "    X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "\n",
    "    splitter = make_splitter(n_splits, random_state=seed)\n",
    "    split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "    gains = {c: 0.0 for c in X.columns}\n",
    "\n",
    "    base_params = dict(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"aucpr\",\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        n_estimators=6000,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=6,\n",
    "        min_child_weight=10,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=2.0,\n",
    "        reg_lambda=2.0,\n",
    "        gamma=0.0,\n",
    "        max_bin=256,\n",
    "    )\n",
    "\n",
    "    for tr_idx, va_idx in split_iter:\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        spw = float(neg / max(1, pos))\n",
    "\n",
    "        model = XGBClassifier(**{**base_params, \"scale_pos_weight\": spw})\n",
    "        model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "        score = model.get_booster().get_score(importance_type=\"gain\")\n",
    "        for feat, g in score.items():\n",
    "            if feat in gains:\n",
    "                gains[feat] += float(g)\n",
    "\n",
    "    ranked = sorted(gains.items(), key=lambda x: x[1], reverse=True)\n",
    "    top = [f for f, _ in ranked[:k]]\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20251e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_xgb_f1(train_feat, feature_cols, n_folds_tune=10, timeout_sec=28800, seed=6):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X_all = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "    X = X_all[feature_cols].copy()\n",
    "\n",
    "    splitter = make_splitter(n_folds_tune, random_state=seed)\n",
    "    split_iter_all = list(splitter.split(X, y, groups))\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"aucpr\",\n",
    "            \"random_state\": seed,\n",
    "            \"n_jobs\": -1,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1500, 14000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.002, 0.08, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 80),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "            \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "            \"max_bin\": trial.suggest_int(\"max_bin\", 128, 512),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 12.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 35.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.05, 50.0),\n",
    "            \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "            \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "        }\n",
    "\n",
    "        if params[\"grow_policy\"] == \"lossguide\":\n",
    "            params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 512)\n",
    "\n",
    "        oof = np.zeros(len(X), dtype=float)\n",
    "        f1_progress = []\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(split_iter_all, 1):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "            neg = np.sum(y_tr == 0)\n",
    "            pos = np.sum(y_tr == 1)\n",
    "            spw = float(neg / max(1, pos))\n",
    "\n",
    "            model = XGBClassifier(**{**params, \"scale_pos_weight\": spw})\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_va, y_va)],\n",
    "                verbose=False,\n",
    "                callbacks=[xgb.callback.EarlyStopping(rounds=200, save_best=True)]\n",
    "            )\n",
    "\n",
    "            oof[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "            th_fold, f1_fold = best_threshold_f1(y_va, oof[va_idx])\n",
    "            f1_progress.append(f1_fold)\n",
    "\n",
    "            trial.report(float(np.mean(f1_progress)), step=fold)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        th, f1 = best_threshold_f1(y, oof)\n",
    "        return float(f1)\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=seed, multivariate=True, group=True)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=40, n_warmup_steps=3)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name=\"xgb_oof_f1_splitcv_gpu_selected\",\n",
    "        storage=\"sqlite:///optuna_xgb_oof_f1_gpu_selected.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=999999, timeout=timeout_sec)\n",
    "\n",
    "    print(\"\\nOptuna best OOF F1:\", study.best_value)\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(k, \"=\", v)\n",
    "\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acf37d",
   "metadata": {},
   "source": [
    "change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5fb7341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_fit_kaggle(model, X_tr, y_tr, X_va=None, y_va=None, verbose=False):\n",
    "    if X_va is not None and y_va is not None:\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=verbose)\n",
    "    else:\n",
    "        model.fit(X_tr, y_tr, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad3562",
   "metadata": {},
   "source": [
    "change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f993efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_xgb_multiseed(train_feat, test_feat, best_params, feature_cols, n_splits_oof=20, seeds=(6, 67, 6767)):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X_all = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "    X_test_all = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"], add_missing_flags=True)\n",
    "\n",
    "    X = X_all[feature_cols].copy()\n",
    "    X_test = X_test_all[feature_cols].copy()\n",
    "\n",
    "    splitter = make_splitter(n_splits_oof, random_state=6)\n",
    "    split_iter = list(splitter.split(X, y, groups))\n",
    "\n",
    "    oof = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        spw = float(neg / max(1, pos))\n",
    "\n",
    "        probs_va = []\n",
    "        for sd in seeds:\n",
    "            model = XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                eval_metric=\"logloss\",\n",
    "                random_state=sd,\n",
    "                n_jobs=-1,\n",
    "                tree_method=\"hist\",\n",
    "                device=\"cuda\",\n",
    "                scale_pos_weight=spw,\n",
    "                **best_params\n",
    "            )\n",
    "            xgb_fit_kaggle(model, X_tr, y_tr, X_va, y_va, verbose=False)\n",
    "            probs_va.append(model.predict_proba(X_va)[:, 1])\n",
    "\n",
    "        oof[va_idx] = np.mean(probs_va, axis=0)\n",
    "\n",
    "    best_th, best_f1 = best_threshold_f1(y, oof)\n",
    "    ap = average_precision_score(y, oof)\n",
    "    print(\"\\nOOF multiseed best threshold:\", best_th)\n",
    "    print(\"OOF multiseed best F1:\", best_f1)\n",
    "    print(\"OOF AP (aucpr-ish):\", ap)\n",
    "\n",
    "    probs_test = []\n",
    "    neg_full = np.sum(y == 0)\n",
    "    pos_full = np.sum(y == 1)\n",
    "    spw_full = float(neg_full / max(1, pos_full))\n",
    "\n",
    "    for sd in seeds:\n",
    "        model = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"aucpr\",\n",
    "            random_state=sd,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\",\n",
    "            device=\"cuda\",\n",
    "            scale_pos_weight=spw_full,\n",
    "            **best_params\n",
    "        )\n",
    "        xgb_fit_kaggle(model, X, y, verbose=False)\n",
    "        probs_test.append(model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    p_test = np.mean(probs_test, axis=0)\n",
    "    return p_test, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ff2aaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (9129, 559)\n",
      "test shape: (7135, 558)\n"
     ]
    }
   ],
   "source": [
    "N_AUG = 2\n",
    "FS_TOPK = 380\n",
    "FS_FOLDS = 10\n",
    "OPTUNA_FOLDS = 10\n",
    "OPTUNA_TIMEOUT_SEC = 28800\n",
    "FINAL_OOF_FOLDS = 20\n",
    "SEEDS = (6, 67, 6767)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parents[1]\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "train_log = pd.read_csv(DATA_DIR / \"train_log.csv\")\n",
    "test_log  = pd.read_csv(DATA_DIR / \"test_log.csv\")\n",
    "\n",
    "train_log[\"object_id\"] = train_log[\"object_id\"].astype(str)\n",
    "test_log[\"object_id\"] = test_log[\"object_id\"].astype(str)\n",
    "\n",
    "if \"Z_err\" not in train_log.columns:\n",
    "    train_log[\"Z_err\"] = 0.0\n",
    "train_log[\"Z_err\"] = train_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "if \"Z_err\" not in test_log.columns:\n",
    "    test_log[\"Z_err\"] = 0.0\n",
    "test_log[\"Z_err\"] = test_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "train_splits = sorted(train_log[\"split\"].unique())\n",
    "test_splits = sorted(test_log[\"split\"].unique())\n",
    "\n",
    "\n",
    "train_lc_cache, train_idx_cache = build_lightcurve_cache(\n",
    "    train_splits, base_dir=DATA_DIR, kind=\"train\"\n",
    ")\n",
    "\n",
    "test_lc_cache, test_idx_cache = build_lightcurve_cache(\n",
    "    test_splits, base_dir=DATA_DIR, kind=\"test\"\n",
    ")\n",
    "\n",
    "test_zerr_pool = test_log[\"Z_err\"].dropna().values\n",
    "\n",
    "train_feat = build_feature_table(\n",
    "    train_log, train_lc_cache, train_idx_cache,\n",
    "    augment_photoz=True,\n",
    "    test_zerr_pool=test_zerr_pool,\n",
    "    n_aug=N_AUG,\n",
    "    seed=6\n",
    ")\n",
    "\n",
    "test_feat = build_feature_table(\n",
    "    test_log, test_lc_cache, test_idx_cache,\n",
    "    augment_photoz=False\n",
    ")\n",
    "print(f\"train shape: {train_feat.shape}\")\n",
    "print(f\"test shape: {test_feat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1d75763",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat, test_feat = add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10, seed=6)\n",
    "selected_cols = feature_select_gain_topk(train_feat, k=FS_TOPK, n_splits=FS_FOLDS, seed=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = run_optuna_xgb_f1(\n",
    "    train_feat,\n",
    "    feature_cols=selected_cols,\n",
    "    n_folds_tune=OPTUNA_FOLDS,\n",
    "    timeout_sec=OPTUNA_TIMEOUT_SEC,\n",
    "    seed=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee18984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 9476,\n",
    "               'learning_rate': 0.0024306289953670325,\n",
    "               'max_depth': 7, 'min_child_weight': 6,\n",
    "               'subsample': 0.5344962939912224,\n",
    "               'colsample_bytree': 0.464696420753079,\n",
    "               'colsample_bylevel': 0.8146569410634974,\n",
    "               'colsample_bynode': 0.7285475291884695,\n",
    "               'max_bin': 181, 'gamma': 8.476938947246458,\n",
    "               'reg_alpha': 0.44957196104419117,\n",
    "               'reg_lambda': 5.23806334613521,\n",
    "               'max_delta_step': 0,\n",
    "               'grow_policy': 'depthwise'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a738b14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OOF multiseed best threshold: 0.419\n",
      "OOF multiseed best F1: 0.6243705941591138\n",
      "OOF AP (aucpr-ish): 0.5164263302782434\n"
     ]
    }
   ],
   "source": [
    "p_test, best_th = predict_xgb_multiseed(\n",
    "    train_feat,\n",
    "    test_feat,\n",
    "    best_params,\n",
    "    feature_cols=selected_cols,\n",
    "    n_splits_oof=min(FINAL_OOF_FOLDS, len(train_splits)),\n",
    "    seeds=(99, 999, 909)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7116f",
   "metadata": {},
   "source": [
    "OOF multiseed best threshold: 0.402\n",
    "OOF multiseed best F1: 0.6187624750499002\n",
    "OOF AP (aucpr-ish): 0.5153549152691365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_feat = pd.read_parquet(\"artifacts/train_feat.parquet\")\n",
    "test_feat  = pd.read_parquet(\"artifacts/test_feat.parquet\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bad79a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGB_multiseed_teacher5.csv  threshold: 0.419\n"
     ]
    }
   ],
   "source": [
    "test_pred = (p_test > best_th).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"object_id\": test_feat[\"object_id\"].values,\n",
    "    \"target\": test_pred\n",
    "})\n",
    "out_name = \"XGB_multiseed_teacher5.csv\"\n",
    "sub.to_csv(out_name, index=False)\n",
    "print(\"Saved\", out_name, \" threshold:\", best_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53e901cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGB_multiseed_teacher.csv  threshold: 0.402\n",
      "Predicted positive rate: 0.05199719691660827\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "BEST_PARAMS = {'n_estimators': 11766, 'learning_rate': 0.024381791324748325, 'max_depth': 4, 'min_child_weight': 23, 'subsample': 0.6004305768627155, 'colsample_bytree': 0.594854201978887, 'colsample_bylevel': 0.8241023733283193, 'colsample_bynode': 0.5372150805015081, 'max_bin': 406, 'gamma': 11.596780399929514, 'reg_alpha': 4.092494873246832, 'reg_lambda': 10.592102344523834, 'max_delta_step': 8, 'grow_policy': 'lossguide', 'max_leaves': 16}\n",
    "\n",
    "THRESH = 0.402\n",
    "\n",
    "SEEDS = (563,)\n",
    "\n",
    "y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "\n",
    "X_all = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"], add_missing_flags=True)\n",
    "X_test_all = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"], add_missing_flags=True)\n",
    "\n",
    "X = X_all.reindex(columns=selected_cols, fill_value=0.0)\n",
    "X_test = X_test_all.reindex(columns=selected_cols, fill_value=0.0)\n",
    "\n",
    "neg = np.sum(y == 0)\n",
    "pos = np.sum(y == 1)\n",
    "spw_full = float(neg / max(1, pos))\n",
    "\n",
    "probs_test = []\n",
    "\n",
    "for sd in SEEDS:\n",
    "    model = XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"aucpr\",\n",
    "        random_state=sd,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        scale_pos_weight=spw_full,\n",
    "        **BEST_PARAMS\n",
    "    )\n",
    "\n",
    "    if \"xgb_fit_kaggle\" in globals() and callable(xgb_fit_kaggle):\n",
    "        xgb_fit_kaggle(model, X, y, verbose=False)\n",
    "    else:\n",
    "        model.fit(X, y, verbose=False)\n",
    "\n",
    "    probs_test.append(model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "p_test = np.mean(probs_test, axis=0)\n",
    "\n",
    "test_pred = (p_test > THRESH).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"object_id\": test_feat[\"object_id\"].values,\n",
    "    \"target\": test_pred\n",
    "})\n",
    "\n",
    "out_name = \"XGB_multiseed_teacher.csv\"\n",
    "sub.to_csv(out_name, index=False)\n",
    "\n",
    "print(\"Saved\", out_name, \" threshold:\", THRESH)\n",
    "print(\"Predicted positive rate:\", test_pred.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eaad11",
   "metadata": {},
   "source": [
    "F1: 0.81742\n",
    "ROC-AUC: 0.99906\n",
    "PR-AUC: 0.98116\n",
    "Accuracy: 0.97842\n",
    "Threshold: 0.42\n",
    "OOF positive rate: 0.06955854967685399\n",
    "\n",
    "Saved XGB_multiseed_teacher4.csv\n",
    "Test positive rate: 0.051156271899089"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb910d9",
   "metadata": {},
   "source": [
    "Saved XGB_multiseed_teacher.csv  threshold: 0.42\n",
    "Predicted positive rate: 0.051296426068675544"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
