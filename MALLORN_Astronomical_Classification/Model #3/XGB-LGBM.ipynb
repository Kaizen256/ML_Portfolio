{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68cd4200",
   "metadata": {},
   "source": [
    "# Model 3 (XGB/LGBM Blend)\n",
    "\n",
    "This model was an attempt to push performance by combining:\n",
    "- a stronger feature set,\n",
    "- photometric redshift augmentation using `Z_err`,\n",
    "- an ensemble with both XGBoost and LightGBM,\n",
    "- and an OOF-tuned blend weight (alpha) + decision threshold.\n",
    "\n",
    "## Results\n",
    "\n",
    "Best parameters:\n",
    "- n_estimators: 4328\n",
    "- learning_rate: 0.007079630495604182\n",
    "- max_depth: 4\n",
    "- min_child_weight: 1\n",
    "- subsample: 0.5936362024881353\n",
    "- colsample_bytree: 0.9146736072473098\n",
    "- gamma: 0.6466321530023438\n",
    "- reg_alpha: 4.130135428812432\n",
    "- reg_lambda: 5.5649710878468825\n",
    "- max_delta_step: 1\n",
    "- grow_policy: depthwise\n",
    "\n",
    "OOF multiseed best threshold: 0.01  \n",
    "OOF multiseed best F1: 0.51875  \n",
    "OOF best alpha: 0.03  \n",
    "\n",
    "| Submission | Public LB F1 | Private LB F1 |\n",
    "|-------------|--------------|----------------|\n",
    "| 1 | 0.5613 | 0.5313 |\n",
    "| 2 | 0.5082 | 0.5119 |\n",
    "\n",
    "## What changed vs Model 2\n",
    "- Added Z_err usage and photo-z augmentation\n",
    "- Added a LightGBM model alongside XGBoost.\n",
    "- Added OOF blending: `p = alpha * p_xgb + (1 - alpha) * p_lgb`.\n",
    "- Optuna tuning target changed to PR AUC inside CV, while thresholding is still tuned for F1.\n",
    "\n",
    "## Likely reasons this underperformed\n",
    "- The OOF-optimal blend weight was alpha = 0.03, meaning the blend heavily preferred one model (mostly LGB).\n",
    "- The OOF-optimal threshold was extremely low (0.01), suggesting probability calibration / distribution mismatch.\n",
    "- Optimizing PR AUC but selecting thresholds by F1 can create objective mismatch.\n",
    "- Ensembles seem to cause noise. I later checked with other people in the top 100 at the end of the competition. Some said ensembles lowered LB results, and some said they were using 100+ ensembles in their models and getting great results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923bcf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "from extinction import fitzpatrick99\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7c727",
   "metadata": {},
   "source": [
    "## Filter and extinction constants\n",
    "\n",
    "- `FILTERS`: the 6 LSST bands used in the lightcurves (u,g,r,i,z,y)\n",
    "- `EFF_WL_AA`: effective wavelength (Angstrom) per band\n",
    "- `R_V`: extinction curve parameter used to convert EBV -> A_V\n",
    "- `STETSON_DT_MAX`: max time separation for pairing points in the Stetson J variability statistic\n",
    "\n",
    "These constants support consistent de-extinction and time-series variability calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4d2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "EFF_WL_AA = {\n",
    "    \"u\": 3641.0,\n",
    "    \"g\": 4704.0,\n",
    "    \"r\": 6155.0,\n",
    "    \"i\": 7504.0,\n",
    "    \"z\": 8695.0,\n",
    "    \"y\": 10056.0,\n",
    "}\n",
    "\n",
    "R_V = 3.1\n",
    "STETSON_DT_MAX = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866c90f",
   "metadata": {},
   "source": [
    "### Data safety + numerics\n",
    "- `safe_float`: safe conversion that handles missing / NaN values\n",
    "- `trapz_safe`: numerical integration\n",
    "\n",
    "### Robust statistics\n",
    "- MAD, IQR, skewness, kurtosis\n",
    "- von Neumann eta (variability / smoothness)\n",
    "\n",
    "### Time-series dynamics\n",
    "- slope summaries (`max_slope`, `median_abs_slope`, `linear_slope`)\n",
    "- goodness-of-fit to constant (`chi2_to_constant`)\n",
    "- interpolation for color-at-time features (`interp_flux_at_time`)\n",
    "\n",
    "### Variability metrics\n",
    "- `stetson_J`: correlated variability statistic for closely-spaced observations\n",
    "- `fractional_variability`: noise-corrected intrinsic variability amplitude\n",
    "\n",
    "### De-extinction (dust correction)\n",
    "- `deextinct_band` and `deextinct_lightcurve` correct flux + flux_err using EBV and wavelength-dependent extinction.\n",
    "\n",
    "These functions are building blocks for the per-object feature extraction function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87699215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return default\n",
    "        x = float(x)\n",
    "        if np.isnan(x):\n",
    "            return default\n",
    "        return x\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def trapz_safe(y, x):\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return np.trapezoid(y, x)\n",
    "    y = np.asarray(y)\n",
    "    x = np.asarray(x)\n",
    "    return np.sum((x[1:] - x[:-1]) * (y[1:] + y[:-1]) * 0.5)\n",
    "\n",
    "\n",
    "def median_abs_dev(x):\n",
    "    x = np.asarray(x)\n",
    "    med = np.median(x)\n",
    "    return float(np.median(np.abs(x - med)))\n",
    "\n",
    "\n",
    "def iqr(x):\n",
    "    q75, q25 = np.percentile(x, [75, 25])\n",
    "    return float(q75 - q25)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    x = np.asarray(x)\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m3 = np.mean((x - mu) ** 3)\n",
    "    return float(m3 / (s ** 3))\n",
    "\n",
    "\n",
    "def kurtosis_excess(x):\n",
    "    x = np.asarray(x)\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m4 = np.mean((x - mu) ** 4)\n",
    "    return float(m4 / (s ** 4) - 3.0)\n",
    "\n",
    "\n",
    "def von_neumann_eta(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    v = np.var(x)\n",
    "    if v < 1e-12:\n",
    "        return 0.0\n",
    "    dif = np.diff(x)\n",
    "    return float(np.mean(dif ** 2) / v)\n",
    "\n",
    "\n",
    "def max_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.max(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def median_abs_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.median(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def linear_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    try:\n",
    "        a, b = np.polyfit(t, f, 1)\n",
    "        return float(a)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def chi2_to_constant(f, ferr):\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.median(f)\n",
    "    denom = (ferr + 1e-8) ** 2\n",
    "    chi2 = np.sum((f - mu) ** 2 / denom)\n",
    "    dof = max(1, n - 1)\n",
    "    return float(chi2 / dof)\n",
    "\n",
    "\n",
    "def interp_flux_at_time(tb, fb, t0):\n",
    "    # returns flux at t0 using linear interpolation\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan\n",
    "    if (t0 < tb.min()) or (t0 > tb.max()):\n",
    "        return np.nan\n",
    "    return float(np.interp(t0, tb, fb))\n",
    "\n",
    "\n",
    "def stetson_J(t, f, ferr, dt_max=STETSON_DT_MAX):\n",
    "    \"\"\"\n",
    "    Simplified Stetson J:\n",
    "    Pair consecutive observations that are close in time.\n",
    "    delta_i = sqrt(n/(n-1)) * (f_i - mean_f) / err_i\n",
    "    J = mean( sign(P_k)*sqrt(|P_k|) ) where P_k = delta_i * delta_j\n",
    "    \"\"\"\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "\n",
    "    n = len(t)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "\n",
    "    mu = np.mean(f)\n",
    "    scale = np.sqrt(n / max(1, n - 1))\n",
    "    delta = scale * (f - mu) / (ferr + 1e-8)\n",
    "\n",
    "    pairs = []\n",
    "    for i in range(n - 1):\n",
    "        if (t[i + 1] - t[i]) <= dt_max:\n",
    "            pairs.append((i, i + 1))\n",
    "\n",
    "    if len(pairs) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    vals = []\n",
    "    for i, j in pairs:\n",
    "        P = delta[i] * delta[j]\n",
    "        vals.append(np.sign(P) * np.sqrt(np.abs(P)))\n",
    "\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "\n",
    "def fractional_variability(f, ferr):\n",
    "    \"\"\"\n",
    "    Noise-corrected intrinsic variability amplitude:\n",
    "    F_var = sqrt(max(0, S^2 - mean(err^2))) / |mean(f)|\n",
    "    \"\"\"\n",
    "    f = np.asarray(f, float)\n",
    "    ferr = np.asarray(ferr, float)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "\n",
    "    mu = np.mean(f)\n",
    "    if np.abs(mu) < 1e-8:\n",
    "        return np.nan\n",
    "\n",
    "    s2 = np.var(f, ddof=1)\n",
    "    mean_err2 = np.mean(ferr**2)\n",
    "\n",
    "    excess = max(0.0, s2 - mean_err2)\n",
    "    return float(np.sqrt(excess) / np.abs(mu))\n",
    "\n",
    "\n",
    "def deextinct_band(flux, flux_err, ebv, band, r_v=R_V):\n",
    "    if ebv is None or (isinstance(ebv, float) and np.isnan(ebv)):\n",
    "        return flux, flux_err, 0.0\n",
    "\n",
    "    A_V = float(ebv) * float(r_v)\n",
    "    wave = np.array([EFF_WL_AA[band]], dtype=float)  # Angstrom\n",
    "    A_lambda = float(fitzpatrick99(wave, A_V, r_v=r_v, unit=\"aa\")[0])  # mag\n",
    "\n",
    "    fac = 10.0 ** (0.4 * A_lambda)\n",
    "    return flux * fac, flux_err * fac, A_lambda\n",
    "\n",
    "\n",
    "def deextinct_lightcurve(lc, ebv):\n",
    "    flux = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    ferr = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    flux_corr = flux.copy()\n",
    "    ferr_corr = ferr.copy()\n",
    "\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        flux_corr[m], ferr_corr[m], _ = deextinct_band(flux_corr[m], ferr_corr[m], ebv, b)\n",
    "\n",
    "    return flux_corr, ferr_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bcf701",
   "metadata": {},
   "source": [
    "# Model 3: Expanded variability + per-band diagnostics + color-evolution features\n",
    "\n",
    "Differences vs Model 2:\n",
    " - Adds photo-z uncertainty features (Z_err, log1pZerr) so the model can learn when redshift is unreliable.\n",
    " - Adds stronger variability diagnostics: chi2-to-constant, Stetson J, fractional variability (global + per-band).\n",
    " - Adds robust amplitude features (p95 - p5) globally and per-band to reduce sensitivity to outliers.\n",
    " - Computes widths/rise/decay/asymmetry at 50% amplitude in both observed + rest frame (more detailed transient shape).\n",
    " - Adds color features (g-r and r-i) at r-band peak and their 10-day color slopes (captures spectral evolution).\n",
    "\n",
    "## Global (all-filters combined) features\n",
    "\n",
    "These are computed using all observations across all bands for a given object.  \n",
    "They summarize time coverage, brightness distribution, cadence, variability, and context (redshift + dust + redshift uncertainty).\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_obs` | Total number of observations across all filters | Captures overall sampling density and how well-measured the object is |\n",
    "| `total_time_obs` | Observed-frame time baseline: `max(t_rel) - min(t_rel)` | Separates short transients vs long events and measures overall monitoring duration |\n",
    "| `total_time_rest` | Rest-frame time baseline: `total_time_obs / (1+z)` | Removes time dilation so the model compares intrinsic evolution speed across redshifts |\n",
    "\n",
    "### Flux distribution (dust-corrected `flux_corr`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `flux_mean` | Mean corrected flux | Measures average intrinsic brightness level (sensitive to sustained high flux) |\n",
    "| `flux_median` | Median corrected flux | Robust typical brightness baseline (less sensitive to one-off spikes) |\n",
    "| `flux_std` | Standard deviation of corrected flux | Captures variability strength (high = more change over time) |\n",
    "| `flux_min` | Minimum corrected flux | Captures deep fades / dips / negative excursions from noise-subtraction artifacts |\n",
    "| `flux_max` | Maximum corrected flux | Captures peak brightness or flare intensity (key transient signature) |\n",
    "| `flux_mad` | Median absolute deviation of corrected flux | Robust variability estimate that doesn’t get bullied by outliers |\n",
    "| `flux_iqr` | Interquartile range of corrected flux | Another robust variability measure (spread of the middle 50%) |\n",
    "| `flux_skew` | Skewness of corrected flux distribution | Detects asymmetric lightcurves (fast rise / slow decay vs vice versa) |\n",
    "| `flux_kurt_excess` | Excess kurtosis of corrected flux distribution | Detects heavy tails/spiky behavior from rare bursts or sharp transients |\n",
    "| `flux_p5` | 5th percentile of corrected flux | Robust low-end brightness level (less sensitive than min) |\n",
    "| `flux_p25` | 25th percentile of corrected flux | Lower-quartile brightness level |\n",
    "| `flux_p75` | 75th percentile of corrected flux | Upper-quartile brightness level |\n",
    "| `flux_p95` | 95th percentile of corrected flux | Robust high-end brightness level (less sensitive than max) |\n",
    "| `robust_amp_global` | Robust amplitude: `flux_p95 - flux_p5` | Outlier-resistant variability scale, often better than max-min |\n",
    "| `neg_flux_frac` | Fraction of corrected flux values `< 0` | Flags noise-dominated objects or weak detections where measurements hover around zero |\n",
    "\n",
    "### SNR (using corrected errors `err_corr`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `snr_median` | Median SNR where `snr = \\|flux_corr\\| / (err_corr + 1e-8)` | Typical detection quality (separates clean signals from noisy junk) |\n",
    "| `snr_max` | Maximum SNR | Captures the strongest detection event (some transients “light up” briefly) |\n",
    "\n",
    "### Cadence / gaps\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `median_dt` | Median time gap between consecutive observations in `t_rel` | Describes typical cadence (important since sparse sampling hides shape) |\n",
    "| `max_gap` | Maximum time gap between consecutive observations in `t_rel` | Detects missing windows (large gaps can explain unreliable peak/width estimates) |\n",
    "\n",
    "### Time-series variability / shape\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `eta_von_neumann` | Von Neumann eta statistic on `flux_corr` (smoothness vs jumpiness) | Separates smooth evolving curves from noisy jitter or sudden jumps |\n",
    "| `chi2_const_global` | Chi-square vs constant-flux model using `err_corr` | Quantifies variability relative to measurement noise (true variability vs noise) |\n",
    "| `stetsonJ_global_obs` | Stetson J index using observed-frame times | Captures correlated variability behavior (often strong for real transients) |\n",
    "| `stetsonJ_global_rest` | Stetson J index using rest-frame times | Same idea, but corrected for time dilation so timing-related correlation is comparable |\n",
    "\n",
    "### Slopes / rate of change (global)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `max_slope_global_obs` | Maximum absolute slope in observed time (`t_rel`) | Captures fastest brightness change (sharp rise/fall events) |\n",
    "| `max_slope_global_rest` | Maximum absolute slope in rest-frame time (`t_rest`) | Intrinsic fastest change rate (removes redshift stretching) |\n",
    "| `med_abs_slope_global_obs` | Median absolute slope in observed time | Typical observed change rate (slow drifters vs active transients) |\n",
    "| `med_abs_slope_global_rest` | Median absolute slope in rest-frame time | Typical intrinsic change rate |\n",
    "| `slope_global_obs` | Best-fit linear slope over observed time | Captures long-term trend direction (rising vs fading overall) |\n",
    "| `slope_global_rest` | Best-fit linear slope over rest-frame time | Same trend, but comparable across redshifts |\n",
    "\n",
    "### Fractional variability\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `fvar_global` | Fractional variability accounting for measurement errors | Estimates intrinsic variability strength after subtracting noise contribution |\n",
    "\n",
    "### Context metadata\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `Z` | Redshift `z` | Encodes distance/epoch effects and shifts events into different observed regimes |\n",
    "| `log1pZ` | `log(1+z)` | Stabilizes redshift scaling for models (less extreme leverage at high `z`) |\n",
    "| `Z_err` | Redshift uncertainty (clipped to `>= 0`) | Captures confidence in rest-frame correction; noisy redshifts degrade timing features |\n",
    "| `log1pZerr` | `log(1+Z_err)` | Stabilizes uncertainty scaling and helps tree models split more smoothly |\n",
    "| `EBV` | Dust reddening used for extinction correction | Helps the model learn residual dust systematics and measurement conditions |\n",
    "\n",
    "### Filter coverage\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_filters_present` | Number of filters with ≥ 1 observation | Multi-band coverage gives richer color/shape info; missing bands can correlate with class |\n",
    "| `total_obs` | Total observations summed across all filters (same as `n_obs`) | Redundant but convenient sanity/coverage signal that some models exploit |\n",
    "\n",
    "## Per-filter (band-wise) features\n",
    "\n",
    "For each band `b ∈ {u,g,r,i,z,y}`, the following features are computed independently per filter.  \n",
    "These capture color-dependent brightness behavior and band-specific temporal dynamics.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_{b}` | Number of observations in band `b` | Band missingness and sampling density vary by object/class and affect reliability |\n",
    "| `amp_{b}` | Amplitude above baseline: `max(fb) - median(fb)` | Measures event strength in that band (key for color-specific transient signatures) |\n",
    "| `robust_amp_{b}` | Robust amplitude: `p95_b - p5_b` | More stable amplitude estimate when peaks/outliers are noisy |\n",
    "| `tpeak_{b}_obs` | Observed-frame time of peak flux in band `b` | Captures when the band reaches maximum brightness (timing is class-dependent) |\n",
    "| `tpeak_{b}_rest` | Rest-frame time of peak flux: `tpeak_obs / (1+z)` | Removes time dilation so peak timing is comparable across redshifts |\n",
    "| `width50_{b}_obs` | Observed-frame width above 50% amplitude | Measures mid-brightness duration in observed time (useful when timing is observationally relevant) |\n",
    "| `width50_{b}_rest` | Rest-frame width above 50% amplitude | Intrinsic mid-brightness duration (fast vs slow transients) |\n",
    "| `width80_{b}_obs` | Observed-frame width above 80% amplitude | Captures core peak width in observed time (sharp vs broad peaks) |\n",
    "| `width80_{b}_rest` | Rest-frame width above 80% amplitude | Intrinsic core peak width (class-discriminative) |\n",
    "| `rise50_{b}_obs` | Observed-frame time from 50% rise crossing to peak | Encodes rise speed in observed time (cadence-aware) |\n",
    "| `decay50_{b}_obs` | Observed-frame time from peak to 50% decay crossing | Encodes decay speed in observed time |\n",
    "| `asym50_{b}_obs` | Rise/decay asymmetry: `rise50 / (decay50 + 1e-8)` | Separates fast-rise slow-decay vs slow-rise fast-decay shapes |\n",
    "| `rise50_{b}_rest` | Rest-frame time from 50% rise crossing to peak | Intrinsic rise timescale per band |\n",
    "| `decay50_{b}_rest` | Rest-frame time from peak to 50% decay crossing | Intrinsic decay timescale per band |\n",
    "| `asym50_{b}_rest` | Rest-frame rise/decay asymmetry | Intrinsic shape asymmetry (less biased by redshift) |\n",
    "| `auc_pos_{b}_obs` | Observed-frame AUC of positive signal: `∫ max(fb - baseline, 0) dt` | Energy-like summary in observed time (cadence and time dilation included) |\n",
    "| `auc_pos_{b}_rest` | Rest-frame AUC of positive signal | Energy-like summary comparable across redshifts |\n",
    "| `snrmax_{b}` | Maximum SNR within band `b` | Strongest detection in that band (some classes peak strongly only in certain filters) |\n",
    "| `eta_{b}` | Von Neumann eta within band `b` | Detects smooth evolution vs noise inside a single wavelength band |\n",
    "| `chi2_const_{b}` | Chi-square vs constant-flux model within band | Measures variability significance relative to band-specific noise |\n",
    "| `slope_{b}_obs` | Best-fit linear slope in band over observed time | Captures overall rise/fade trend per band |\n",
    "| `slope_{b}_rest` | Best-fit linear slope in band over rest-frame time | Intrinsic trend per band (comparable across redshifts) |\n",
    "| `maxslope_{b}_obs` | Maximum absolute slope in band (observed time) | Captures sharpest observed change (rise/fall) per band |\n",
    "| `maxslope_{b}_rest` | Maximum absolute slope in band (rest time) | Captures sharpest intrinsic change rate per band |\n",
    "| `stetsonJ_{b}_obs` | Stetson J in band using observed time | Detects correlated variability patterns per band |\n",
    "| `stetsonJ_{b}_rest` | Stetson J in band using rest-frame time | Same, but corrected for time dilation |\n",
    "| `p5_{b}` | 5th percentile of band flux `fb` | Robust low-end level per band |\n",
    "| `p25_{b}` | 25th percentile of `fb` | Lower-quartile level per band |\n",
    "| `p75_{b}` | 75th percentile of `fb` | Upper-quartile level per band |\n",
    "| `p95_{b}` | 95th percentile of `fb` | Robust high-end level per band |\n",
    "| `mad_{b}` | Median absolute deviation of `fb` | Robust band variability (outlier-resistant) |\n",
    "| `iqr_{b}` | Interquartile range of `fb` | Robust spread of the middle 50% per band |\n",
    "| `mad_over_std_{b}` | `mad_b / (std_b + 1e-8)` | Flags spike-dominated vs Gaussian-like variability (robustness/shape cue) |\n",
    "| `fvar_{b}` | Fractional variability within band (noise-corrected) | Intrinsic variability strength per band |\n",
    "\n",
    "## Cross-band pair features (adjacent pairs: `ug, gr, ri, iz, zy`)\n",
    "\n",
    "For each adjacent filter pair `(a,b)`, these compare amplitude, timing, and peak ratios across wavelengths.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `ampdiff_{a}{b}` | Amplitude difference: `amp_a - amp_b` | Captures color gradients and temperature evolution signatures between bands |\n",
    "| `tpeakdiff_{a}{b}_obs` | Observed-frame peak time difference: `tpeak_a_obs - tpeak_b_obs` | Chromatic peak lag/lead in observed time (also reflects cadence + time dilation) |\n",
    "| `tpeakdiff_{a}{b}_rest` | Rest-frame peak time difference: `tpeak_a_rest - tpeak_b_rest` | Measures intrinsic chromatic peak lag/lead (more physically comparable) |\n",
    "| `peakratio_{a}{b}` | Peak flux ratio: `peak_flux_a / (peak_flux_b + 1e-8)` | Strong color/SED proxy without needing explicit magnitudes |\n",
    "\n",
    "## Color features at r-peak (observed-frame) + 10-day color evolution\n",
    "\n",
    "These interpolate `g`, `r`, `i` flux at the observed time when the r-band peaks (`tpeak_r_obs`), then compute log-flux colors.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `color_gr_at_rpeak_obs` | `log1p(f_g) - log1p(f_r)` evaluated at `tpeak_r_obs` | Measures g-r color at peak, which is highly class-dependent |\n",
    "| `color_ri_at_rpeak_obs` | `log1p(f_r) - log1p(f_i)` evaluated at `tpeak_r_obs` | Measures r-i color at peak (temperature / SED proxy) |\n",
    "| `color_gr_slope10_obs` | `(color_gr(t+10) - color_gr(t)) / 10` days | Captures how color evolves after peak (cooling/heating signatures) |\n",
    "| `color_ri_slope10_obs` | `(color_ri(t+10) - color_ri(t)) / 10` days | Another post-peak color evolution cue (very discriminative for transients) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665c7a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments added through AI\n",
    "def extract_features_for_object(lc_raw, z, z_err, ebv):\n",
    "    feats = {}\n",
    "\n",
    "    # Sort observations by time so time-based calculations make sense\n",
    "    lc = lc_raw.sort_values(\"Time (MJD)\").reset_index(drop=True)\n",
    "\n",
    "    # Extract time values and filter (band) labels\n",
    "    t = lc[\"Time (MJD)\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    # If there are no observations, return minimal info\n",
    "    if len(t) == 0:\n",
    "        feats[\"n_obs\"] = 0\n",
    "        return feats\n",
    "\n",
    "    # Make sure metadata fields are valid numbers (avoid NaNs / strings / missing values)\n",
    "    z = safe_float(z, default=0.0)                     # redshift (distance proxy)\n",
    "    z_err = safe_float(z_err, default=0.0)             # redshift uncertainty\n",
    "    ebv = safe_float(ebv, default=np.nan)              # dust amount (can be missing)\n",
    "\n",
    "    # Convert time to start at 0 (relative time axis)\n",
    "    t_rel = t - t.min()\n",
    "\n",
    "    # Convert observed time to intrinsic time of the object\n",
    "    # Distant objects appear to evolve slower, so divide by (1 + z)\n",
    "    t_rest = t_rel / (1.0 + z)\n",
    "\n",
    "    # Correct brightness values for dust in the Milky Way\n",
    "    # (dust makes objects look dimmer than they really are)\n",
    "    flux_corr, err_corr = deextinct_lightcurve(lc, ebv)\n",
    "\n",
    "    # Basic observation statistics\n",
    "    feats[\"n_obs\"] = int(len(t))                                  # total number of measurements\n",
    "    feats[\"total_time_obs\"] = float(t_rel.max() - t_rel.min())    # total observed duration\n",
    "    feats[\"total_time_rest\"] = float(t_rest.max() - t_rest.min()) # duration corrected for distance effects\n",
    "\n",
    "    # Global brightness statistics (after dust correction)\n",
    "    feats[\"flux_mean\"] = float(np.mean(flux_corr))                # average brightness level\n",
    "    feats[\"flux_median\"] = float(np.median(flux_corr))            # robust typical brightness\n",
    "    feats[\"flux_std\"] = float(np.std(flux_corr))                  # overall variability\n",
    "    feats[\"flux_min\"] = float(np.min(flux_corr))                  # dimmest point\n",
    "    feats[\"flux_max\"] = float(np.max(flux_corr))                  # brightest point\n",
    "\n",
    "    # Robust statistics that are less sensitive to outliers\n",
    "    feats[\"flux_mad\"] = median_abs_dev(flux_corr)                 # median absolute deviation\n",
    "    feats[\"flux_iqr\"] = iqr(flux_corr)                            # interquartile range (Q3 - Q1)\n",
    "\n",
    "    # Distribution shape features\n",
    "    feats[\"flux_skew\"] = skewness(flux_corr)                      # asymmetry of values\n",
    "    feats[\"flux_kurt_excess\"] = kurtosis_excess(flux_corr)        # tail heaviness / spikiness\n",
    "\n",
    "    # Robust amplitude using percentiles (stable against a few extreme points)\n",
    "    p5, p25, p75, p95 = np.percentile(flux_corr, [5, 25, 75, 95])\n",
    "    feats[\"flux_p5\"] = float(p5)\n",
    "    feats[\"flux_p25\"] = float(p25)\n",
    "    feats[\"flux_p75\"] = float(p75)\n",
    "    feats[\"flux_p95\"] = float(p95)\n",
    "    feats[\"robust_amp_global\"] = float(p95 - p5)                  # robust amplitude proxy\n",
    "\n",
    "    # Fraction of measurements that are below zero\n",
    "    # (often indicates noise-dominated detections)\n",
    "    feats[\"neg_flux_frac\"] = float(np.mean(flux_corr < 0))\n",
    "\n",
    "    # Signal-to-noise ratio summaries\n",
    "    snr = np.abs(flux_corr) / (err_corr + 1e-8)\n",
    "    feats[\"snr_median\"] = float(np.median(snr))                   # typical signal quality\n",
    "    feats[\"snr_max\"] = float(np.max(snr))                         # strongest detection\n",
    "\n",
    "    # Observation timing properties\n",
    "    if len(t_rel) >= 2:\n",
    "        dt = np.diff(t_rel)\n",
    "        feats[\"median_dt\"] = float(np.median(dt))                 # typical time between observations\n",
    "        feats[\"max_gap\"] = float(np.max(dt))                      # largest observation gap\n",
    "    else:\n",
    "        feats[\"median_dt\"] = np.nan                               # undefined with <2 points\n",
    "        feats[\"max_gap\"] = np.nan\n",
    "\n",
    "    # Global time-series shape / variability diagnostics\n",
    "    feats[\"eta_von_neumann\"] = von_neumann_eta(flux_corr)              # smoothness vs noise proxy\n",
    "    feats[\"chi2_const_global\"] = chi2_to_constant(flux_corr, err_corr) # variability vs constant model\n",
    "    feats[\"stetsonJ_global_obs\"] = stetson_J(t_rel, flux_corr, err_corr)  # correlated variability (obs frame)\n",
    "    feats[\"stetsonJ_global_rest\"] = stetson_J(t_rest, flux_corr, err_corr) # correlated variability (rest frame)\n",
    "\n",
    "    # Global slope features (obs + rest frame)\n",
    "    feats[\"max_slope_global_obs\"] = max_slope(t_rel, flux_corr)        # fastest brightness change (obs)\n",
    "    feats[\"max_slope_global_rest\"] = max_slope(t_rest, flux_corr)      # fastest brightness change (rest)\n",
    "\n",
    "    feats[\"med_abs_slope_global_obs\"] = median_abs_slope(t_rel, flux_corr)   # typical change rate (obs)\n",
    "    feats[\"med_abs_slope_global_rest\"] = median_abs_slope(t_rest, flux_corr) # typical change rate (rest)\n",
    "\n",
    "    feats[\"slope_global_obs\"] = linear_slope(t_rel, flux_corr)         # best-fit linear trend (obs)\n",
    "    feats[\"slope_global_rest\"] = linear_slope(t_rest, flux_corr)       # best-fit linear trend (rest)\n",
    "\n",
    "    # Fractional variability (accounts for measurement noise)\n",
    "    feats[\"fvar_global\"] = fractional_variability(flux_corr, err_corr)\n",
    "\n",
    "    # Metadata features\n",
    "    feats[\"Z\"] = float(z)                           # distance proxy (redshift)\n",
    "    feats[\"log1pZ\"] = float(np.log1p(max(0.0, z)))  # compressed redshift scale\n",
    "    feats[\"Z_err\"] = float(max(0.0, z_err))         # clamp negative uncertainty to 0\n",
    "    feats[\"log1pZerr\"] = float(np.log1p(max(0.0, feats[\"Z_err\"])))  # compressed uncertainty scale\n",
    "    feats[\"EBV\"] = ebv                              # dust amount\n",
    "\n",
    "    # Counters for band coverage\n",
    "    feats[\"n_filters_present\"] = 0                  # how many bands have >= 1 observation\n",
    "    feats[\"total_obs\"] = 0                          # total observations across all bands\n",
    "\n",
    "    # Storage for cross-band comparison features\n",
    "    band_amp = {}                                   # per-band amplitude values\n",
    "    band_tpeak_rest = {}                            # per-band peak time (rest frame)\n",
    "    band_tpeak_obs = {}                             # per-band peak time (obs frame)\n",
    "    band_peak = {}                                  # per-band peak flux values\n",
    "\n",
    "    # Storage for interpolation-based color features (need time series per band)\n",
    "    band_tb_rest = {}                               # per-band time arrays (rest frame)\n",
    "    band_tb_obs = {}                                # per-band time arrays (obs frame)\n",
    "    band_fb = {}                                    # per-band flux arrays (dust-corrected)\n",
    "\n",
    "    # Loop over each wavelength band (u, g, r, i, z, y)\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        nb = int(np.sum(m))\n",
    "\n",
    "        # Number of observations in this band\n",
    "        feats[f\"n_{b}\"] = nb\n",
    "        feats[\"total_obs\"] += nb\n",
    "\n",
    "        # Initialize band features as missing by default\n",
    "        feats[f\"amp_{b}\"] = np.nan                   # peak - baseline amplitude\n",
    "        feats[f\"robust_amp_{b}\"] = np.nan            # p95 - p5 amplitude (robust)\n",
    "\n",
    "        feats[f\"tpeak_{b}_obs\"] = np.nan             # peak time in observed frame\n",
    "        feats[f\"tpeak_{b}_rest\"] = np.nan            # peak time in rest frame\n",
    "\n",
    "        feats[f\"width50_{b}_obs\"] = np.nan           # width above 50% amplitude (obs)\n",
    "        feats[f\"width50_{b}_rest\"] = np.nan          # width above 50% amplitude (rest)\n",
    "        feats[f\"width80_{b}_obs\"] = np.nan           # width above 80% amplitude (obs)\n",
    "        feats[f\"width80_{b}_rest\"] = np.nan          # width above 80% amplitude (rest)\n",
    "\n",
    "        feats[f\"rise50_{b}_obs\"] = np.nan            # time from 50% crossing to peak (obs)\n",
    "        feats[f\"decay50_{b}_obs\"] = np.nan           # time from peak to 50% decay (obs)\n",
    "        feats[f\"asym50_{b}_obs\"] = np.nan            # rise/decay asymmetry at 50% (obs)\n",
    "\n",
    "        feats[f\"rise50_{b}_rest\"] = np.nan           # time from 50% crossing to peak (rest)\n",
    "        feats[f\"decay50_{b}_rest\"] = np.nan          # time from peak to 50% decay (rest)\n",
    "        feats[f\"asym50_{b}_rest\"] = np.nan           # rise/decay asymmetry at 50% (rest)\n",
    "\n",
    "        feats[f\"auc_pos_{b}_obs\"] = np.nan           # area above baseline, positive only (obs)\n",
    "        feats[f\"auc_pos_{b}_rest\"] = np.nan          # area above baseline, positive only (rest)\n",
    "\n",
    "        feats[f\"snrmax_{b}\"] = np.nan                # best SNR in this band\n",
    "        feats[f\"eta_{b}\"] = np.nan                   # Von Neumann smoothness for this band\n",
    "        feats[f\"chi2_const_{b}\"] = np.nan            # variability vs constant model (band)\n",
    "\n",
    "        feats[f\"slope_{b}_obs\"] = np.nan             # best-fit linear trend (obs)\n",
    "        feats[f\"slope_{b}_rest\"] = np.nan            # best-fit linear trend (rest)\n",
    "\n",
    "        feats[f\"maxslope_{b}_obs\"] = np.nan          # fastest change rate (obs)\n",
    "        feats[f\"maxslope_{b}_rest\"] = np.nan         # fastest change rate (rest)\n",
    "\n",
    "        feats[f\"stetsonJ_{b}_obs\"] = np.nan          # correlated variability (obs)\n",
    "        feats[f\"stetsonJ_{b}_rest\"] = np.nan         # correlated variability (rest)\n",
    "\n",
    "        feats[f\"p5_{b}\"] = np.nan                    # 5th percentile flux\n",
    "        feats[f\"p25_{b}\"] = np.nan                   # 25th percentile flux\n",
    "        feats[f\"p75_{b}\"] = np.nan                   # 75th percentile flux\n",
    "        feats[f\"p95_{b}\"] = np.nan                   # 95th percentile flux\n",
    "        feats[f\"mad_{b}\"] = np.nan                   # median absolute deviation\n",
    "        feats[f\"iqr_{b}\"] = np.nan                   # interquartile range\n",
    "        feats[f\"mad_over_std_{b}\"] = np.nan          # robust-to-standard variability ratio\n",
    "\n",
    "        feats[f\"fvar_{b}\"] = np.nan                  # fractional variability (noise-corrected)\n",
    "\n",
    "        # Skip bands with no data\n",
    "        if nb == 0:\n",
    "            continue\n",
    "\n",
    "        feats[\"n_filters_present\"] += 1\n",
    "\n",
    "        # Extract time, brightness, and error for this band\n",
    "        tb_obs = t_rel[m]\n",
    "        fb = flux_corr[m]\n",
    "        eb = err_corr[m]\n",
    "\n",
    "        # Sort observations within the band by time\n",
    "        order = np.argsort(tb_obs)\n",
    "        tb_obs = tb_obs[order]\n",
    "        fb = fb[order]\n",
    "        eb = eb[order]\n",
    "\n",
    "        # Convert to intrinsic time scale\n",
    "        tb_rest = tb_obs / (1.0 + z)\n",
    "\n",
    "        # Define baseline brightness and peak brightness\n",
    "        baseline = float(np.median(fb))              # typical level (robust baseline)\n",
    "        pidx = int(np.argmax(fb))                    # index of brightest point\n",
    "        peak_flux = float(fb[pidx])                  # peak brightness\n",
    "\n",
    "        tpeak_obs = float(tb_obs[pidx])              # time of peak (observed)\n",
    "        tpeak_rest = float(tb_rest[pidx])            # time of peak (intrinsic)\n",
    "\n",
    "        # Amplitude of brightening (relative to baseline)\n",
    "        amp = peak_flux - baseline\n",
    "\n",
    "        # Robust per-band amplitude using percentiles (stable against outliers)\n",
    "        p5b, p25b, p75b, p95b = np.percentile(fb, [5, 25, 75, 95])\n",
    "        feats[f\"p5_{b}\"] = float(p5b)\n",
    "        feats[f\"p25_{b}\"] = float(p25b)\n",
    "        feats[f\"p75_{b}\"] = float(p75b)\n",
    "        feats[f\"p95_{b}\"] = float(p95b)\n",
    "        feats[f\"robust_amp_{b}\"] = float(p95b - p5b)\n",
    "\n",
    "        # Robust variability summaries\n",
    "        feats[f\"mad_{b}\"] = median_abs_dev(fb)\n",
    "        feats[f\"iqr_{b}\"] = iqr(fb)\n",
    "        stdb = float(np.std(fb))\n",
    "        feats[f\"mad_over_std_{b}\"] = float(feats[f\"mad_{b}\"] / (stdb + 1e-8))\n",
    "\n",
    "        # Core band features\n",
    "        feats[f\"amp_{b}\"] = float(amp)\n",
    "        feats[f\"tpeak_{b}_obs\"] = tpeak_obs\n",
    "        feats[f\"tpeak_{b}_rest\"] = tpeak_rest\n",
    "\n",
    "        # Band-level quality + variability diagnostics\n",
    "        feats[f\"snrmax_{b}\"] = float(np.max(np.abs(fb) / (eb + 1e-8)))  # best detection quality\n",
    "        feats[f\"eta_{b}\"] = von_neumann_eta(fb)                         # smoothness vs noise\n",
    "        feats[f\"chi2_const_{b}\"] = chi2_to_constant(fb, eb)             # variability vs constant\n",
    "\n",
    "        # Linear trend + slope features (obs + rest frame)\n",
    "        feats[f\"slope_{b}_obs\"] = linear_slope(tb_obs, fb)              # best-fit trend (obs)\n",
    "        feats[f\"slope_{b}_rest\"] = linear_slope(tb_rest, fb)            # best-fit trend (rest)\n",
    "\n",
    "        feats[f\"maxslope_{b}_obs\"] = max_slope(tb_obs, fb)              # fastest change (obs)\n",
    "        feats[f\"maxslope_{b}_rest\"] = max_slope(tb_rest, fb)            # fastest change (rest)\n",
    "\n",
    "        feats[f\"stetsonJ_{b}_obs\"] = stetson_J(tb_obs, fb, eb)          # correlated variability (obs)\n",
    "        feats[f\"stetsonJ_{b}_rest\"] = stetson_J(tb_rest, fb, eb)        # correlated variability (rest)\n",
    "\n",
    "        # Noise-corrected variability\n",
    "        feats[f\"fvar_{b}\"] = fractional_variability(fb, eb)\n",
    "\n",
    "        # Total positive signal above baseline (area under curve)\n",
    "        if nb >= 2:\n",
    "            feats[f\"auc_pos_{b}_obs\"] = float(trapz_safe(np.maximum(fb - baseline, 0.0), tb_obs))\n",
    "            feats[f\"auc_pos_{b}_rest\"] = float(trapz_safe(np.maximum(fb - baseline, 0.0), tb_rest))\n",
    "\n",
    "        # Width / rise / decay features at 50% and 80% of amplitude\n",
    "        # (requires a positive amplitude and enough points to have both sides of the peak)\n",
    "        if (amp > 0) and (nb >= 3):\n",
    "            lvl50 = baseline + 0.50 * amp\n",
    "            lvl80 = baseline + 0.80 * amp\n",
    "\n",
    "            # Find the first time the curve crosses a level on the rise/decay side\n",
    "            # (this is a coarse but stable width proxy without curve fitting)\n",
    "            def first_crossing_time(tt, ff, level, mode):\n",
    "                if len(tt) < 2:\n",
    "                    return np.nan\n",
    "                if mode == \"rise\":\n",
    "                    idx = np.where(ff >= level)[0]\n",
    "                    return float(tt[idx[0]]) if len(idx) else np.nan\n",
    "                if mode == \"decay\":\n",
    "                    idx = np.where(ff <= level)[0]\n",
    "                    return float(tt[idx[0]]) if len(idx) else np.nan\n",
    "                return np.nan\n",
    "\n",
    "            # Split into rising and falling segments around the peak\n",
    "            tb_rise_obs = tb_obs[:pidx + 1]\n",
    "            fb_rise = fb[:pidx + 1]\n",
    "            tb_dec_obs = tb_obs[pidx:]\n",
    "            fb_dec = fb[pidx:]\n",
    "\n",
    "            # Observed-frame width at 50% amplitude\n",
    "            t_rise50_obs = first_crossing_time(tb_rise_obs, fb_rise, lvl50, \"rise\")\n",
    "            t_fall50_obs = first_crossing_time(tb_dec_obs, fb_dec, lvl50, \"decay\")\n",
    "            if (not np.isnan(t_rise50_obs)) and (not np.isnan(t_fall50_obs)):\n",
    "                feats[f\"width50_{b}_obs\"] = float(t_fall50_obs - t_rise50_obs)\n",
    "                feats[f\"rise50_{b}_obs\"] = float(tpeak_obs - t_rise50_obs)\n",
    "                feats[f\"decay50_{b}_obs\"] = float(t_fall50_obs - tpeak_obs)\n",
    "                feats[f\"asym50_{b}_obs\"] = float(feats[f\"rise50_{b}_obs\"] / (feats[f\"decay50_{b}_obs\"] + 1e-8))\n",
    "\n",
    "            # Observed-frame width at 80% amplitude (no rise/decay split stored here)\n",
    "            t_rise80_obs = first_crossing_time(tb_rise_obs, fb_rise, lvl80, \"rise\")\n",
    "            t_fall80_obs = first_crossing_time(tb_dec_obs, fb_dec, lvl80, \"decay\")\n",
    "            if (not np.isnan(t_rise80_obs)) and (not np.isnan(t_fall80_obs)):\n",
    "                feats[f\"width80_{b}_obs\"] = float(t_fall80_obs - t_rise80_obs)\n",
    "\n",
    "            # Rest-frame times for the same segments\n",
    "            tb_rise_rest = tb_rest[:pidx + 1]\n",
    "            tb_dec_rest = tb_rest[pidx:]\n",
    "\n",
    "            # Rest-frame width at 50% amplitude\n",
    "            t_rise50_rest = first_crossing_time(tb_rise_rest, fb_rise, lvl50, \"rise\")\n",
    "            t_fall50_rest = first_crossing_time(tb_dec_rest, fb_dec, lvl50, \"decay\")\n",
    "            if (not np.isnan(t_rise50_rest)) and (not np.isnan(t_fall50_rest)):\n",
    "                feats[f\"width50_{b}_rest\"] = float(t_fall50_rest - t_rise50_rest)\n",
    "                feats[f\"rise50_{b}_rest\"] = float(tpeak_rest - t_rise50_rest)\n",
    "                feats[f\"decay50_{b}_rest\"] = float(t_fall50_rest - tpeak_rest)\n",
    "                feats[f\"asym50_{b}_rest\"] = float(feats[f\"rise50_{b}_rest\"] / (feats[f\"decay50_{b}_rest\"] + 1e-8))\n",
    "\n",
    "            # Rest-frame width at 80% amplitude\n",
    "            t_rise80_rest = first_crossing_time(tb_rise_rest, fb_rise, lvl80, \"rise\")\n",
    "            t_fall80_rest = first_crossing_time(tb_dec_rest, fb_dec, lvl80, \"decay\")\n",
    "            if (not np.isnan(t_rise80_rest)) and (not np.isnan(t_fall80_rest)):\n",
    "                feats[f\"width80_{b}_rest\"] = float(t_fall80_rest - t_rise80_rest)\n",
    "\n",
    "        # Store values for cross-band comparisons\n",
    "        band_amp[b] = feats[f\"amp_{b}\"]\n",
    "        band_tpeak_obs[b] = feats[f\"tpeak_{b}_obs\"]\n",
    "        band_tpeak_rest[b] = feats[f\"tpeak_{b}_rest\"]\n",
    "        band_peak[b] = peak_flux\n",
    "\n",
    "        # Store time series for interpolation-based color features\n",
    "        band_tb_obs[b] = tb_obs\n",
    "        band_tb_rest[b] = tb_rest\n",
    "        band_fb[b] = fb\n",
    "\n",
    "    # Cross-band comparison features between adjacent wavelengths\n",
    "    # (captures color differences and peak-time lags across filters)\n",
    "    pairs = [(\"u\", \"g\"), (\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\"), (\"z\", \"y\")]\n",
    "    for a, b in pairs:\n",
    "        va, vb = band_amp.get(a, np.nan), band_amp.get(b, np.nan)\n",
    "        ta_obs, tb_obs = band_tpeak_obs.get(a, np.nan), band_tpeak_obs.get(b, np.nan)\n",
    "        ta_rest, tb_rest = band_tpeak_rest.get(a, np.nan), band_tpeak_rest.get(b, np.nan)\n",
    "        pa, pb = band_peak.get(a, np.nan), band_peak.get(b, np.nan)\n",
    "\n",
    "        # Difference in brightness amplitude\n",
    "        feats[f\"ampdiff_{a}{b}\"] = (va - vb) if (not np.isnan(va) and not np.isnan(vb)) else np.nan\n",
    "\n",
    "        # Difference in peak timing (obs + rest frame)\n",
    "        feats[f\"tpeakdiff_{a}{b}_obs\"] = (ta_obs - tb_obs) if (not np.isnan(ta_obs) and not np.isnan(tb_obs)) else np.nan\n",
    "        feats[f\"tpeakdiff_{a}{b}_rest\"] = (ta_rest - tb_rest) if (not np.isnan(ta_rest) and not np.isnan(tb_rest)) else np.nan\n",
    "\n",
    "        # Ratio of peak brightness values\n",
    "        feats[f\"peakratio_{a}{b}\"] = (pa / (pb + 1e-8)) if (not np.isnan(pa) and not np.isnan(pb)) else np.nan\n",
    "\n",
    "    # Safe log transform used for color features\n",
    "    # (log1p + clamp avoids log(negative) explosions when flux is noisy)\n",
    "    def logp(x):\n",
    "        if np.isnan(x):\n",
    "            return np.nan\n",
    "        return float(np.log1p(max(0.0, x)))\n",
    "\n",
    "    # Color features anchored at r-band peak time\n",
    "    # (measures spectral shape at peak and how it evolves shortly after)\n",
    "    tpr_obs = feats.get(\"tpeak_r_obs\", np.nan)\n",
    "    if not np.isnan(tpr_obs):\n",
    "        # Interpolate g, r, i flux at the r-band peak time\n",
    "        fr = interp_flux_at_time(band_tb_obs.get(\"r\", np.array([])), band_fb.get(\"r\", np.array([])), tpr_obs)\n",
    "        fg = interp_flux_at_time(band_tb_obs.get(\"g\", np.array([])), band_fb.get(\"g\", np.array([])), tpr_obs)\n",
    "        fi = interp_flux_at_time(band_tb_obs.get(\"i\", np.array([])), band_fb.get(\"i\", np.array([])), tpr_obs)\n",
    "\n",
    "        # Approximate colors using log-flux differences (stable scale, less dominated by raw amplitude)\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = (logp(fg) - logp(fr)) if (not np.isnan(fg) and not np.isnan(fr)) else np.nan\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = (logp(fr) - logp(fi)) if (not np.isnan(fr) and not np.isnan(fi)) else np.nan\n",
    "\n",
    "        # Evaluate colors again 10 days after peak to estimate color evolution slope\n",
    "        dt_obs = 10.0\n",
    "        t2 = tpr_obs + dt_obs\n",
    "\n",
    "        fr2 = interp_flux_at_time(band_tb_obs.get(\"r\", np.array([])), band_fb.get(\"r\", np.array([])), t2)\n",
    "        fg2 = interp_flux_at_time(band_tb_obs.get(\"g\", np.array([])), band_fb.get(\"g\", np.array([])), t2)\n",
    "        fi2 = interp_flux_at_time(band_tb_obs.get(\"i\", np.array([])), band_fb.get(\"i\", np.array([])), t2)\n",
    "\n",
    "        cgr1 = feats[\"color_gr_at_rpeak_obs\"]\n",
    "        cri1 = feats[\"color_ri_at_rpeak_obs\"]\n",
    "\n",
    "        cgr2 = (logp(fg2) - logp(fr2)) if (not np.isnan(fg2) and not np.isnan(fr2)) else np.nan\n",
    "        cri2 = (logp(fr2) - logp(fi2)) if (not np.isnan(fr2) and not np.isnan(fi2)) else np.nan\n",
    "\n",
    "        # Color slopes (change per day over a 10-day window)\n",
    "        feats[\"color_gr_slope10_obs\"] = ((cgr2 - cgr1) / dt_obs) if (not np.isnan(cgr2) and not np.isnan(cgr1)) else np.nan\n",
    "        feats[\"color_ri_slope10_obs\"] = ((cri2 - cri1) / dt_obs) if (not np.isnan(cri2) and not np.isnan(cri1)) else np.nan\n",
    "    else:\n",
    "        # If r-band has no peak time (missing r-band), color-at-rpeak is undefined\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope10_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope10_obs\"] = np.nan\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "583b4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lightcurve_cache(splits, base_dir, kind=\"train\"):\n",
    "    base_dir = Path(base_dir)\n",
    "    lc_cache = {}\n",
    "    idx_cache = {}\n",
    "\n",
    "    for s in splits:\n",
    "        path = base_dir / str(s) / f\"{kind}_full_lightcurves.csv\"\n",
    "        lc = pd.read_csv(path)\n",
    "        lc[\"object_id\"] = lc[\"object_id\"].astype(str)\n",
    "        groups = lc.groupby(\"object_id\").indices\n",
    "        lc_cache[s] = lc\n",
    "        idx_cache[s] = groups\n",
    "\n",
    "    return lc_cache, idx_cache\n",
    "\n",
    "\n",
    "def get_lightcurve(lc_cache, idx_cache, split, object_id):\n",
    "    idx = idx_cache[split].get(object_id, None)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    return lc_cache[split].iloc[idx]\n",
    "\n",
    "\n",
    "def build_feature_table(\n",
    "    log_df,\n",
    "    lc_cache,\n",
    "    idx_cache,\n",
    "    augment_photoz=False,\n",
    "    test_zerr_pool=None,\n",
    "    n_aug=1,\n",
    "    seed=6\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    if test_zerr_pool is not None:\n",
    "        test_zerr_pool = np.asarray(test_zerr_pool, float)\n",
    "        test_zerr_pool = test_zerr_pool[np.isfinite(test_zerr_pool)]\n",
    "        test_zerr_pool = test_zerr_pool[test_zerr_pool > 0]\n",
    "\n",
    "    for i in range(len(log_df)):\n",
    "        r = log_df.iloc[i]\n",
    "        obj = r[\"object_id\"]\n",
    "        split = r[\"split\"]\n",
    "\n",
    "        lc = get_lightcurve(lc_cache, idx_cache, split, obj)\n",
    "\n",
    "        if lc is None:\n",
    "            feats = {\"n_obs\": 0}\n",
    "            feats[\"object_id\"] = obj\n",
    "            feats[\"split\"] = split\n",
    "            feats[\"photoz_aug\"] = 0\n",
    "            if \"target\" in log_df.columns:\n",
    "                feats[\"target\"] = int(r[\"target\"])\n",
    "            rows.append(feats)\n",
    "            continue\n",
    "\n",
    "        feats = extract_features_for_object(\n",
    "            lc_raw=lc,\n",
    "            z=r[\"Z\"],\n",
    "            z_err=r.get(\"Z_err\", 0.0),\n",
    "            ebv=r[\"EBV\"],\n",
    "        )\n",
    "        feats[\"object_id\"] = obj\n",
    "        feats[\"split\"] = split\n",
    "        feats[\"photoz_aug\"] = 0\n",
    "        if \"target\" in log_df.columns:\n",
    "            feats[\"target\"] = int(r[\"target\"])\n",
    "        rows.append(feats)\n",
    "\n",
    "        if augment_photoz and (\"target\" in log_df.columns) and (test_zerr_pool is not None) and (len(test_zerr_pool) > 0):\n",
    "            z0 = safe_float(r[\"Z\"], default=0.0)\n",
    "\n",
    "            for _ in range(n_aug):\n",
    "                sigma = float(rng.choice(test_zerr_pool))\n",
    "                z_sim = max(0.0, z0 + float(rng.normal(0.0, sigma)))\n",
    "\n",
    "                feats2 = extract_features_for_object(\n",
    "                    lc_raw=lc,\n",
    "                    z=z_sim,\n",
    "                    z_err=sigma,\n",
    "                    ebv=r[\"EBV\"],\n",
    "                )\n",
    "                feats2[\"object_id\"] = obj\n",
    "                feats2[\"split\"] = split\n",
    "                feats2[\"target\"] = int(r[\"target\"])\n",
    "                feats2[\"photoz_aug\"] = 1\n",
    "                rows.append(feats2)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e4c6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(df, drop_cols):\n",
    "    X = df.drop(columns=drop_cols).copy()\n",
    "\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    med = X.median(numeric_only=True)\n",
    "    X = X.fillna(med)\n",
    "    X = X.fillna(0.0)\n",
    "    return X\n",
    "\n",
    "\n",
    "def best_threshold_f1(y_true, probs):\n",
    "    ths = np.linspace(0.01, 0.99, 200)\n",
    "    f1s = [f1_score(y_true, probs > t, zero_division=0) for t in ths]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return float(ths[j]), float(f1s[j])\n",
    "\n",
    "\n",
    "def best_alpha_and_threshold(y_true, p_xgb, p_lgb):\n",
    "    alphas = np.linspace(0.0, 1.0, 101)\n",
    "    best = (0.5, 0.5, -1.0)  # alpha, th, f1\n",
    "\n",
    "    for a in alphas:\n",
    "        p = a * p_xgb + (1.0 - a) * p_lgb\n",
    "        th, f1 = best_threshold_f1(y_true, p)\n",
    "        if f1 > best[2]:\n",
    "            best = (float(a), float(th), float(f1))\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def make_splitter(n_splits, random_state=6):\n",
    "    return StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef4dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_xgb(train_feat, n_folds_tune=10, timeout_sec=7200):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"])\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"random_state\": 6,\n",
    "            \"n_jobs\": -1,\n",
    "\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"device\": \"cuda\",\n",
    "\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 600, 6000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.12, log=True),\n",
    "\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 40),\n",
    "\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 20.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.05, 30.0),\n",
    "\n",
    "            \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "\n",
    "            \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "        }\n",
    "\n",
    "        if params[\"grow_policy\"] == \"lossguide\":\n",
    "            params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 256)\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        splitter = make_splitter(n_folds_tune, random_state=6)\n",
    "        split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "            neg = np.sum(y_tr == 0)\n",
    "            pos = np.sum(y_tr == 1)\n",
    "            params[\"scale_pos_weight\"] = float(neg / max(1, pos))\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "            probs = model.predict_proba(X_va)[:, 1]\n",
    "            ap = average_precision_score(y_va, probs)\n",
    "            scores.append(ap)\n",
    "\n",
    "            trial.report(float(np.mean(scores)), step=fold)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=6, multivariate=True, group=True)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=30, n_warmup_steps=3)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name=\"xgb_ap_split_cv_gpu\",\n",
    "        storage=\"sqlite:///optuna_xgb_ap_gpu.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=999999, timeout=timeout_sec)\n",
    "\n",
    "    print(\"\\nOptuna best AP:\", study.best_value)\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(k, \"=\", v)\n",
    "\n",
    "    return study.best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6171132",
   "metadata": {},
   "source": [
    "## Training the full ensemble (XGB + LGB) and calibrating the blend\n",
    "\n",
    "This function trains two separate model families per fold:\n",
    "- XGBoost models (using Optuna-tuned params)\n",
    "- LightGBM models (fixed baseline params)\n",
    "\n",
    "For each fold:\n",
    "- train both models on the fold training set\n",
    "- store OOF probabilities for both models\n",
    "- report a temporary 0.5/0.5 blend best-F1 threshold (for sanity checking)\n",
    "\n",
    "After all folds:\n",
    "- compute the best blend weight `alpha` and best threshold using full OOF predictions:\n",
    "  - `alpha_best = 0.03`\n",
    "  - `best_th = 0.01`\n",
    "  - OOF blended best F1 = 0.51875\n",
    "\n",
    "These values are then used for test prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a3c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_ensemble(train_feat, xgb_params, n_splits_full=20):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"])\n",
    "\n",
    "    splitter = make_splitter(n_splits_full, random_state=6)\n",
    "    split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "    xgb_base = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 6,\n",
    "        \"n_jobs\": -1,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\",\n",
    "        **xgb_params\n",
    "    }\n",
    "\n",
    "    lgb_base = dict(\n",
    "        objective=\"binary\",\n",
    "        boosting_type=\"gbdt\",\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=64,\n",
    "        max_depth=-1,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        n_jobs=-1,\n",
    "        random_state=6\n",
    "    )\n",
    "\n",
    "    xgb_models = []\n",
    "    lgb_models = []\n",
    "\n",
    "    oof_xgb = np.zeros(len(X), dtype=float)\n",
    "    oof_lgb = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "        neg = np.sum(y_tr == 0)\n",
    "        pos = np.sum(y_tr == 1)\n",
    "        spw = float(neg / max(1, pos))\n",
    "\n",
    "        xgb_base[\"scale_pos_weight\"] = spw\n",
    "        xgb_model = XGBClassifier(**xgb_base)\n",
    "        xgb_model.fit(X_tr, y_tr, verbose=False)\n",
    "        p_xgb = xgb_model.predict_proba(X_va)[:, 1]\n",
    "        oof_xgb[va_idx] = p_xgb\n",
    "        xgb_models.append(xgb_model)\n",
    "\n",
    "        lgb_model = LGBMClassifier(**{**lgb_base, \"scale_pos_weight\": spw})\n",
    "        lgb_model.fit(X_tr, y_tr)\n",
    "        p_lgb = lgb_model.predict_proba(X_va)[:, 1]\n",
    "        oof_lgb[va_idx] = p_lgb\n",
    "        lgb_models.append(lgb_model)\n",
    "\n",
    "        p_tmp = 0.5 * p_xgb + 0.5 * p_lgb\n",
    "        th, f1 = best_threshold_f1(y_va, p_tmp)\n",
    "        print(f\"Fold {fold:02d} | temp blend(0.5) best F1={f1:.4f} @ th={th:.3f}\")\n",
    "\n",
    "    alpha_best, th_best, f1_best = best_alpha_and_threshold(y, oof_xgb, oof_lgb)\n",
    "    print(\"\\nOOF best alpha:\", alpha_best)\n",
    "    print(\"OOF best threshold:\", th_best)\n",
    "    print(\"OOF blended best F1:\", f1_best)\n",
    "\n",
    "    return xgb_models, lgb_models, alpha_best, th_best\n",
    "\n",
    "\n",
    "def predict_ensemble(test_feat, xgb_models, lgb_models, alpha):\n",
    "    X_test = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"])\n",
    "\n",
    "    p_xgb = np.mean([m.predict_proba(X_test)[:, 1] for m in xgb_models], axis=0)\n",
    "    p_lgb = np.mean([m.predict_proba(X_test)[:, 1] for m in lgb_models], axis=0)\n",
    "\n",
    "    p_blend = alpha * p_xgb + (1.0 - alpha) * p_lgb\n",
    "    return p_blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75024d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path.cwd().parents[0]\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "train_log = pd.read_csv(DATA_DIR / \"train_log.csv\")\n",
    "test_log  = pd.read_csv(DATA_DIR / \"test_log.csv\")\n",
    "\n",
    "train_log[\"Z_err\"] = train_log[\"Z_err\"].fillna(0.0)\n",
    "test_log[\"Z_err\"] = test_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "train_splits = sorted(train_log[\"split\"].unique())\n",
    "test_splits = sorted(test_log[\"split\"].unique())\n",
    "\n",
    "train_lc_cache, train_idx_cache = build_lightcurve_cache(train_splits, DATA_DIR, kind=\"train\")\n",
    "test_lc_cache, test_idx_cache = build_lightcurve_cache(test_splits, DATA_DIR, kind=\"test\")\n",
    "test_zerr_pool = test_log[\"Z_err\"].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea35f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feat: (6086, 272)\n",
      "test_feat : (7135, 271)\n"
     ]
    }
   ],
   "source": [
    "train_feat = build_feature_table(\n",
    "    train_log, train_lc_cache, train_idx_cache,\n",
    "    augment_photoz=True,\n",
    "    test_zerr_pool=test_zerr_pool,\n",
    "    n_aug=1,\n",
    "    seed=6\n",
    ")\n",
    "\n",
    "test_feat = build_feature_table(test_log, test_lc_cache, test_idx_cache)\n",
    "\n",
    "print(\"train_feat:\", train_feat.shape)\n",
    "print(\"test_feat :\", test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_params = run_optuna_xgb(train_feat, n_folds_tune=10, timeout_sec=7200)\n",
    "xgb_models, lgb_models, alpha_best, best_th = train_full_ensemble(\n",
    "    train_feat, best_xgb_params, n_splits_full=len(train_splits)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0426c3d",
   "metadata": {},
   "source": [
    "Best Parameters:\n",
    "```json\n",
    "{\n",
    "  \"n_estimators\": 4328,\n",
    "  \"learning_rate\": 0.007080,\n",
    "  \"max_depth\": 4,\n",
    "  \"min_child_weight\": 1,\n",
    "  \"subsample\": 0.5936,\n",
    "  \"colsample_bytree\": 0.9145,\n",
    "  \"gamma\": 0.6466,\n",
    "  \"reg_alpha\": 4.1301,\n",
    "  \"reg_lambda\": 5.5650,\n",
    "  \"max_delta_step\": 1,\n",
    "  \"grow_policy\": 'depthwise'\n",
    "}\n",
    "```\n",
    "\n",
    "OOF best alpha: 0.03  \n",
    "OOF best threshold: 0.01  \n",
    "OOF blended best F1: 0.51875  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28bd798",
   "metadata": {},
   "source": [
    "These results are extremely strange, the threshold is extremely low. Almost looks like an error. I wouldn't be surprised. I didn't give much attention to this model, it was more of a test I ran over night to see if a small ensemble improved performance. I saw the 0.03 and just assumed LGBM wasn't pulling it's weight, when it was actually XGB that wasn't helping. I even did a test on a later model and LGBM was weighted at ~0 every single time. I removed LGBM from subsequent models because of that. Next competition I will give LGBM and possibly CatBoost more of a chance instead of ignoring them. I know I used LGBM for predicting SpecType, but I could use it a lot more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e72428",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = predict_ensemble(test_feat, xgb_models, lgb_models, alpha=alpha_best)\n",
    "test_pred = (test_probs > best_th).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"object_id\": test_feat[\"object_id\"].values,\n",
    "    \"target\": test_pred\n",
    "})\n",
    "sub.to_csv(\"XGB-LGBM-2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
