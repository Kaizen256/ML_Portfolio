{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132b323a",
   "metadata": {},
   "source": [
    "# Model 4: XGB SpecType\n",
    "\n",
    "This notebook contains Model 4 for the MALLORN challenge.\n",
    "\n",
    "This model is the first one that performed very well. It was enough to put me in around 130th / 925 participants on the public leaderboard. that success did not carry over to the final LB though.\n",
    "The biggest change is using SpecType (train-only metadata) to generate features that can also be computed for the test set.\n",
    "\n",
    "A Kaggle user in a discussion post pointed out focusing on TDE vs SN/AGN which are values in SpecType. Since SpecType is not available at test time, I train a separate model to predict SpecType, and then use its predicted probabilities as additional features in the main TDE classifier.\n",
    "\n",
    "1) Train a multiclass LightGBM model to predict `SpecTypeGroup`:\n",
    "   - TDE\n",
    "   - AGN\n",
    "   - SNIa\n",
    "   - SNother\n",
    "   - Other\n",
    "\n",
    "2) Generate OOF predicted probabilities for the train set:\n",
    "   - Each training object only gets probabilities from a model that did not train on its group-split fold.\n",
    "\n",
    "3) Fit the multiclass model on full train and predict probabilities for test.\n",
    "\n",
    "4) Append these probabilities as features:\n",
    "   - `p_spec_<class>` for each class\n",
    "   - `spec_entropy` as a confidence / ambiguity signal\n",
    "\n",
    "This gives the main classifier extra information about \"what kind of transient this looks like\" using only lightcurve-derived features.\n",
    "\n",
    "## Results\n",
    "\n",
    "Best parameters:\n",
    "- n_estimators: 4770\n",
    "- learning_rate: 0.009408\n",
    "- max_depth: 5\n",
    "- min_child_weight: 38\n",
    "- subsample: 0.9580\n",
    "- colsample_bytree: 0.5860\n",
    "- gamma: 8.6793\n",
    "- reg_alpha: 17.5374\n",
    "- reg_lambda: 24.2249\n",
    "- max_delta_step: 2\n",
    "- grow_policy: depthwise\n",
    "\n",
    "OOF multiseed best threshold: 0.46798994974874375  \n",
    "OOF multiseed best F1: 0.5531914893617021  \n",
    "OOF AP (aucpr-ish): 0.6134734232399863  \n",
    "\n",
    "| Submission | Public LB F1 | Private LB F1 |\n",
    "|-------------|--------------|----------------|\n",
    "| 1 | 0.6309 | 0.5688 |\n",
    "| 2 | 0.6024 | 0.5333 |\n",
    "| 3 | 0.6009 | 0.5467 |\n",
    "\n",
    "\n",
    "At the time, the model seemed to generalize well. The first submission pushed me to around the top 200, and the follow-up submissions were all scoring above 0.6 F1. By the end of the competition, though, the results were more disappointing. The final leaderboard clearly contained harder examples than the public one, since most participants saw their F1 scores drop too. If I had stopped at this stage, I would have placed 178th, which is still respectable. Fortunately, this didn’t turn out to be my strongest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43252d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "from extinction import fitzpatrick99\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2434cb",
   "metadata": {},
   "source": [
    "## Constants / Configuration\n",
    "\n",
    "- `PRE_BASE_FRAC`: fraction of early-time points used to estimate baseline before peak\n",
    "- `MIN_BAND_POINTS`: minimum points needed for certain per-band features\n",
    "- `PEAK_SIGMA_K`: how strict a “significant peak” must be relative to noise\n",
    "- `REBRIGHT_FRAC`: what fraction of amplitude counts as rebrightening\n",
    "- `EPS`: numerical stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15183be",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "EFF_WL_AA = {\n",
    "    \"u\": 3641.0,\n",
    "    \"g\": 4704.0,\n",
    "    \"r\": 6155.0,\n",
    "    \"i\": 7504.0,\n",
    "    \"z\": 8695.0,\n",
    "    \"y\": 10056.0,\n",
    "}\n",
    "\n",
    "R_V = 3.1\n",
    "\n",
    "PRE_BASE_FRAC = 0.20\n",
    "MIN_BAND_POINTS = 5\n",
    "PEAK_SIGMA_K = 3.0\n",
    "REBRIGHT_FRAC = 0.30\n",
    "EPS = 1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return default\n",
    "        x = float(x)\n",
    "        if np.isnan(x):\n",
    "            return default\n",
    "        return x\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def trapz_safe(y, x):\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return float(np.trapezoid(y, x))\n",
    "    y = np.asarray(y)\n",
    "    x = np.asarray(x)\n",
    "    return float(np.sum((x[1:] - x[:-1]) * (y[1:] + y[:-1]) * 0.5))\n",
    "\n",
    "\n",
    "def median_abs_dev(x):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    med = np.median(x)\n",
    "    return float(np.median(np.abs(x - med)))\n",
    "\n",
    "\n",
    "def iqr(x):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    q75, q25 = np.percentile(x, [75, 25])\n",
    "    return float(q75 - q25)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m3 = np.mean((x - mu) ** 3)\n",
    "    return float(m3 / (s ** 3))\n",
    "\n",
    "\n",
    "def kurtosis_excess(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "    mu = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    if s < 1e-12:\n",
    "        return 0.0\n",
    "    m4 = np.mean((x - mu) ** 4)\n",
    "    return float(m4 / (s ** 4) - 3.0)\n",
    "\n",
    "\n",
    "def von_neumann_eta(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    v = np.var(x)\n",
    "    if v < 1e-12:\n",
    "        return 0.0\n",
    "    dif = np.diff(x)\n",
    "    return float(np.mean(dif ** 2) / v)\n",
    "\n",
    "\n",
    "def max_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.max(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def median_abs_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(t)\n",
    "    df = np.diff(f)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "    slopes = df[good] / dt[good]\n",
    "    return float(np.median(np.abs(slopes)))\n",
    "\n",
    "\n",
    "def linear_slope(t, f):\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    if len(t) < 3:\n",
    "        return np.nan\n",
    "    try:\n",
    "        a, b = np.polyfit(t, f, 1)\n",
    "        return float(a)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def chi2_to_constant(f, ferr):\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    mu = np.median(f)\n",
    "    denom = (ferr + EPS) ** 2\n",
    "    chi2 = np.sum((f - mu) ** 2 / denom)\n",
    "    dof = max(1, n - 1)\n",
    "    return float(chi2 / dof)\n",
    "\n",
    "\n",
    "def interp_flux_at_time(tb, fb, t0):\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if len(tb) < 2:\n",
    "        return np.nan\n",
    "    if (t0 < tb.min()) or (t0 > tb.max()):\n",
    "        return np.nan\n",
    "    return float(np.interp(t0, tb, fb))\n",
    "\n",
    "\n",
    "def fractional_variability(f, ferr):\n",
    "    \"\"\"\n",
    "    Noise-corrected intrinsic variability:\n",
    "    F_var = sqrt(max(0, S^2 - mean(err^2))) / |mean(f)|\n",
    "    \"\"\"\n",
    "    f = np.asarray(f, float)\n",
    "    ferr = np.asarray(ferr, float)\n",
    "    n = len(f)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "\n",
    "    mu = np.mean(f)\n",
    "    if np.abs(mu) < 1e-8:\n",
    "        return np.nan\n",
    "\n",
    "    s2 = np.var(f, ddof=1)\n",
    "    mean_err2 = np.mean(ferr**2)\n",
    "    excess = max(0.0, s2 - mean_err2)\n",
    "    return float(np.sqrt(excess) / np.abs(mu))\n",
    "\n",
    "def stetson_J_consecutive(t, f, ferr):\n",
    "    \"\"\"\n",
    "    Stetson J using consecutive pairs (always exists if n>=4).\n",
    "    \"\"\"\n",
    "    t = np.asarray(t)\n",
    "    f = np.asarray(f)\n",
    "    ferr = np.asarray(ferr)\n",
    "    n = len(t)\n",
    "    if n < 4:\n",
    "        return np.nan\n",
    "\n",
    "    mu = np.mean(f)\n",
    "    scale = np.sqrt(n / max(1, n - 1))\n",
    "    delta = scale * (f - mu) / (ferr + EPS)\n",
    "\n",
    "    vals = []\n",
    "    for i in range(n - 1):\n",
    "        P = delta[i] * delta[i + 1]\n",
    "        vals.append(np.sign(P) * np.sqrt(np.abs(P)))\n",
    "\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "def pre_peak_baseline(tb, fb, eb, frac=PRE_BASE_FRAC):\n",
    "    \"\"\"\n",
    "    baseline from earliest fraction of times (robust).\n",
    "    \"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(tb)\n",
    "    if n < 3:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    k = max(2, int(np.ceil(frac * n)))\n",
    "    k = min(k, n)\n",
    "\n",
    "    base = float(np.median(fb[:k]))\n",
    "    mad_pre = median_abs_dev(fb[:k])\n",
    "    mederr_pre = float(np.median(eb[:k])) if k > 0 else np.nan\n",
    "    return base, mad_pre, mederr_pre\n",
    "\n",
    "\n",
    "def count_significant_peaks(tb, fb, eb, baseline_pre, k_sigma=PEAK_SIGMA_K):\n",
    "    \"\"\"\n",
    "    Simple local-maximum peak count above baseline_pre + k_sigma * median_err_pre.\n",
    "    \"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    eb = np.asarray(eb)\n",
    "    n = len(fb)\n",
    "    if n < 5:\n",
    "        return 0\n",
    "\n",
    "    mederr = float(np.median(eb)) if np.isfinite(np.median(eb)) else 0.0\n",
    "    thresh = baseline_pre + k_sigma * mederr\n",
    "\n",
    "    peaks = 0\n",
    "    for i in range(1, n - 1):\n",
    "        if (fb[i] > fb[i - 1]) and (fb[i] > fb[i + 1]) and (fb[i] > thresh):\n",
    "            peaks += 1\n",
    "    return int(peaks)\n",
    "\n",
    "\n",
    "def postpeak_monotonicity(tb, fb, pidx):\n",
    "    \"\"\"\n",
    "    fraction of negative slopes after peak (monotone decline score).\n",
    "    \"\"\"\n",
    "    tb = np.asarray(tb)\n",
    "    fb = np.asarray(fb)\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return np.nan\n",
    "\n",
    "    t2 = tb[pidx:]\n",
    "    f2 = fb[pidx:]\n",
    "    if len(f2) < 3:\n",
    "        return np.nan\n",
    "\n",
    "    dt = np.diff(t2)\n",
    "    df = np.diff(f2)\n",
    "    good = dt > 0\n",
    "    if not np.any(good):\n",
    "        return np.nan\n",
    "\n",
    "    frac_neg = float(np.mean((df[good] / dt[good]) < 0))\n",
    "    return frac_neg\n",
    "\n",
    "\n",
    "def count_rebrighten(tb, fb, baseline_pre, amp, pidx, frac=REBRIGHT_FRAC):\n",
    "    \"\"\"\n",
    "    Count how often post-peak rises above baseline_pre + frac*amp after having dropped below it.\n",
    "    \"\"\"\n",
    "    if pidx is None or pidx >= len(fb) - 2:\n",
    "        return 0\n",
    "\n",
    "    level = baseline_pre + frac * amp\n",
    "    post = fb[pidx:]\n",
    "    if len(post) < 3:\n",
    "        return 0\n",
    "\n",
    "    above = post > level\n",
    "    crossings = np.sum((~above[:-1]) & (above[1:]))\n",
    "    return int(crossings)\n",
    "\n",
    "\n",
    "def fall_time_to_level(tb, fb, baseline_pre, amp, pidx, frac):\n",
    "    \"\"\"\n",
    "    t_fallX: time from peak to first time flux <= baseline_pre + frac*amp\n",
    "    using only decay segment.\n",
    "    \"\"\"\n",
    "    if amp <= 0 or pidx is None:\n",
    "        return np.nan\n",
    "\n",
    "    level = baseline_pre + frac * amp\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "    if len(f_dec) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    idx = np.where(f_dec <= level)[0]\n",
    "    if len(idx) == 0:\n",
    "        return np.nan\n",
    "    return float(t_dec[idx[0]] - t_dec[0])\n",
    "\n",
    "\n",
    "def decay_powerlaw_fit(tb, fb, baseline_pre, pidx, tmax=300.0):\n",
    "    \"\"\"\n",
    "    Fit log(f-baseline) = a + b*log(dt) on post-peak points.\n",
    "    Returns slope b, r2, npts.\n",
    "    \"\"\"\n",
    "    if pidx is None or pidx >= len(fb) - 3:\n",
    "        return np.nan, np.nan, 0\n",
    "\n",
    "    t0 = tb[pidx]\n",
    "    t_dec = tb[pidx:]\n",
    "    f_dec = fb[pidx:]\n",
    "\n",
    "    dt = t_dec - t0\n",
    "    m = (dt > 0.0) & (dt <= tmax)\n",
    "    dt = dt[m]\n",
    "    fd = f_dec[m] - baseline_pre\n",
    "\n",
    "    # must be positive\n",
    "    m2 = fd > 0.0\n",
    "    dt = dt[m2]\n",
    "    fd = fd[m2]\n",
    "\n",
    "    if len(dt) < 4:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "\n",
    "    x = np.log(dt + EPS)\n",
    "    y = np.log(fd + EPS)\n",
    "\n",
    "    # linear fit\n",
    "    try:\n",
    "        b, a = np.polyfit(x, y, 1)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, int(len(dt))\n",
    "\n",
    "    yhat = a + b * x\n",
    "    ss_res = float(np.sum((y - yhat) ** 2))\n",
    "    ss_tot = float(np.sum((y - np.mean(y)) ** 2)) + EPS\n",
    "    r2 = 1.0 - ss_res / ss_tot\n",
    "\n",
    "    return float(b), float(r2), int(len(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8431f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deextinct_band(flux, flux_err, ebv, band, r_v=R_V):\n",
    "    if ebv is None or (isinstance(ebv, float) and np.isnan(ebv)):\n",
    "        return flux, flux_err, 0.0\n",
    "\n",
    "    A_V = float(ebv) * float(r_v)\n",
    "    wave = np.array([EFF_WL_AA[band]], dtype=float)  # Angstrom\n",
    "    A_lambda = float(fitzpatrick99(wave, A_V, r_v=r_v, unit=\"aa\")[0])  # mag\n",
    "\n",
    "    fac = 10.0 ** (0.4 * A_lambda)\n",
    "    return flux * fac, flux_err * fac, A_lambda\n",
    "\n",
    "\n",
    "def deextinct_lightcurve(lc, ebv):\n",
    "    flux = lc[\"Flux\"].to_numpy().astype(float)\n",
    "    ferr = lc[\"Flux_err\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    flux_corr = flux.copy()\n",
    "    ferr_corr = ferr.copy()\n",
    "\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        flux_corr[m], ferr_corr[m], _ = deextinct_band(flux_corr[m], ferr_corr[m], ebv, b)\n",
    "\n",
    "    return flux_corr, ferr_corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f8ffa",
   "metadata": {},
   "source": [
    "# Model 4: Pre-peak baseline + peak morphology + decay fits + SpecType teacher stacking\n",
    "\n",
    "Differences vs Model 3:\n",
    " - Replaces baseline = \"median(flux)\" with a pre-peak baseline estimate (more physically meaningful for transients).\n",
    " - Adds peak morphology features: peak dominance, sharpness, fall-times, monotonicity, peak counts, rebrightening counts.\n",
    " - Adds decay-shape features via power-law fits on the post-peak segment (slope, R^2, number of points).\n",
    " - Upgrades Stetson J to a consecutive-pairs version (more time-series aware).\n",
    " - Adds a SpecType teacher multiclass model to generate out-of-fold probabilities + entropy as extra features (legal stacking).\n",
    "\n",
    "\n",
    "## Global features\n",
    "\n",
    "These are computed using all observations across all bands for a given object.  \n",
    "They summarize time coverage, brightness distribution, cadence, variability, and context (redshift + dust + redshift uncertainty).\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_obs` | Total number of observations across all filters | Captures overall sampling density and how well-measured the object is |\n",
    "| `total_time_obs` | Observed-frame time baseline: `max(t_rel) - min(t_rel)` | Separates short transients vs long events and measures overall monitoring duration |\n",
    "| `total_time_rest` | Rest-frame time baseline: `total_time_obs / (1+z)` | Removes time dilation so the model compares intrinsic evolution speed across redshifts |\n",
    "\n",
    "### Flux distribution (dust-corrected `flux_corr`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `flux_mean` | Mean corrected flux | Measures average intrinsic brightness level (sensitive to sustained high flux) |\n",
    "| `flux_median` | Median corrected flux | Robust typical brightness baseline (less sensitive to one-off spikes) |\n",
    "| `flux_std` | Standard deviation of corrected flux | Captures variability strength (high = more change over time) |\n",
    "| `flux_min` | Minimum corrected flux | Captures deep fades / dips / negative excursions from noise-subtraction artifacts |\n",
    "| `flux_max` | Maximum corrected flux | Captures peak brightness or flare intensity (key transient signature) |\n",
    "| `flux_mad` | Median absolute deviation of corrected flux | Robust variability estimate that doesn’t get bullied by outliers |\n",
    "| `flux_iqr` | Interquartile range of corrected flux | Another robust variability measure (spread of the middle 50%) |\n",
    "| `flux_skew` | Skewness of corrected flux distribution | Detects asymmetric lightcurves (fast rise / slow decay vs vice versa) |\n",
    "| `flux_kurt_excess` | Excess kurtosis of corrected flux distribution | Detects heavy tails/spiky behavior from rare bursts or sharp transients |\n",
    "| `flux_p5` | 5th percentile of corrected flux | Robust low-end level (less sensitive than min) |\n",
    "| `flux_p25` | 25th percentile of corrected flux | Lower-quartile level |\n",
    "| `flux_p75` | 75th percentile of corrected flux | Upper-quartile level |\n",
    "| `flux_p95` | 95th percentile of corrected flux | Robust high-end level (less sensitive than max) |\n",
    "| `robust_amp_global` | Robust amplitude: `flux_p95 - flux_p5` | Outlier-resistant variability scale, often better than max-min |\n",
    "| `neg_flux_frac` | Fraction of corrected flux values `< 0` | Flags noise-dominated objects or weak detections where measurements hover around zero |\n",
    "\n",
    "### SNR (using corrected errors `err_corr`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `snr_median` | Median SNR where `snr = \\|flux_corr\\| / (err_corr + EPS)` | Typical detection quality (separates clean signals from noisy junk) |\n",
    "| `snr_max` | Maximum SNR | Captures the strongest detection event (some transients “light up” briefly) |\n",
    "\n",
    "### Cadence / gaps\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `median_dt` | Median time gap between consecutive observations in `t_rel` | Describes typical cadence (important since sparse sampling hides shape) |\n",
    "| `max_gap` | Maximum time gap between consecutive observations in `t_rel` | Detects missing windows (large gaps can explain unreliable peak/width estimates) |\n",
    "\n",
    "### Time-series variability / shape\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `eta_von_neumann` | Von Neumann eta statistic on `flux_corr` (smoothness vs jumpiness) | Separates smooth evolving curves from noisy jitter or sudden jumps |\n",
    "| `chi2_const_global` | Chi-square vs constant-flux model using `err_corr` | Quantifies variability relative to measurement noise (true variability vs noise) |\n",
    "| `stetsonJ_global_obs` | Stetson J (consecutive-pairs) using observed-frame times | More cadence-aware correlation metric; reduces sensitivity to irregular sampling |\n",
    "| `stetsonJ_global_rest` | Stetson J (consecutive-pairs) using rest-frame times | Same correlation idea, but corrected for time dilation |\n",
    "\n",
    "### Slopes / rate of change (global)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `max_slope_global_obs` | Maximum absolute slope in observed time (`t_rel`) | Captures fastest brightness change (sharp rise/fall events) |\n",
    "| `max_slope_global_rest` | Maximum absolute slope in rest-frame time (`t_rest`) | Intrinsic fastest change rate (removes redshift stretching) |\n",
    "| `med_abs_slope_global_obs` | Median absolute slope in observed time | Typical observed change rate (slow drifters vs active transients) |\n",
    "| `med_abs_slope_global_rest` | Median absolute slope in rest-frame time | Typical intrinsic change rate |\n",
    "| `slope_global_obs` | Best-fit linear slope over observed time | Captures long-term trend direction (rising vs fading overall) |\n",
    "| `slope_global_rest` | Best-fit linear slope over rest-frame time | Same trend, but comparable across redshifts |\n",
    "\n",
    "### Fractional variability\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `fvar_global` | Fractional variability accounting for measurement errors | Estimates intrinsic variability strength after subtracting noise contribution |\n",
    "\n",
    "### Context metadata\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `Z` | Redshift `z` | Encodes distance/epoch effects and shifts events into different observed regimes |\n",
    "| `log1pZ` | `log(1+z)` | Stabilizes redshift scaling for models (less extreme leverage at high `z`) |\n",
    "| `Z_err` | Redshift uncertainty (clipped to `>= 0`) | Captures confidence in rest-frame correction; noisy redshifts degrade timing features |\n",
    "| `log1pZerr` | `log(1+Z_err)` | Stabilizes uncertainty scaling and helps tree models split more smoothly |\n",
    "| `EBV` | Dust reddening used for extinction correction | Helps the model learn residual dust systematics and measurement conditions |\n",
    "\n",
    "### Filter coverage\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_filters_present` | Number of filters with ≥ 1 observation | Multi-band coverage gives richer color/shape info; missing bands can correlate with class |\n",
    "| `total_obs` | Total observations summed across all filters (same as `n_obs`) | Redundant but convenient sanity/coverage signal that some models exploit |\n",
    "\n",
    "## Per-filter (band-wise) features\n",
    "\n",
    "For each band `b ∈ {u,g,r,i,z,y}`, the following features are computed independently per filter.  \n",
    "This version adds **pre-peak baseline features** and richer **post-peak decay morphology**.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `n_{b}` | Number of observations in band `b` | Band missingness and sampling density vary by object/class and affect reliability |\n",
    "| `baseline_pre_{b}` | Estimated baseline flux before the main peak (from earliest fraction of points) | Gives a cleaner “true baseline” than median when post-peak tail biases the median |\n",
    "| `amp_{b}` | Peak above median baseline: `peak_flux - median(fb)` | Simple band strength; works even if pre-peak baseline is unreliable |\n",
    "| `amp_pre_{b}` | Peak above pre-peak baseline: `peak_flux - baseline_pre` | Physically better peak amplitude when baseline is stable; improves peak-related shape features |\n",
    "| `robust_amp_{b}` | Robust amplitude: `p95_b - p5_b` | More stable amplitude estimate when peaks/outliers are noisy |\n",
    "| `tpeak_{b}_obs` | Observed-frame time of peak flux in band `b` | Captures when the band reaches maximum brightness (timing is class-dependent) |\n",
    "| `tpeak_{b}_rest` | Rest-frame time of peak flux: `tpeak_obs / (1+z)` | Removes time dilation so peak timing is comparable across redshifts |\n",
    "| `snrmax_{b}` | Maximum SNR within band `b` | Strongest detection in that band (some classes peak strongly only in certain filters) |\n",
    "| `eta_{b}` | Von Neumann eta within band `b` | Detects smooth evolution vs noise inside a single wavelength band |\n",
    "| `chi2_const_{b}` | Chi-square vs constant-flux model within band | Measures variability significance relative to band-specific noise |\n",
    "| `slope_{b}_obs` | Best-fit linear slope in band over observed time | Captures overall rise/fade trend per band |\n",
    "| `slope_{b}_rest` | Best-fit linear slope in band over rest-frame time | Intrinsic trend per band (comparable across redshifts) |\n",
    "| `maxslope_{b}_obs` | Maximum absolute slope in band (observed time) | Captures sharpest observed change per band |\n",
    "| `maxslope_{b}_rest` | Maximum absolute slope in band (rest time) | Captures sharpest intrinsic change rate per band |\n",
    "| `stetsonJ_{b}_obs` | Stetson J (consecutive-pairs) in band using observed time | Cadence-aware correlation metric per band |\n",
    "| `stetsonJ_{b}_rest` | Stetson J (consecutive-pairs) in band using rest time | Same, but corrected for time dilation |\n",
    "| `fvar_{b}` | Fractional variability within band (noise-corrected) | Intrinsic variability strength per band |\n",
    "| `p5_{b}` | 5th percentile of band flux `fb` | Robust low-end level per band |\n",
    "| `p25_{b}` | 25th percentile of `fb` | Lower-quartile level per band |\n",
    "| `p75_{b}` | 75th percentile of `fb` | Upper-quartile level per band |\n",
    "| `p95_{b}` | 95th percentile of `fb` | Robust high-end level per band |\n",
    "| `mad_{b}` | Median absolute deviation of `fb` | Robust band variability (outlier-resistant) |\n",
    "| `iqr_{b}` | Interquartile range of `fb` | Robust spread of the middle 50% per band |\n",
    "| `mad_over_std_{b}` | `mad_b / (std_b + EPS)` | Flags spike-dominated vs Gaussian-like variability (robustness/shape cue) |\n",
    "\n",
    "### Post-peak fall times, widths, and sharpness (only if `amp_pre_{b} > 0`)\n",
    "\n",
    "These use `baseline_pre_{b}` and `amp_pre_{b}` to define levels as fractions of the peak amplitude.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `t_fall50_{b}_obs` | Observed-frame time after peak to reach `baseline_pre + 0.50 * amp_pre` | Encodes decay speed in observed time (fast vs slow fall) |\n",
    "| `t_fall20_{b}_obs` | Observed-frame time after peak to reach `baseline_pre + 0.20 * amp_pre` | Longer-tail decay behavior; distinguishes slow fade vs quick drop |\n",
    "| `t_fall50_{b}_rest` | Rest-frame fall time to 50% level | Intrinsic decay speed comparable across redshifts |\n",
    "| `t_fall20_{b}_rest` | Rest-frame fall time to 20% level | Intrinsic late-time fading timescale |\n",
    "| `width50_{b}_obs` | Observed-frame width above 50% level (time span where `fb >= base + 0.5*amp`) | Measures how long the event stays bright in observed time |\n",
    "| `width80_{b}_obs` | Observed-frame width above 80% level | Captures core peak width (sharp vs broad peak) |\n",
    "| `width50_{b}_rest` | Rest-frame width above 50% level | Intrinsic duration at mid-brightness |\n",
    "| `width80_{b}_rest` | Rest-frame width above 80% level | Intrinsic core-peak duration |\n",
    "| `sharp50_{b}_obs` | Sharpness proxy: `amp_pre / (width50_obs + EPS)` | High = tall + narrow peaks (very class-discriminative) |\n",
    "| `sharp50_{b}_rest` | Sharpness proxy in rest-frame | Same idea, but intrinsic (less redshift-biased) |\n",
    "| `auc_pos_{b}_obs` | Observed-frame AUC above `baseline_pre`: `∫ max(fb - baseline_pre, 0) dt` | Energy-like summary tied to true baseline, not median-biased |\n",
    "| `auc_pos_{b}_rest` | Rest-frame AUC above `baseline_pre` | Comparable across redshifts; strong spectral-energy cue |\n",
    "\n",
    "### Peak structure and post-peak behavior (only if `amp_pre_{b} > 0`)\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `peak_dominance_{b}` | `amp_pre / (mad_pre + EPS)` where `mad_pre` is pre-peak baseline MAD | Measures how dominant the peak is relative to baseline noise (real transients pop out) |\n",
    "| `std_ratio_prepost_{b}` | `std(pre_seg) / (std(post_seg) + EPS)` | Captures how variability changes after peak (e.g., noisy baseline vs smooth decay) |\n",
    "| `n_peaks_{b}` | Count of significant peaks above baseline (sigma-thresholded) | Distinguishes single-peaked transients from multi-peaked/variable sources |\n",
    "| `postpeak_monotone_frac_{b}` | Fraction of post-peak steps that are monotonic decreasing | Smooth decays (high) vs rebrightening/AGN-like variability (low) |\n",
    "| `n_rebrighten_{b}` | Count of rebrightening events after peak (relative to `amp_pre`) | Strong discriminator: rebrightening often means non-simple transient behavior |\n",
    "\n",
    "### Decay power-law fit (post-peak, only if `amp_pre_{b} > 0`)\n",
    "\n",
    "A power-law fit is attempted on the post-peak decay segment (up to a max time window).\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `decay_pl_slope_{b}_obs` | Fitted power-law decay slope in observed time | Encodes decay physics/shape; different classes have different decay slopes |\n",
    "| `decay_pl_r2_{b}_obs` | R² of the observed-frame power-law fit | Measures how well a clean power-law explains the decay (clean transient vs messy variability) |\n",
    "| `decay_pl_npts_{b}_obs` | Number of points used in the observed-frame decay fit | Reliability indicator: more points = more trustworthy slope |\n",
    "| `decay_pl_slope_{b}_rest` | Fitted power-law decay slope in rest-frame time | Intrinsic decay slope, comparable across redshifts |\n",
    "| `decay_pl_r2_{b}_rest` | R² of the rest-frame power-law fit | Fit quality after time dilation correction |\n",
    "| `decay_pl_npts_{b}_rest` | Number of points used in the rest-frame decay fit | Reliability indicator in rest-frame |\n",
    "\n",
    "## Multi-band peak timing dispersion\n",
    "\n",
    "These summarize how synchronized (or not) the band peaks are across filters.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `tpeak_std_obs` | Standard deviation of `tpeak_b_obs` across bands with peaks | Measures chromatic timing spread in observed time (class-dependent) |\n",
    "| `tpeak_std_rest` | Standard deviation of `tpeak_b_rest` across bands with peaks | Intrinsic chromatic peak spread (less redshift-biased) |\n",
    "\n",
    "## Cross-band pair features (adjacent pairs: `ug, gr, ri, iz, zy`)\n",
    "\n",
    "For each adjacent filter pair `(a,b)`, these compare peak timing and peak flux ratios across wavelengths.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `tpeakdiff_{a}{b}_obs` | Observed-frame peak time difference: `tpeak_a_obs - tpeak_b_obs` | Chromatic peak lag/lead in observed time (includes cadence + dilation effects) |\n",
    "| `tpeakdiff_{a}{b}_rest` | Rest-frame peak time difference: `tpeak_a_rest - tpeak_b_rest` | Intrinsic chromatic lag/lead; strong class signature (blue earlier than red, etc.) |\n",
    "| `peakratio_{a}{b}` | Peak flux ratio: `peak_flux_a / (peak_flux_b + EPS)` | Strong color/SED proxy without needing explicit magnitudes |\n",
    "\n",
    "## Color features at r-peak (observed-frame) + 20/40-day evolution\n",
    "\n",
    "These interpolate `g`, `r`, `i` flux at the observed time when the r-band peaks (`tpeak_r_obs`), then compute log-flux colors.  \n",
    "They also sample the same colors at `+20` and `+40` days to capture cooling/heating trends.\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `color_gr_at_rpeak_obs` | `log1p(f_g) - log1p(f_r)` evaluated at `tpeak_r_obs` | Measures g-r color at peak, highly class-dependent |\n",
    "| `color_ri_at_rpeak_obs` | `log1p(f_r) - log1p(f_i)` evaluated at `tpeak_r_obs` | Measures r-i color at peak (temperature / SED proxy) |\n",
    "| `color_gr_rpeak_p20_obs` | g-r color at `tpeak_r_obs + 20` days | Captures medium-term color evolution after peak |\n",
    "| `color_ri_rpeak_p20_obs` | r-i color at `tpeak_r_obs + 20` days | Same, for redder color index |\n",
    "| `color_gr_rpeak_p40_obs` | g-r color at `tpeak_r_obs + 40` days | Captures longer-term cooling/heating behavior |\n",
    "| `color_ri_rpeak_p40_obs` | r-i color at `tpeak_r_obs + 40` days | Longer-term evolution in redder bands |\n",
    "| `color_gr_slope20_obs` | `(color_gr(+20) - color_gr(0)) / 20` | Rate of color change over 20 days (cooling slope) |\n",
    "| `color_ri_slope20_obs` | `(color_ri(+20) - color_ri(0)) / 20` | Rate of red color change over 20 days |\n",
    "| `color_gr_slope40_obs` | `(color_gr(+40) - color_gr(0)) / 40` | Rate of color change over 40 days (more stable, less noisy) |\n",
    "| `color_ri_slope40_obs` | `(color_ri(+40) - color_ri(0)) / 40` | Rate of red color change over 40 days |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_object(lc_raw, z, z_err, ebv):\n",
    "    feats = {}\n",
    "\n",
    "    # Sort observations by time so time-based calculations make sense\n",
    "    lc = lc_raw.sort_values(\"Time (MJD)\").reset_index(drop=True)\n",
    "\n",
    "    # Extract time values and filter (band) labels\n",
    "    t = lc[\"Time (MJD)\"].to_numpy().astype(float)\n",
    "    filt = lc[\"Filter\"].to_numpy()\n",
    "\n",
    "    # If there are no observations, return minimal info\n",
    "    if len(t) == 0:\n",
    "        feats[\"n_obs\"] = 0\n",
    "        return feats\n",
    "\n",
    "    # Make sure metadata fields are valid numbers (avoid NaNs / strings / missing values)\n",
    "    z = safe_float(z, default=0.0)                     # redshift (distance proxy)\n",
    "    z_err = safe_float(z_err, default=0.0)             # redshift uncertainty\n",
    "    ebv = safe_float(ebv, default=np.nan)              # dust amount (can be missing)\n",
    "\n",
    "    # Convert time to start at 0 (relative time axis)\n",
    "    t_rel = t - t.min()\n",
    "\n",
    "    # Convert observed time to intrinsic time of the object\n",
    "    # Distant objects appear to evolve slower, so divide by (1 + z)\n",
    "    t_rest = t_rel / (1.0 + z)\n",
    "\n",
    "    # Correct brightness values for dust in the Milky Way\n",
    "    # (dust makes objects look dimmer than they really are)\n",
    "    flux_corr, err_corr = deextinct_lightcurve(lc, ebv)\n",
    "\n",
    "    # Basic observation statistics\n",
    "    feats[\"n_obs\"] = int(len(t))                                  # total number of measurements\n",
    "    feats[\"total_time_obs\"] = float(t_rel.max() - t_rel.min())    # total observed duration\n",
    "    feats[\"total_time_rest\"] = float(t_rest.max() - t_rest.min()) # duration corrected for distance effects\n",
    "\n",
    "    # Global brightness statistics (after dust correction)\n",
    "    feats[\"flux_mean\"] = float(np.mean(flux_corr))                # average brightness level\n",
    "    feats[\"flux_median\"] = float(np.median(flux_corr))            # robust typical brightness\n",
    "    feats[\"flux_std\"] = float(np.std(flux_corr))                  # overall variability\n",
    "    feats[\"flux_min\"] = float(np.min(flux_corr))                  # dimmest point\n",
    "    feats[\"flux_max\"] = float(np.max(flux_corr))                  # brightest point\n",
    "\n",
    "    # Robust statistics that are less sensitive to outliers\n",
    "    feats[\"flux_mad\"] = median_abs_dev(flux_corr)                 # median absolute deviation\n",
    "    feats[\"flux_iqr\"] = iqr(flux_corr)                            # interquartile range (Q3 - Q1)\n",
    "\n",
    "    # Distribution shape features\n",
    "    feats[\"flux_skew\"] = skewness(flux_corr)                      # asymmetry of values\n",
    "    feats[\"flux_kurt_excess\"] = kurtosis_excess(flux_corr)        # tail heaviness / spikiness\n",
    "\n",
    "    # Robust amplitude using percentiles (stable against a few extreme points)\n",
    "    p5, p25, p75, p95 = np.percentile(flux_corr, [5, 25, 75, 95])\n",
    "    feats[\"flux_p5\"] = float(p5)\n",
    "    feats[\"flux_p25\"] = float(p25)\n",
    "    feats[\"flux_p75\"] = float(p75)\n",
    "    feats[\"flux_p95\"] = float(p95)\n",
    "    feats[\"robust_amp_global\"] = float(p95 - p5)                  # robust amplitude proxy\n",
    "\n",
    "    # Fraction of measurements that are below zero\n",
    "    # (often indicates noise-dominated detections)\n",
    "    feats[\"neg_flux_frac\"] = float(np.mean(flux_corr < 0))\n",
    "\n",
    "    # Signal-to-noise ratio summaries\n",
    "    snr = np.abs(flux_corr) / (err_corr + EPS)\n",
    "    feats[\"snr_median\"] = float(np.median(snr))                   # typical signal quality\n",
    "    feats[\"snr_max\"] = float(np.max(snr))                         # strongest detection\n",
    "\n",
    "    # Observation timing properties\n",
    "    if len(t_rel) >= 2:\n",
    "        dt = np.diff(t_rel)\n",
    "        feats[\"median_dt\"] = float(np.median(dt))                 # typical time between observations\n",
    "        feats[\"max_gap\"] = float(np.max(dt))                      # largest observation gap\n",
    "    else:\n",
    "        feats[\"median_dt\"] = np.nan                               # undefined with <2 points\n",
    "        feats[\"max_gap\"] = np.nan\n",
    "\n",
    "    # Global time-series variability diagnostics\n",
    "    feats[\"eta_von_neumann\"] = von_neumann_eta(flux_corr)              # smoothness vs noise proxy\n",
    "    feats[\"chi2_const_global\"] = chi2_to_constant(flux_corr, err_corr) # variability vs constant model\n",
    "\n",
    "    # Stetson J using consecutive pairs (more time-series aware than global pairing)\n",
    "    feats[\"stetsonJ_global_obs\"] = stetson_J_consecutive(t_rel, flux_corr, err_corr)\n",
    "    feats[\"stetsonJ_global_rest\"] = stetson_J_consecutive(t_rest, flux_corr, err_corr)\n",
    "\n",
    "    # Global slope features (obs + rest frame)\n",
    "    feats[\"max_slope_global_obs\"] = max_slope(t_rel, flux_corr)        # fastest brightness change (obs)\n",
    "    feats[\"max_slope_global_rest\"] = max_slope(t_rest, flux_corr)      # fastest brightness change (rest)\n",
    "\n",
    "    feats[\"med_abs_slope_global_obs\"] = median_abs_slope(t_rel, flux_corr)   # typical change rate (obs)\n",
    "    feats[\"med_abs_slope_global_rest\"] = median_abs_slope(t_rest, flux_corr) # typical change rate (rest)\n",
    "\n",
    "    feats[\"slope_global_obs\"] = linear_slope(t_rel, flux_corr)         # best-fit linear trend (obs)\n",
    "    feats[\"slope_global_rest\"] = linear_slope(t_rest, flux_corr)       # best-fit linear trend (rest)\n",
    "\n",
    "    # Fractional variability (accounts for measurement noise)\n",
    "    feats[\"fvar_global\"] = fractional_variability(flux_corr, err_corr)\n",
    "\n",
    "    # Metadata features\n",
    "    feats[\"Z\"] = float(z)                           # distance proxy (redshift)\n",
    "    feats[\"log1pZ\"] = float(np.log1p(max(0.0, z)))  # compressed redshift scale\n",
    "    feats[\"Z_err\"] = float(max(0.0, z_err))         # clamp negative uncertainty to 0\n",
    "    feats[\"log1pZerr\"] = float(np.log1p(max(0.0, feats[\"Z_err\"])))  # compressed uncertainty scale\n",
    "    feats[\"EBV\"] = ebv                              # dust amount\n",
    "\n",
    "    # Counters for band coverage\n",
    "    feats[\"n_filters_present\"] = 0                  # how many bands have >= 1 observation\n",
    "    feats[\"total_obs\"] = 0                          # total observations across all bands\n",
    "\n",
    "    # Storage for cross-band timing/color features later\n",
    "    band_tpeak_obs = {}                             # per-band peak time (obs frame)\n",
    "    band_tpeak_rest = {}                            # per-band peak time (rest frame)\n",
    "    band_peak_flux = {}                             # per-band peak flux\n",
    "    band_tb_obs = {}                                # per-band time arrays (obs frame)\n",
    "    band_tb_rest = {}                               # per-band time arrays (rest frame)\n",
    "    band_fb = {}                                    # per-band flux arrays (dust-corrected)\n",
    "\n",
    "    # Loop over each wavelength band (u, g, r, i, z, y)\n",
    "    for b in FILTERS:\n",
    "        m = (filt == b)\n",
    "        nb = int(np.sum(m))\n",
    "\n",
    "        # Number of observations in this band\n",
    "        feats[f\"n_{b}\"] = nb\n",
    "        feats[\"total_obs\"] += nb\n",
    "\n",
    "        # Initialize band features as missing by default\n",
    "        # (we only fill these if the band has data)\n",
    "        for k in [\n",
    "            f\"amp_{b}\",                        # peak - median amplitude\n",
    "            f\"amp_pre_{b}\",                    # peak - pre-peak baseline amplitude\n",
    "            f\"baseline_pre_{b}\",               # estimated pre-peak baseline flux\n",
    "            f\"robust_amp_{b}\",                 # p95 - p5 amplitude (robust)\n",
    "            f\"tpeak_{b}_obs\",                  # time of peak (observed)\n",
    "            f\"tpeak_{b}_rest\",                 # time of peak (rest)\n",
    "            f\"width50_{b}_obs\",                # width above 50% amplitude (obs)\n",
    "            f\"width50_{b}_rest\",               # width above 50% amplitude (rest)\n",
    "            f\"width80_{b}_obs\",                # width above 80% amplitude (obs)\n",
    "            f\"width80_{b}_rest\",               # width above 80% amplitude (rest)\n",
    "            f\"auc_pos_{b}_obs\",                # area above baseline, positive only (obs)\n",
    "            f\"auc_pos_{b}_rest\",               # area above baseline, positive only (rest)\n",
    "            f\"snrmax_{b}\",                     # best SNR in this band\n",
    "            f\"eta_{b}\",                        # Von Neumann smoothness for this band\n",
    "            f\"chi2_const_{b}\",                 # variability vs constant model (band)\n",
    "            f\"slope_{b}_obs\",                  # best-fit linear trend (obs)\n",
    "            f\"slope_{b}_rest\",                 # best-fit linear trend (rest)\n",
    "            f\"maxslope_{b}_obs\",               # fastest change rate (obs)\n",
    "            f\"maxslope_{b}_rest\",              # fastest change rate (rest)\n",
    "            f\"stetsonJ_{b}_obs\",               # correlated variability (obs)\n",
    "            f\"stetsonJ_{b}_rest\",              # correlated variability (rest)\n",
    "            f\"p5_{b}\",                         # 5th percentile flux\n",
    "            f\"p25_{b}\",                        # 25th percentile flux\n",
    "            f\"p75_{b}\",                        # 75th percentile flux\n",
    "            f\"p95_{b}\",                        # 95th percentile flux\n",
    "            f\"mad_{b}\",                        # median absolute deviation\n",
    "            f\"iqr_{b}\",                        # interquartile range\n",
    "            f\"mad_over_std_{b}\",               # robust-to-standard variability ratio\n",
    "            f\"fvar_{b}\",                       # fractional variability (noise-corrected)\n",
    "            f\"t_fall50_{b}_obs\",               # time from peak to 50% level (obs)\n",
    "            f\"t_fall20_{b}_obs\",               # time from peak to 20% level (obs)\n",
    "            f\"t_fall50_{b}_rest\",              # time from peak to 50% level (rest)\n",
    "            f\"t_fall20_{b}_rest\",              # time from peak to 20% level (rest)\n",
    "            f\"sharp50_{b}_obs\",                # amplitude / width50 (obs)\n",
    "            f\"sharp50_{b}_rest\",               # amplitude / width50 (rest)\n",
    "            f\"peak_dominance_{b}\",             # peak amplitude relative to pre-peak variability\n",
    "            f\"std_ratio_prepost_{b}\",          # std(pre-peak) / std(post-peak)\n",
    "            f\"n_peaks_{b}\",                    # count of significant peaks\n",
    "            f\"postpeak_monotone_frac_{b}\",     # how monotone the decay is\n",
    "            f\"n_rebrighten_{b}\",               # number of rebrightening episodes\n",
    "            f\"decay_pl_slope_{b}_obs\",         # power-law decay slope (obs)\n",
    "            f\"decay_pl_r2_{b}_obs\",            # power-law fit quality (obs)\n",
    "            f\"decay_pl_npts_{b}_obs\",          # points used in decay fit (obs)\n",
    "            f\"decay_pl_slope_{b}_rest\",        # power-law decay slope (rest)\n",
    "            f\"decay_pl_r2_{b}_rest\",           # power-law fit quality (rest)\n",
    "            f\"decay_pl_npts_{b}_rest\",         # points used in decay fit (rest)\n",
    "        ]:\n",
    "            feats[k] = np.nan\n",
    "\n",
    "        # Skip bands with no data\n",
    "        if nb == 0:\n",
    "            continue\n",
    "\n",
    "        feats[\"n_filters_present\"] += 1\n",
    "\n",
    "        # Extract time, brightness, and error for this band\n",
    "        tb_obs = t_rel[m]\n",
    "        fb = flux_corr[m]\n",
    "        eb = err_corr[m]\n",
    "\n",
    "        # Sort observations within the band by time\n",
    "        order = np.argsort(tb_obs)\n",
    "        tb_obs = tb_obs[order]\n",
    "        fb = fb[order]\n",
    "        eb = eb[order]\n",
    "\n",
    "        # Convert to intrinsic time scale\n",
    "        tb_rest = tb_obs / (1.0 + z)\n",
    "\n",
    "        # Robust per-band amplitude using percentiles (stable against outliers)\n",
    "        p5b, p25b, p75b, p95b = np.percentile(fb, [5, 25, 75, 95])\n",
    "        feats[f\"p5_{b}\"] = float(p5b)\n",
    "        feats[f\"p25_{b}\"] = float(p25b)\n",
    "        feats[f\"p75_{b}\"] = float(p75b)\n",
    "        feats[f\"p95_{b}\"] = float(p95b)\n",
    "        feats[f\"robust_amp_{b}\"] = float(p95b - p5b)\n",
    "\n",
    "        # Robust variability summaries\n",
    "        feats[f\"mad_{b}\"] = median_abs_dev(fb)\n",
    "        feats[f\"iqr_{b}\"] = iqr(fb)\n",
    "        stdb = float(np.std(fb))\n",
    "        feats[f\"mad_over_std_{b}\"] = float(feats[f\"mad_{b}\"] / (stdb + EPS))\n",
    "\n",
    "        # Estimate a pre-peak baseline using early observations\n",
    "        # (better baseline for transients than global median when the event dominates)\n",
    "        base_pre, mad_pre, mederr_pre = pre_peak_baseline(tb_obs, fb, eb, frac=PRE_BASE_FRAC)\n",
    "        feats[f\"baseline_pre_{b}\"] = float(base_pre) if np.isfinite(base_pre) else np.nan\n",
    "\n",
    "        # Identify peak flux and peak time\n",
    "        pidx = int(np.argmax(fb))\n",
    "        peak_flux = float(fb[pidx])\n",
    "        tpeak_obs = float(tb_obs[pidx])\n",
    "        tpeak_rest = float(tb_rest[pidx])\n",
    "\n",
    "        # Amplitude relative to two different baselines\n",
    "        amp_median = peak_flux - float(np.median(fb))                             # peak relative to median\n",
    "        amp_pre = peak_flux - base_pre if np.isfinite(base_pre) else np.nan       # peak relative to pre-peak baseline\n",
    "\n",
    "        feats[f\"amp_{b}\"] = float(amp_median)\n",
    "        feats[f\"amp_pre_{b}\"] = float(amp_pre) if np.isfinite(amp_pre) else np.nan\n",
    "\n",
    "        feats[f\"tpeak_{b}_obs\"] = tpeak_obs\n",
    "        feats[f\"tpeak_{b}_rest\"] = tpeak_rest\n",
    "        feats[f\"snrmax_{b}\"] = float(np.max(np.abs(fb) / (eb + EPS)))              # best detection quality\n",
    "\n",
    "        # Band-level variability diagnostics\n",
    "        feats[f\"eta_{b}\"] = von_neumann_eta(fb)                                    # smoothness vs noise\n",
    "        feats[f\"chi2_const_{b}\"] = chi2_to_constant(fb, eb)                        # variability vs constant\n",
    "\n",
    "        feats[f\"slope_{b}_obs\"] = linear_slope(tb_obs, fb)                         # best-fit trend (obs)\n",
    "        feats[f\"slope_{b}_rest\"] = linear_slope(tb_rest, fb)                       # best-fit trend (rest)\n",
    "\n",
    "        feats[f\"maxslope_{b}_obs\"] = max_slope(tb_obs, fb)                         # fastest change (obs)\n",
    "        feats[f\"maxslope_{b}_rest\"] = max_slope(tb_rest, fb)                       # fastest change (rest)\n",
    "\n",
    "        feats[f\"stetsonJ_{b}_obs\"] = stetson_J_consecutive(tb_obs, fb, eb)         # correlated variability (obs)\n",
    "        feats[f\"stetsonJ_{b}_rest\"] = stetson_J_consecutive(tb_rest, fb, eb)       # correlated variability (rest)\n",
    "\n",
    "        feats[f\"fvar_{b}\"] = fractional_variability(fb, eb)                        # noise-corrected variability\n",
    "\n",
    "        # Peak morphology features only make sense if we have a positive transient above baseline\n",
    "        if np.isfinite(amp_pre) and amp_pre > 0:\n",
    "            # Peak dominance: how strong the peak is compared to baseline variability\n",
    "            feats[f\"peak_dominance_{b}\"] = float(amp_pre / (mad_pre + EPS))\n",
    "\n",
    "            # Pre vs post variability ratio (separates stable baseline vs messy post-peak behavior)\n",
    "            pre_seg = fb[:max(2, pidx)]\n",
    "            post_seg = fb[pidx:]\n",
    "            std_pre = float(np.std(pre_seg)) if len(pre_seg) >= 2 else np.nan\n",
    "            std_post = float(np.std(post_seg)) if len(post_seg) >= 2 else np.nan\n",
    "            if np.isfinite(std_pre) and np.isfinite(std_post):\n",
    "                feats[f\"std_ratio_prepost_{b}\"] = float(std_pre / (std_post + EPS))\n",
    "\n",
    "            # Post-peak shape summaries\n",
    "            feats[f\"postpeak_monotone_frac_{b}\"] = float(postpeak_monotonicity(tb_obs, fb, pidx))  # decay smoothness\n",
    "            feats[f\"n_peaks_{b}\"] = float(count_significant_peaks(tb_obs, fb, eb, base_pre, k_sigma=PEAK_SIGMA_K))     # multi-peak behavior\n",
    "            feats[f\"n_rebrighten_{b}\"] = float(count_rebrighten(tb_obs, fb, base_pre, amp_pre, pidx, frac=REBRIGHT_FRAC)) # rebrightening episodes\n",
    "\n",
    "            # Fall times from peak to given fractional levels (observed + rest frame)\n",
    "            feats[f\"t_fall50_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_obs\"] = float(fall_time_to_level(tb_obs, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "            feats[f\"t_fall50_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.50))\n",
    "            feats[f\"t_fall20_{b}_rest\"] = float(fall_time_to_level(tb_rest, fb, base_pre, amp_pre, pidx, frac=0.20))\n",
    "\n",
    "            # Area under the curve above the pre-peak baseline (positive only)\n",
    "            feats[f\"auc_pos_{b}_obs\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_obs))\n",
    "            feats[f\"auc_pos_{b}_rest\"] = float(trapz_safe(np.maximum(fb - base_pre, 0.0), tb_rest))\n",
    "\n",
    "            # Width at a given fractional level (simple “time above threshold” proxy)\n",
    "            def width_at_level(tt, ff, base, amp, frac):\n",
    "                if amp <= 0 or len(ff) < 3:\n",
    "                    return np.nan\n",
    "                level = base + frac * amp\n",
    "                above = ff >= level\n",
    "                if not np.any(above):\n",
    "                    return np.nan\n",
    "                idx = np.where(above)[0]\n",
    "                return float(tt[idx[-1]] - tt[idx[0]])\n",
    "\n",
    "            # Widths at 50% and 80% of amplitude (obs + rest frame)\n",
    "            w50_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_obs = width_at_level(tb_obs, fb, base_pre, amp_pre, 0.80)\n",
    "            w50_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.50)\n",
    "            w80_rest = width_at_level(tb_rest, fb, base_pre, amp_pre, 0.80)\n",
    "\n",
    "            feats[f\"width50_{b}_obs\"] = w50_obs\n",
    "            feats[f\"width80_{b}_obs\"] = w80_obs\n",
    "            feats[f\"width50_{b}_rest\"] = w50_rest\n",
    "            feats[f\"width80_{b}_rest\"] = w80_rest\n",
    "\n",
    "            # Sharpness: high amplitude + short width means a “spiky” transient\n",
    "            feats[f\"sharp50_{b}_obs\"] = float(amp_pre / (w50_obs + EPS)) if np.isfinite(w50_obs) else np.nan\n",
    "            feats[f\"sharp50_{b}_rest\"] = float(amp_pre / (w50_rest + EPS)) if np.isfinite(w50_rest) else np.nan\n",
    "\n",
    "            # Fit a simple power-law decay model post-peak (captures decay steepness)\n",
    "            b_obs, r2_obs, npts_obs = decay_powerlaw_fit(tb_obs, fb, base_pre, pidx, tmax=300.0)\n",
    "            b_rest, r2_rest, npts_rest = decay_powerlaw_fit(tb_rest, fb, base_pre, pidx, tmax=300.0)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_obs\"] = b_obs            # decay slope (obs)\n",
    "            feats[f\"decay_pl_r2_{b}_obs\"] = r2_obs              # fit quality (obs)\n",
    "            feats[f\"decay_pl_npts_{b}_obs\"] = float(npts_obs)   # points used (obs)\n",
    "\n",
    "            feats[f\"decay_pl_slope_{b}_rest\"] = b_rest          # decay slope (rest)\n",
    "            feats[f\"decay_pl_r2_{b}_rest\"] = r2_rest            # fit quality (rest)\n",
    "            feats[f\"decay_pl_npts_{b}_rest\"] = float(npts_rest) # points used (rest)\n",
    "\n",
    "        # Store values for cross-band comparisons and color features\n",
    "        band_tpeak_obs[b] = tpeak_obs\n",
    "        band_tpeak_rest[b] = tpeak_rest\n",
    "        band_peak_flux[b] = peak_flux\n",
    "        band_tb_obs[b] = tb_obs\n",
    "        band_tb_rest[b] = tb_rest\n",
    "        band_fb[b] = fb\n",
    "\n",
    "    # Peak-time dispersion across filters (how synchronized the bands are)\n",
    "    tpeaks_obs = [band_tpeak_obs.get(b, np.nan) for b in FILTERS]\n",
    "    tpeaks_rest = [band_tpeak_rest.get(b, np.nan) for b in FILTERS]\n",
    "\n",
    "    tpeaks_obs = np.array([x for x in tpeaks_obs if np.isfinite(x)], float)\n",
    "    tpeaks_rest = np.array([x for x in tpeaks_rest if np.isfinite(x)], float)\n",
    "\n",
    "    feats[\"tpeak_std_obs\"] = float(np.std(tpeaks_obs)) if len(tpeaks_obs) >= 2 else np.nan\n",
    "    feats[\"tpeak_std_rest\"] = float(np.std(tpeaks_rest)) if len(tpeaks_rest) >= 2 else np.nan\n",
    "\n",
    "    # Cross-band peak-time lags and peak ratios for adjacent filters\n",
    "    pairs = [(\"u\", \"g\"), (\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\"), (\"z\", \"y\")]\n",
    "    for a, b in pairs:\n",
    "        ta_obs, tb_obs = band_tpeak_obs.get(a, np.nan), band_tpeak_obs.get(b, np.nan)\n",
    "        ta_rest, tb_rest = band_tpeak_rest.get(a, np.nan), band_tpeak_rest.get(b, np.nan)\n",
    "        pa, pb = band_peak_flux.get(a, np.nan), band_peak_flux.get(b, np.nan)\n",
    "\n",
    "        feats[f\"tpeakdiff_{a}{b}_obs\"] = (ta_obs - tb_obs) if (np.isfinite(ta_obs) and np.isfinite(tb_obs)) else np.nan\n",
    "        feats[f\"tpeakdiff_{a}{b}_rest\"] = (ta_rest - tb_rest) if (np.isfinite(ta_rest) and np.isfinite(tb_rest)) else np.nan\n",
    "        feats[f\"peakratio_{a}{b}\"] = (pa / (pb + EPS)) if (np.isfinite(pa) and np.isfinite(pb)) else np.nan\n",
    "\n",
    "    # Safe log transform used for color features\n",
    "    # (log1p + clamp avoids log(negative) explosions when flux is noisy)\n",
    "    def logp(x):\n",
    "        if np.isnan(x):\n",
    "            return np.nan\n",
    "        return float(np.log1p(max(0.0, x)))\n",
    "\n",
    "    # Color features anchored at r-band peak time\n",
    "    # (measures spectral shape at peak and how it evolves 20/40 days later)\n",
    "    tpr_obs = feats.get(\"tpeak_r_obs\", np.nan)\n",
    "    if np.isfinite(tpr_obs):\n",
    "        # Compute g-r and r-i colors using interpolated flux at time t0\n",
    "        def colors_at_time(t0):\n",
    "            fr = interp_flux_at_time(band_tb_obs.get(\"r\", np.array([])), band_fb.get(\"r\", np.array([])), t0)\n",
    "            fg = interp_flux_at_time(band_tb_obs.get(\"g\", np.array([])), band_fb.get(\"g\", np.array([])), t0)\n",
    "            fi = interp_flux_at_time(band_tb_obs.get(\"i\", np.array([])), band_fb.get(\"i\", np.array([])), t0)\n",
    "\n",
    "            cgr = (logp(fg) - logp(fr)) if (np.isfinite(fg) and np.isfinite(fr)) else np.nan\n",
    "            cri = (logp(fr) - logp(fi)) if (np.isfinite(fr) and np.isfinite(fi)) else np.nan\n",
    "            return cgr, cri\n",
    "\n",
    "        # Colors at peak\n",
    "        cgr0, cri0 = colors_at_time(tpr_obs)\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = cgr0\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = cri0\n",
    "\n",
    "        # Colors at +20d and +40d to capture slower spectral evolution\n",
    "        cgr20, cri20 = colors_at_time(tpr_obs + 20.0)\n",
    "        cgr40, cri40 = colors_at_time(tpr_obs + 40.0)\n",
    "\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = cgr20\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = cri20\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = cgr40\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = cri40\n",
    "\n",
    "        # Simple finite-difference slopes (color change per day)\n",
    "        def slope(c1, c2, dt):\n",
    "            if np.isfinite(c1) and np.isfinite(c2):\n",
    "                return float((c2 - c1) / dt)\n",
    "            return np.nan\n",
    "\n",
    "        feats[\"color_gr_slope20_obs\"] = slope(cgr0, cgr20, 20.0)\n",
    "        feats[\"color_ri_slope20_obs\"] = slope(cri0, cri20, 20.0)\n",
    "        feats[\"color_gr_slope40_obs\"] = slope(cgr0, cgr40, 40.0)\n",
    "        feats[\"color_ri_slope40_obs\"] = slope(cri0, cri40, 40.0)\n",
    "    else:\n",
    "        # If r-band is missing, color-at-rpeak is undefined\n",
    "        feats[\"color_gr_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_ri_at_rpeak_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p20_obs\"] = np.nan\n",
    "        feats[\"color_gr_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_ri_rpeak_p40_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope20_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope20_obs\"] = np.nan\n",
    "        feats[\"color_gr_slope40_obs\"] = np.nan\n",
    "        feats[\"color_ri_slope40_obs\"] = np.nan\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lightcurve_cache(splits, base_dir, kind=\"train\"):\n",
    "    base_dir = Path(base_dir)\n",
    "    lc_cache = {}\n",
    "    idx_cache = {}\n",
    "\n",
    "    for s in splits:\n",
    "        path = base_dir / str(s) / f\"{kind}_full_lightcurves.csv\"\n",
    "        lc = pd.read_csv(path)\n",
    "        lc[\"object_id\"] = lc[\"object_id\"].astype(str)\n",
    "        groups = lc.groupby(\"object_id\").indices\n",
    "        lc_cache[s] = lc\n",
    "        idx_cache[s] = groups\n",
    "\n",
    "    return lc_cache, idx_cache\n",
    "\n",
    "\n",
    "def get_lightcurve(lc_cache, idx_cache, split, object_id):\n",
    "    idx = idx_cache[split].get(object_id, None)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    return lc_cache[split].iloc[idx]\n",
    "\n",
    "\n",
    "def build_feature_table(\n",
    "    log_df,\n",
    "    lc_cache,\n",
    "    idx_cache,\n",
    "    augment_photoz=False,\n",
    "    test_zerr_pool=None,\n",
    "    n_aug=1,\n",
    "    seed=6\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    if test_zerr_pool is not None:\n",
    "        test_zerr_pool = np.asarray(test_zerr_pool, float)\n",
    "        test_zerr_pool = test_zerr_pool[np.isfinite(test_zerr_pool)]\n",
    "        test_zerr_pool = test_zerr_pool[test_zerr_pool > 0]\n",
    "\n",
    "    for i in range(len(log_df)):\n",
    "        r = log_df.iloc[i]\n",
    "        obj = r[\"object_id\"]\n",
    "        split = r[\"split\"]\n",
    "\n",
    "        lc = get_lightcurve(lc_cache, idx_cache, split, obj)\n",
    "        if lc is None:\n",
    "            feats = {\"n_obs\": 0}\n",
    "            feats[\"object_id\"] = obj\n",
    "            feats[\"split\"] = split\n",
    "            feats[\"photoz_aug\"] = 0\n",
    "            if \"target\" in log_df.columns:\n",
    "                feats[\"target\"] = int(r[\"target\"])\n",
    "            rows.append(feats)\n",
    "            continue\n",
    "\n",
    "        feats = extract_features_for_object(\n",
    "            lc_raw=lc,\n",
    "            z=r[\"Z\"],\n",
    "            z_err=r.get(\"Z_err\", 0.0),\n",
    "            ebv=r[\"EBV\"],\n",
    "        )\n",
    "        feats[\"object_id\"] = obj\n",
    "        feats[\"split\"] = split\n",
    "        feats[\"photoz_aug\"] = 0\n",
    "        if \"target\" in log_df.columns:\n",
    "            feats[\"target\"] = int(r[\"target\"])\n",
    "        rows.append(feats)\n",
    "\n",
    "        if augment_photoz and (\"target\" in log_df.columns) and (test_zerr_pool is not None) and (len(test_zerr_pool) > 0):\n",
    "            z0 = safe_float(r[\"Z\"], default=0.0)\n",
    "            for _ in range(n_aug):\n",
    "                sigma = float(rng.choice(test_zerr_pool))\n",
    "                z_sim = max(0.0, z0 + float(rng.normal(0.0, sigma)))\n",
    "\n",
    "                feats2 = extract_features_for_object(\n",
    "                    lc_raw=lc,\n",
    "                    z=z_sim,\n",
    "                    z_err=sigma,\n",
    "                    ebv=r[\"EBV\"],\n",
    "                )\n",
    "                feats2[\"object_id\"] = obj\n",
    "                feats2[\"split\"] = split\n",
    "                feats2[\"target\"] = int(r[\"target\"])\n",
    "                feats2[\"photoz_aug\"] = 1\n",
    "                rows.append(feats2)\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(df, drop_cols):\n",
    "    X = df.drop(columns=drop_cols).copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    med = X.median(numeric_only=True)\n",
    "    X = X.fillna(med)\n",
    "    X = X.fillna(0.0)\n",
    "    return X\n",
    "\n",
    "\n",
    "def best_threshold_f1(y_true, probs):\n",
    "    ths = np.linspace(0.01, 0.99, 200)\n",
    "    f1s = [f1_score(y_true, probs > t, zero_division=0) for t in ths]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return float(ths[j]), float(f1s[j])\n",
    "\n",
    "\n",
    "def best_alpha_and_threshold(y_true, p_xgb, p_lgb):\n",
    "    alphas = np.linspace(0.0, 1.0, 101)\n",
    "    best = (0.5, 0.5, -1.0)  # alpha, th, f1\n",
    "\n",
    "    for a in alphas:\n",
    "        p = a * p_xgb + (1.0 - a) * p_lgb\n",
    "        th, f1 = best_threshold_f1(y_true, p)\n",
    "        if f1 > best[2]:\n",
    "            best = (float(a), float(th), float(f1))\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7deb1",
   "metadata": {},
   "source": [
    "## SpecType teacher stacking features\n",
    "\n",
    "add_spectype_teacher_features() adds legal stacking features by training a multiclass model on train only to predict a grouped version of SpecType, then appending the predicted class probabilities as new features.\n",
    "\n",
    "Key steps:\n",
    "- Map SpecType --> SpecTypeGroup (TDE, AGN, SNIa, SNother, Other)\n",
    "- Train a LightGBM multiclass model using CV splits by split\n",
    "- Create:\n",
    "  - OOF probabilities for train\n",
    "  - full-fit probabilities for test\n",
    "- Append probabilities + entropy as new features\n",
    "\n",
    "### Per-class probability features\n",
    "\n",
    "For every group label c in classes:\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `p_spec_{c}` | Predicted probability the object belongs to SpecTypeGroup `c` | Injects a strong \"soft label\" summary of transient type, which improves the final binary classifier |\n",
    "\n",
    "### Probability-uncertainty feature\n",
    "\n",
    "| Feature | Meaning | Why it helps |\n",
    "|--------|---------|--------------|\n",
    "| `spec_entropy` | Entropy of the teacher probability vector | High entropy = teacher unsure (ambiguous object); low entropy = confident type signal (more reliable stacking) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10):\n",
    "    \n",
    "    # Join spectroscopic labels onto the feature table\n",
    "    # (SpecType exists only for some training objects)\n",
    "    df = train_feat.merge(train_log[[\"object_id\", \"SpecType\"]], on=\"object_id\", how=\"left\")\n",
    "    spec = df[\"SpecType\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "    # Map detailed SpecType strings into a smaller set of coarse groups\n",
    "    # (reduces label noise and makes the teacher easier to learn)\n",
    "    def map_group(s):\n",
    "        if s == \"TDE\":\n",
    "            return \"TDE\"\n",
    "        if s == \"AGN\":\n",
    "            return \"AGN\"\n",
    "        if s == \"SN Ia\" or s.startswith(\"SN Ia\"):\n",
    "            return \"SNIa\"\n",
    "        if s.startswith(\"SN\"):\n",
    "            return \"SNother\"\n",
    "        return \"Other\"\n",
    "\n",
    "    spec_group = spec.map(map_group).astype(str)\n",
    "\n",
    "    # Encode group labels into integers for multiclass training\n",
    "    classes = sorted(spec_group.unique())\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_mc = spec_group.map(class_to_idx).to_numpy()\n",
    "\n",
    "    # Build train/test matrices using only shared columns\n",
    "    # (teacher must not depend on train-only columns)\n",
    "    X_tr = clean_features(df, drop_cols=[\"object_id\", \"split\", \"target\", \"SpecType\"])\n",
    "    X_te = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"])\n",
    "\n",
    "    # Use split folders as groups to prevent leakage across simulated splits\n",
    "    groups = df[\"split\"].to_numpy()\n",
    "\n",
    "    # Stratified group CV:\n",
    "    # - keeps split groups intact\n",
    "    # - approximately balances SpecTypeGroup across folds\n",
    "    splitter = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=6)\n",
    "    split_iter = splitter.split(X_tr, y_mc, groups)\n",
    "\n",
    "    # Out-of-fold predicted probabilities for the teacher\n",
    "    oof = np.zeros((len(X_tr), len(classes)), dtype=float)\n",
    "\n",
    "    # LightGBM multiclass teacher configuration\n",
    "    base = dict(\n",
    "        objective=\"multiclass\",\n",
    "        num_class=len(classes),\n",
    "        metric=\"multi_logloss\",\n",
    "\n",
    "        n_estimators=20000,\n",
    "        learning_rate=0.03,\n",
    "\n",
    "        num_leaves=63,\n",
    "        min_child_samples=5,\n",
    "\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=0.8,\n",
    "\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=-1,\n",
    "        force_col_wise=True\n",
    "    )\n",
    "\n",
    "    # Train teacher in CV and collect OOF probabilities (legal stacking)\n",
    "    for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "        model = LGBMClassifier(**base)\n",
    "        model.fit(\n",
    "            X_tr.iloc[tr_idx], y_mc[tr_idx],\n",
    "            eval_set=[(X_tr.iloc[va_idx], y_mc[va_idx])],\n",
    "            eval_metric=\"multi_logloss\",\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "        )\n",
    "\n",
    "        # Predict probabilities on the validation fold using the best iteration\n",
    "        oof[va_idx] = model.predict_proba(\n",
    "            X_tr.iloc[va_idx],\n",
    "            num_iteration=model.best_iteration_\n",
    "        )\n",
    "        print(f\"[SpecType teacher] fold {fold:02d} done\")\n",
    "\n",
    "    # Fit on full training data and predict teacher probabilities for test\n",
    "    model_full = LGBMClassifier(**base)\n",
    "    model_full.fit(X_tr, y_mc)\n",
    "    p_test = model_full.predict_proba(X_te)\n",
    "\n",
    "    # Entropy summary: high entropy means the teacher is uncertain\n",
    "    # (uncertainty itself can be predictive)\n",
    "    def entropy(p):\n",
    "        p = np.clip(p, 1e-12, 1.0)\n",
    "        return -np.sum(p * np.log(p), axis=1)\n",
    "\n",
    "    # Append per-class SpecTypeGroup probabilities as new features\n",
    "    for i, c in enumerate(classes):\n",
    "        train_feat[f\"p_spec_{c}\"] = oof[:, i]\n",
    "        test_feat[f\"p_spec_{c}\"] = p_test[:, i]\n",
    "\n",
    "    # Append teacher uncertainty\n",
    "    train_feat[\"spec_entropy\"] = entropy(oof)\n",
    "    test_feat[\"spec_entropy\"] = entropy(p_test)\n",
    "\n",
    "    return train_feat, test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_xgb(train_feat, n_folds_tune=10, timeout_sec=28800):\n",
    "    y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "    groups = train_feat[\"split\"].to_numpy()\n",
    "\n",
    "    X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"])\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"random_state\": 6,\n",
    "            \"n_jobs\": -1,\n",
    "\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"device\": \"cuda\",\n",
    "\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 8000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.003, 0.12, log=True),\n",
    "\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 40),\n",
    "\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 20.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.05, 30.0),\n",
    "\n",
    "            \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "\n",
    "            \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "        }\n",
    "\n",
    "        if params[\"grow_policy\"] == \"lossguide\":\n",
    "            params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 256)\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        splitter = StratifiedGroupKFold(n_splits=n_folds_tune, shuffle=True, random_state=6)\n",
    "        split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "            neg = np.sum(y_tr == 0)\n",
    "            pos = np.sum(y_tr == 1)\n",
    "            params[\"scale_pos_weight\"] = float(neg / max(1, pos))\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "            probs = model.predict_proba(X_va)[:, 1]\n",
    "            ap = average_precision_score(y_va, probs)\n",
    "            scores.append(ap)\n",
    "\n",
    "            trial.report(float(np.mean(scores)), step=fold)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=6, multivariate=True, group=True)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=30, n_warmup_steps=3)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name=\"xgb_ap_split_cv_gpu\",\n",
    "        storage=\"sqlite:///optuna_xgb_ap_gpu.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=999999, timeout=timeout_sec)\n",
    "\n",
    "    print(\"\\nOptuna best AP:\", study.best_value)\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(k, \"=\", v)\n",
    "\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f368d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path.cwd().parents[0]\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "train_log = pd.read_csv(DATA_DIR / \"train_log.csv\")\n",
    "test_log  = pd.read_csv(DATA_DIR / \"test_log.csv\")\n",
    "\n",
    "train_log[\"Z_err\"] = train_log[\"Z_err\"].fillna(0.0)\n",
    "test_log[\"Z_err\"] = test_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "train_splits = sorted(train_log[\"split\"].unique())\n",
    "test_splits = sorted(test_log[\"split\"].unique())\n",
    "\n",
    "train_lc_cache, train_idx_cache = build_lightcurve_cache(train_splits, base_dir=DATA_DIR, kind=\"train\")\n",
    "test_lc_cache, test_idx_cache = build_lightcurve_cache(test_splits, base_dir=DATA_DIR, kind=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3411737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feat: (6086, 353)\n",
      "test_feat : (7135, 352)\n"
     ]
    }
   ],
   "source": [
    "test_zerr_pool = test_log[\"Z_err\"].dropna().values\n",
    "\n",
    "train_feat = build_feature_table(\n",
    "    train_log, train_lc_cache, train_idx_cache,\n",
    "    augment_photoz=True,\n",
    "    test_zerr_pool=test_zerr_pool,\n",
    "    n_aug=1,\n",
    "    seed=6\n",
    ")\n",
    "test_feat = build_feature_table(test_log, test_lc_cache, test_idx_cache)\n",
    "print(\"train_feat:\", train_feat.shape)\n",
    "print(\"test_feat :\", test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat, test_feat = add_spectype_teacher_features(train_feat, train_log, test_feat, n_splits=10)\n",
    "best_xgb_params = run_optuna_xgb(train_feat, n_folds_tune=10, timeout_sec=28800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d660ef",
   "metadata": {},
   "source": [
    "Best AP: **0.6134734232399863**\n",
    "Best Parameters:\n",
    "```json\n",
    "{\n",
    "  \"n_estimators\" : 4770,\n",
    "  \"learning_rate\" : 0.009408348066891026,\n",
    "  \"max_depth\" : 5,\n",
    "  \"min_child_weight\" : 38,\n",
    "  \"subsample\" : 0.9580408244820326,\n",
    "  \"colsample_bytree\" : 0.5859885271647445,\n",
    "  \"gamma\" : 8.679259249940205,\n",
    "  \"reg_alpha\" : 17.53744145401043,\n",
    "  \"reg_lambda\" : 24.224933334472816,\n",
    "  \"max_delta_step\" : 2,\n",
    "  \"grow_policy\" : \"depthwise\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382a8a0",
   "metadata": {},
   "source": [
    "Kernel reset so reinitialize xgb from best params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 | XGB best F1=0.8571 @ th=0.571\n",
      "Fold 02 | XGB best F1=0.4000 @ th=0.537\n",
      "Fold 03 | XGB best F1=0.4444 @ th=0.335\n",
      "Fold 04 | XGB best F1=0.0000 @ th=0.010\n",
      "Fold 05 | XGB best F1=0.5000 @ th=0.488\n",
      "Fold 06 | XGB best F1=0.6829 @ th=0.463\n",
      "Fold 07 | XGB best F1=0.6667 @ th=0.468\n",
      "Fold 08 | XGB best F1=0.5333 @ th=0.389\n",
      "Fold 09 | XGB best F1=0.6667 @ th=0.517\n",
      "Fold 10 | XGB best F1=0.6667 @ th=0.586\n",
      "Fold 11 | XGB best F1=0.3846 @ th=0.207\n",
      "Fold 12 | XGB best F1=0.6000 @ th=0.246\n",
      "Fold 13 | XGB best F1=0.7660 @ th=0.502\n",
      "Fold 14 | XGB best F1=0.6753 @ th=0.374\n",
      "Fold 15 | XGB best F1=0.6875 @ th=0.547\n",
      "Fold 16 | XGB best F1=0.0000 @ th=0.010\n",
      "Fold 17 | XGB best F1=0.5000 @ th=0.251\n",
      "Fold 18 | XGB best F1=0.5714 @ th=0.670\n",
      "Fold 19 | XGB best F1=0.8000 @ th=0.424\n",
      "Fold 20 | XGB best F1=0.5405 @ th=0.286\n",
      "\n",
      "OOF XGB best threshold: 0.46798994974874375\n",
      "OOF XGB best F1: 0.5531914893617021\n",
      "Saved XGB-only2.csv | threshold: 0.46798994974874375\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "y = train_feat[\"target\"].astype(int).to_numpy()\n",
    "groups = train_feat[\"split\"].to_numpy()\n",
    "X = clean_features(train_feat, drop_cols=[\"object_id\", \"split\", \"target\"])\n",
    "\n",
    "X_test = clean_features(test_feat, drop_cols=[\"object_id\", \"split\"])\n",
    "\n",
    "xgb_base = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"random_state\": 67,\n",
    "    \"n_jobs\": -1,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"n_estimators\" : 4770,\n",
    "    \"learning_rate\" : 0.009408348066891026,\n",
    "    \"max_depth\" : 5,\n",
    "    \"min_child_weight\" : 38,\n",
    "    \"subsample\" : 0.9580408244820326,\n",
    "    \"colsample_bytree\" : 0.5859885271647445,\n",
    "    \"gamma\" : 8.679259249940205,\n",
    "    \"reg_alpha\" : 17.53744145401043,\n",
    "    \"reg_lambda\" : 24.224933334472816,\n",
    "    \"max_delta_step\" : 2,\n",
    "    \"grow_policy\" : \"depthwise\"\n",
    "}\n",
    "\n",
    "splitter = StratifiedGroupKFold(n_splits=len(train_splits), shuffle=True, random_state=6)\n",
    "split_iter = splitter.split(X, y, groups)\n",
    "\n",
    "oof = np.zeros(len(X), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(split_iter, 1):\n",
    "    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "    neg = np.sum(y_tr == 0)\n",
    "    pos = np.sum(y_tr == 1)\n",
    "    spw = float(neg / max(1, pos))\n",
    "\n",
    "    model = XGBClassifier(**{**xgb_base, \"scale_pos_weight\": spw})\n",
    "    model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "    oof[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    th, f1 = best_threshold_f1(y_va, oof[va_idx])\n",
    "    print(f\"Fold {fold:02d} | XGB best F1={f1:.4f} @ th={th:.3f}\")\n",
    "\n",
    "best_th, best_f1 = best_threshold_f1(y, oof)\n",
    "print(\"\\nOOF XGB best threshold:\", best_th)\n",
    "print(\"OOF XGB best F1:\", best_f1)\n",
    "\n",
    "neg = np.sum(y == 0)\n",
    "pos = np.sum(y == 1)\n",
    "spw_full = float(neg / pos)\n",
    "\n",
    "final_model = XGBClassifier(**{**xgb_base, \"scale_pos_weight\": spw_full})\n",
    "final_model.fit(X, y, verbose=False)\n",
    "\n",
    "test_probs = final_model.predict_proba(X_test)[:, 1]\n",
    "test_pred = (test_probs > best_th).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"object_id\": test_feat[\"object_id\"].values,\n",
    "    \"target\": test_pred\n",
    "})\n",
    "sub.to_csv(\"XGB-only3.csv\", index=False)\n",
    "print(\"Saved XGB-only2.csv | threshold:\", best_th)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
